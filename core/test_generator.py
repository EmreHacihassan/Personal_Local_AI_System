"""
AI ile √ñƒüren - Test Olu≈üturucu
Enterprise Test Generator

NotebookLM tarzƒ± interaktif testler olu≈üturur.
"""

import json
import re
import uuid
from typing import Optional, List, Dict, Any, Generator
from datetime import datetime

from core.config import settings
from core.llm_manager import llm_manager
from core.vector_store import vector_store
from core.learning_workspace import (
    learning_workspace_manager,
    Test,
    TestQuestion,
    TestType,
    TestStatus
)


class TestGenerator:
    """
    Test olu≈üturucu.
    
    √ñzellikler:
    - √áoktan se√ßmeli, doƒüru/yanlƒ±≈ü, bo≈üluk doldurma, kƒ±sa cevap
    - Zorluk seviyeleri (kolay, orta, zor, karƒ±≈üƒ±k)
    - Kaynak bazlƒ± soru √ºretimi
    - A√ßƒ±klama ve feedback
    - Anlamadƒ±ƒüƒ±nƒ± sor √∂zelliƒüi
    """
    
    # Soru t√ºr√º ≈üablonlarƒ±
    QUESTION_TEMPLATES = {
        TestType.MULTIPLE_CHOICE: {
            "name": "√áoktan Se√ßmeli",
            "description": "4 se√ßenekli, tek doƒüru cevaplƒ± sorular",
            "option_count": 4
        },
        TestType.TRUE_FALSE: {
            "name": "Doƒüru/Yanlƒ±≈ü",
            "description": "ƒ∞fadenin doƒüruluƒüunu deƒüerlendirme",
            "option_count": 2
        },
        TestType.FILL_BLANK: {
            "name": "Bo≈üluk Doldurma",
            "description": "Eksik kelime/kavramƒ± tamamlama",
            "option_count": 0
        },
        TestType.SHORT_ANSWER: {
            "name": "Kƒ±sa Cevap",
            "description": "1-3 c√ºmlelik a√ßƒ±k u√ßlu sorular",
            "option_count": 0
        },
        TestType.MIXED: {
            "name": "Karƒ±≈üƒ±k",
            "description": "T√ºm soru t√ºrlerinden karƒ±≈üƒ±k",
            "option_count": 0
        }
    }
    
    DIFFICULTY_LEVELS = {
        "easy": {
            "name": "Kolay",
            "description": "Temel kavramlar, tanƒ±mlar",
            "complexity": "basit ve doƒürudan"
        },
        "medium": {
            "name": "Orta",
            "description": "Uygulama ve anlama gerektiren",
            "complexity": "orta d√ºzey analiz gerektiren"
        },
        "hard": {
            "name": "Zor",
            "description": "Analiz, sentez ve deƒüerlendirme",
            "complexity": "derinlemesine d√º≈ü√ºnme ve analiz gerektiren"
        },
        "mixed": {
            "name": "Karƒ±≈üƒ±k",
            "description": "T√ºm zorluk seviyelerinden",
            "complexity": "√ße≈üitli zorluk seviyelerinde"
        }
    }
    
    def __init__(self):
        self.manager = learning_workspace_manager
    
    async def generate_test(
        self,
        test_id: str,
        active_source_ids: List[str] = None,
        custom_instructions: str = ""
    ) -> Generator[Dict, None, None]:
        """
        Test olu≈ütur (streaming).
        
        Args:
            test_id: Test ID
            active_source_ids: Aktif kaynak ID'leri
            custom_instructions: √ñzel talimatlar
            
        Yields:
            Progress updates ve sorular
        """
        test = self.manager.get_test(test_id)
        if not test:
            yield {"type": "error", "message": "Test bulunamadƒ±"}
            return
        
        try:
            # ===== PHASE 1: KAYNAK TOPLAMA =====
            yield {
                "type": "status",
                "phase": "gathering",
                "message": "üìö Kaynaklar toplanƒ±yor...",
                "progress": 10
            }
            
            # Workspace bilgilerini al
            workspace = self.manager.get_workspace(test.workspace_id)
            topic = workspace.topic if workspace else "Genel"
            
            # RAG ile kaynak topla
            sources = await self._gather_sources(topic, active_source_ids)
            
            yield {
                "type": "sources",
                "count": len(sources),
                "message": f"üìñ {len(sources)} kaynak bulundu",
                "progress": 20
            }
            
            # ===== PHASE 2: SORU √úRETƒ∞Mƒ∞ =====
            questions = []
            questions_per_batch = 5
            total_batches = (test.question_count + questions_per_batch - 1) // questions_per_batch
            
            for batch_idx in range(total_batches):
                start_q = batch_idx * questions_per_batch
                end_q = min(start_q + questions_per_batch, test.question_count)
                batch_count = end_q - start_q
                
                progress = 20 + int((batch_idx / total_batches) * 60)
                
                yield {
                    "type": "status",
                    "phase": "generating",
                    "message": f"‚úçÔ∏è Sorular olu≈üturuluyor ({start_q + 1}-{end_q}/{test.question_count})...",
                    "progress": progress
                }
                
                # Bu batch i√ßin soru √ºret
                batch_questions = await self._generate_questions(
                    test=test,
                    sources=sources,
                    count=batch_count,
                    existing_questions=questions,
                    custom_instructions=custom_instructions
                )
                
                questions.extend(batch_questions)
                
                yield {
                    "type": "questions_batch",
                    "batch_index": batch_idx,
                    "questions": [q.to_dict() for q in batch_questions],
                    "total_so_far": len(questions),
                    "progress": progress + 10
                }
            
            # ===== PHASE 3: Fƒ∞NALƒ∞ZE =====
            yield {
                "type": "status",
                "phase": "finalizing",
                "message": "üîß Test tamamlanƒ±yor...",
                "progress": 90
            }
            
            # Testi g√ºncelle
            test.questions = [q.to_dict() for q in questions]
            test.status = TestStatus.NOT_STARTED
            self.manager.update_test(test)
            
            yield {
                "type": "complete",
                "message": "‚úÖ Test ba≈üarƒ±yla olu≈üturuldu!",
                "progress": 100,
                "test_id": test.id,
                "question_count": len(questions),
                "test_type": test.test_type.value,
                "difficulty": test.difficulty
            }
            
        except Exception as e:
            yield {
                "type": "error",
                "message": f"‚ùå Hata olu≈ütu: {str(e)}",
                "progress": -1
            }
    
    async def _gather_sources(
        self,
        topic: str,
        active_source_ids: List[str] = None
    ) -> List[Dict]:
        """Kaynak topla."""
        
        all_sources = []
        seen = set()
        
        try:
            results = vector_store.search_with_scores(
                query=topic,
                n_results=20,
                score_threshold=0.25
            )
            
            for result in results:
                content = result.get("document", "")
                content_hash = hash(content[:100])
                
                if content_hash in seen:
                    continue
                seen.add(content_hash)
                
                metadata = result.get("metadata", {})
                source_id = metadata.get("document_id", "")
                
                # Aktif kaynak filtresi
                if active_source_ids:
                    filename = metadata.get("original_filename", metadata.get("filename", ""))
                    if source_id not in active_source_ids and not any(sid in filename for sid in active_source_ids):
                        continue
                
                all_sources.append({
                    "content": content,
                    "source": metadata.get("original_filename", metadata.get("filename", "Bilinmeyen")),
                    "source_id": source_id,
                    "page": metadata.get("page")
                })
                
        except Exception as e:
            print(f"Source gathering error: {e}")
        
        return all_sources
    
    async def _generate_questions(
        self,
        test: Test,
        sources: List[Dict],
        count: int,
        existing_questions: List[TestQuestion],
        custom_instructions: str
    ) -> List[TestQuestion]:
        """Soru √ºret."""
        
        test_type_info = self.QUESTION_TEMPLATES.get(test.test_type, self.QUESTION_TEMPLATES[TestType.MIXED])
        difficulty_info = self.DIFFICULTY_LEVELS.get(test.difficulty, self.DIFFICULTY_LEVELS["mixed"])
        
        # Kaynak metni olu≈ütur
        sources_text = ""
        for i, src in enumerate(sources[:15], 1):
            sources_text += f"\n[KAYNAK {i}] ({src['source']}):\n{src['content'][:600]}\n"
        
        # Mevcut sorularƒ± listele (tekrar √∂nlemek i√ßin)
        existing_text = ""
        if existing_questions:
            existing_text = "\n\nZATEN OLU≈ûTURULMU≈û SORULAR (bunlardan farklƒ± olmalƒ±):\n"
            for q in existing_questions[-10:]:
                existing_text += f"- {q.question[:100]}\n"
        
        # Test t√ºr√ºne g√∂re format
        if test.test_type == TestType.MULTIPLE_CHOICE:
            format_instruction = """Her soru i√ßin:
- Soru metni
- 4 se√ßenek (A, B, C, D)
- Doƒüru cevap (sadece harf)
- A√ßƒ±klama (neden doƒüru/yanlƒ±≈ü)"""
        elif test.test_type == TestType.TRUE_FALSE:
            format_instruction = """Her soru i√ßin:
- ƒ∞fade metni
- Cevap: "Doƒüru" veya "Yanlƒ±≈ü"
- A√ßƒ±klama (neden doƒüru/yanlƒ±≈ü)"""
        elif test.test_type == TestType.FILL_BLANK:
            format_instruction = """Her soru i√ßin:
- C√ºmle (bo≈üluk ___ ile g√∂sterilmeli)
- Doƒüru cevap
- A√ßƒ±klama"""
        elif test.test_type == TestType.SHORT_ANSWER:
            format_instruction = """Her soru i√ßin:
- Soru metni
- Beklenen cevap (kƒ±sa)
- A√ßƒ±klama (detaylƒ±)"""
        else:  # MIXED
            format_instruction = """Karƒ±≈üƒ±k soru t√ºrleri olu≈ütur:
- √áoktan se√ßmeli (4 se√ßenek)
- Doƒüru/Yanlƒ±≈ü
- Bo≈üluk doldurma
- Kƒ±sa cevap"""
        
        prompt = f"""A≈üaƒüƒ±daki kaynaklara dayanarak {count} adet {test_type_info['name']} sorusu olu≈ütur.

KAYNAKLAR:
{sources_text}

TEST Bƒ∞LGƒ∞LERƒ∞:
- Test T√ºr√º: {test_type_info['name']} - {test_type_info['description']}
- Zorluk: {difficulty_info['name']} - {difficulty_info['complexity']}
- Soru Sayƒ±sƒ±: {count}

{f'KULLANICI TALƒ∞MATLARI: {custom_instructions}' if custom_instructions else ''}
{existing_text}

FORMAT:
{format_instruction}

JSON formatƒ±nda d√∂nd√ºr:
```json
[
  {{
    "question": "Soru metni",
    "question_type": "{test.test_type.value}",
    "options": ["A) Se√ßenek 1", "B) Se√ßenek 2", "C) Se√ßenek 3", "D) Se√ßenek 4"],
    "correct_answer": "A",
    "explanation": "Bu cevabƒ±n doƒüru olmasƒ±nƒ±n nedeni...",
    "difficulty": "medium",
    "source_ref": "KAYNAK 1"
  }}
]
```

√ñnemli:
- Sorular kaynaklardaki bilgilere dayansƒ±n
- Her sorunun a√ßƒ±k ve net bir cevabƒ± olsun
- A√ßƒ±klamalar √∂ƒüretici olsun
- Yanƒ±ltƒ±cƒ± se√ßenekler mantƒ±klƒ± ama yanlƒ±≈ü olsun
- Sorular {difficulty_info['complexity']} olsun

≈ûimdi {count} soru olu≈ütur:"""

        response = llm_manager.generate(prompt)
        
        # JSON parse
        questions = []
        try:
            json_match = re.search(r'```json\s*([\s\S]*?)\s*```', response)
            if json_match:
                data = json.loads(json_match.group(1))
            else:
                data = json.loads(response)
            
            for item in data:
                q_type = item.get("question_type", test.test_type.value)
                try:
                    q_type_enum = TestType(q_type)
                except:
                    q_type_enum = test.test_type
                
                question = TestQuestion(
                    id=str(uuid.uuid4()),
                    question=item.get("question", ""),
                    question_type=q_type_enum,
                    options=item.get("options", []),
                    correct_answer=item.get("correct_answer", ""),
                    explanation=item.get("explanation", ""),
                    difficulty=item.get("difficulty", "medium"),
                    source_ref=item.get("source_ref", "")
                )
                questions.append(question)
                
        except json.JSONDecodeError as e:
            print(f"JSON parse error: {e}")
            # Fallback: basit soru olu≈ütur
            questions.append(TestQuestion(
                id=str(uuid.uuid4()),
                question="Kaynaklardan bir soru olu≈üturulamadƒ±.",
                question_type=test.test_type,
                options=["A) Se√ßenek 1", "B) Se√ßenek 2", "C) Se√ßenek 3", "D) Se√ßenek 4"] if test.test_type == TestType.MULTIPLE_CHOICE else [],
                correct_answer="A",
                explanation="L√ºtfen testi yeniden olu≈üturun.",
                difficulty="medium"
            ))
        
        return questions
    
    async def explain_question(
        self,
        test_id: str,
        question_id: str,
        user_question: str
    ) -> str:
        """
        Soru hakkƒ±nda a√ßƒ±klama yap (anlamadƒ±ƒüƒ±nƒ± sor √∂zelliƒüi).
        
        Args:
            test_id: Test ID
            question_id: Soru ID
            user_question: Kullanƒ±cƒ±nƒ±n sorusu
            
        Returns:
            A√ßƒ±klama metni
        """
        test = self.manager.get_test(test_id)
        if not test:
            return "Test bulunamadƒ±."
        
        # Soruyu bul
        question_data = None
        for q in test.questions:
            if q.get("id") == question_id:
                question_data = q
                break
        
        if not question_data:
            return "Soru bulunamadƒ±."
        
        prompt = f"""Bir √∂ƒürenci a≈üaƒüƒ±daki test sorusu hakkƒ±nda yardƒ±m istiyor.

TEST SORUSU:
{question_data.get('question', '')}

SE√áENEKLER:
{chr(10).join(question_data.get('options', []))}

DOƒûRU CEVAP: {question_data.get('correct_answer', '')}

MEVCUT A√áIKLAMA:
{question_data.get('explanation', '')}

√ñƒûRENCƒ∞Nƒ∞N SORUSU:
"{user_question}"

L√ºtfen √∂ƒürenciye yardƒ±mcƒ± ol:
1. Sorusunu net bir ≈üekilde cevapla
2. Kavramƒ± a√ßƒ±kla
3. Doƒüru cevabƒ± verme, sadece anlamasƒ±na yardƒ±m et
4. √ñƒüretici ve te≈üvik edici ol
5. Gerekirse √∂rnekler ver

Yanƒ±tƒ±nƒ± T√ºrk√ße yaz:"""

        response = llm_manager.generate(prompt)
        return response
    
    async def grade_answer(
        self,
        test_id: str,
        question_id: str,
        user_answer: str
    ) -> Dict:
        """
        Cevabƒ± deƒüerlendir (√∂zellikle kƒ±sa cevap i√ßin).
        
        Returns:
            {is_correct, feedback, score}
        """
        test = self.manager.get_test(test_id)
        if not test:
            return {"is_correct": False, "feedback": "Test bulunamadƒ±", "score": 0}
        
        # Soruyu bul
        question_data = None
        for q in test.questions:
            if q.get("id") == question_id:
                question_data = q
                break
        
        if not question_data:
            return {"is_correct": False, "feedback": "Soru bulunamadƒ±", "score": 0}
        
        correct_answer = question_data.get("correct_answer", "")
        question_type = question_data.get("question_type", "")
        
        # Basit kar≈üƒ±la≈ütƒ±rma (√ßoktan se√ßmeli, D/Y)
        if question_type in [TestType.MULTIPLE_CHOICE.value, TestType.TRUE_FALSE.value]:
            is_correct = user_answer.strip().upper() == correct_answer.strip().upper()
            return {
                "is_correct": is_correct,
                "feedback": question_data.get("explanation", ""),
                "score": 100 if is_correct else 0,
                "correct_answer": correct_answer
            }
        
        # Bo≈üluk doldurma - basit kar≈üƒ±la≈ütƒ±rma
        if question_type == TestType.FILL_BLANK.value:
            is_correct = user_answer.strip().lower() == correct_answer.strip().lower()
            return {
                "is_correct": is_correct,
                "feedback": question_data.get("explanation", ""),
                "score": 100 if is_correct else 0,
                "correct_answer": correct_answer
            }
        
        # Kƒ±sa cevap - LLM ile deƒüerlendir
        prompt = f"""A≈üaƒüƒ±daki soruya verilen √∂ƒürenci cevabƒ±nƒ± deƒüerlendir.

SORU:
{question_data.get('question', '')}

BEKLENEN CEVAP:
{correct_answer}

√ñƒûRENCƒ∞Nƒ∞N CEVABI:
{user_answer}

Deƒüerlendirme kriterleri:
1. Kavram doƒüruluƒüu
2. ƒ∞√ßerik b√ºt√ºnl√ºƒü√º
3. Anahtar noktalarƒ±n varlƒ±ƒüƒ±

JSON formatƒ±nda d√∂nd√ºr:
```json
{{
  "score": 0-100,
  "is_correct": true/false,
  "feedback": "Deƒüerlendirme a√ßƒ±klamasƒ±"
}}
```"""

        response = llm_manager.generate(prompt)
        
        try:
            json_match = re.search(r'```json\s*([\s\S]*?)\s*```', response)
            if json_match:
                result = json.loads(json_match.group(1))
            else:
                result = json.loads(response)
            
            result["correct_answer"] = correct_answer
            return result
            
        except:
            # Fallback
            return {
                "is_correct": False,
                "feedback": "Deƒüerlendirme yapƒ±lamadƒ±. Manuel kontrol gerekli.",
                "score": 0,
                "correct_answer": correct_answer
            }
    
    def get_test_summary(self, test_id: str) -> Dict:
        """Test √∂zeti."""
        test = self.manager.get_test(test_id)
        if not test:
            return {}
        
        return {
            "id": test.id,
            "title": test.title,
            "description": test.description,
            "test_type": test.test_type.value,
            "question_count": len(test.questions),
            "difficulty": test.difficulty,
            "status": test.status.value,
            "score": test.score,
            "created_at": test.created_at,
            "completed_at": test.completed_at
        }
    
    def get_available_types(self) -> Dict:
        """Kullanƒ±labilir test t√ºrlerini d√∂nd√ºr."""
        return {
            k.value: v for k, v in self.QUESTION_TEMPLATES.items()
        }
    
    def get_difficulty_levels(self) -> Dict:
        """Zorluk seviyelerini d√∂nd√ºr."""
        return self.DIFFICULTY_LEVELS


# Singleton instance
test_generator = TestGenerator()
