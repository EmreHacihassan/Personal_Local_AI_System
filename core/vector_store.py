"""
Enterprise AI Assistant - Vector Store
End√ºstri Standartlarƒ±nda Kurumsal AI √á√∂z√ºm√º

ChromaDB tabanlƒ± vector veritabanƒ± y√∂netimi.
Yeni EnterpriseVectorStore entegrasyonu ile daha dayanƒ±klƒ±.

Features:
- Semantic search with scores
- Page-based retrieval
- Metadata filtering
- Batch operations
- Parent-child document support
- Embedding-based search
- Automatic corruption recovery
- Health monitoring
- ADVANCED: Duplicate detection (exact + semantic)
- ADVANCED: Near-duplicate detection
- ADVANCED: Content quality scoring
- ADVANCED: Query caching
- ADVANCED: Usage analytics
- ADVANCED: Auto-cleanup
- ADVANCED: Export/Import
"""

from typing import List, Dict, Any, Optional
from pathlib import Path
import hashlib
import traceback

from .config import settings
from .embedding import embedding_manager
from .logger import get_logger
from .chromadb_manager import (
    ChromaDBManager,
    ChromaDBConfig,
    get_chromadb_manager,
)
from .enterprise_vector_store import (
    EnterpriseVectorStore,
    DuplicateConfig,
    ContentQualityConfig,
    get_enterprise_vector_store,
)

logger = get_logger("vector_store")


class VectorStore:
    """
    Vector Store y√∂netim sƒ±nƒ±fƒ± - End√ºstri standartlarƒ±na uygun.
    
    Yeni EnterpriseVectorStore ile entegre, daha dayanƒ±klƒ± ve g√ºvenli.
    
    √ñzellikler:
    - ChromaDB persistence (via ChromaDBManager)
    - Automatic recovery
    - Metadata filtering
    - Similarity search
    - Batch operations
    - Health monitoring
    
    ADVANCED √ñzellikler:
    - Akƒ±llƒ± Duplicate Detection (Hash + Semantic)
    - Near-Duplicate Detection
    - Content Quality Scoring
    - Query Caching
    - Usage Analytics
    - Auto-Cleanup
    """
    
    def __init__(
        self,
        persist_directory: Optional[str] = None,
        collection_name: Optional[str] = None,
        enable_advanced_features: bool = True,
    ):
        self.persist_directory = persist_directory or settings.CHROMA_PERSIST_DIR
        self.collection_name = collection_name or settings.CHROMA_COLLECTION_NAME
        self.enable_advanced_features = enable_advanced_features
        
        # Use ChromaDBManager
        config = ChromaDBConfig(
            persist_directory=self.persist_directory,
            collection_name=self.collection_name,
            auto_backup=True,
            auto_recovery=True,
            enable_wal_mode=True,
            max_retries=5,
        )
        
        self._manager = get_chromadb_manager(config)
        
        # Enterprise features
        if enable_advanced_features:
            self._enterprise_store = get_enterprise_vector_store(
                duplicate_config=DuplicateConfig(
                    enabled=True,
                    check_exact_hash=True,
                    check_semantic=True,
                    semantic_threshold=0.95,
                    check_near_duplicates=True,
                    near_duplicate_threshold=0.85,
                ),
                quality_config=ContentQualityConfig(
                    enabled=True,
                    min_length=10,
                    min_word_count=3,
                    min_quality_score=0.2,
                ),
            )
        else:
            self._enterprise_store = None
        
        self._initialized = False
    
    def _ensure_initialized(self):
        """Lazy initialization."""
        if not self._initialized:
            if not self._manager.is_healthy:
                self._manager.initialize()
            self._initialized = True
    
    @property
    def collection(self):
        """Collection eri≈üimi i√ßin backward compatibility."""
        self._ensure_initialized()
        return self._manager._collection
    
    @property
    def client(self):
        """Client eri≈üimi i√ßin backward compatibility."""
        self._ensure_initialized()
        return self._manager._client
    
    def add_documents(
        self,
        documents: List[str],
        metadatas: Optional[List[Dict[str, Any]]] = None,
        ids: Optional[List[str]] = None,
        skip_duplicates: bool = True,
        validate_content: bool = True,
    ) -> List[str]:
        """
        D√∂k√ºmanlarƒ± vector store'a ekle.
        
        Args:
            documents: D√∂k√ºman i√ßerikleri
            metadatas: Metadata listesi (opsiyonel)
            ids: D√∂k√ºman ID'leri (opsiyonel, otomatik olu≈üturulur)
            skip_duplicates: Duplicate d√∂k√ºmanlarƒ± atla (varsayƒ±lan: True)
            validate_content: ƒ∞√ßerik doƒürulama (varsayƒ±lan: True)
            
        Returns:
            Eklenen d√∂k√ºman ID'leri
        """
        self._ensure_initialized()
        
        if not documents:
            return []
        
        # Use enterprise store if available for advanced features
        if self.enable_advanced_features and self._enterprise_store:
            result = self._enterprise_store.add_documents(
                documents=documents,
                metadatas=metadatas,
                ids=ids,
                skip_duplicates=skip_duplicates,
                validate_content=validate_content,
                enrich_metadata=True,
            )
            return result.get("added_ids", [])
        
        # Fallback to basic duplicate check
        unique_docs = []
        unique_metadatas = []
        unique_ids = []
        skipped_count = 0
        
        for i, doc in enumerate(documents):
            # Generate content-based hash for duplicate detection
            content_hash = hashlib.md5(doc.strip().encode()).hexdigest()
            doc_id = ids[i] if ids else f"doc_{content_hash[:16]}"
            
            if skip_duplicates:
                # Check if document with this hash already exists
                existing = self._check_duplicate(content_hash, doc)
                if existing:
                    skipped_count += 1
                    logger.debug(f"Duplicate skipped: {content_hash[:8]}...")
                    continue
            
            unique_docs.append(doc)
            unique_metadatas.append(
                {**(metadatas[i] if metadatas else {}), "content_hash": content_hash}
            )
            unique_ids.append(doc_id)
        
        if not unique_docs:
            if skipped_count > 0:
                print(f"‚è≠Ô∏è {skipped_count} duplicate d√∂k√ºman atlandƒ±, yeni d√∂k√ºman yok.")
            return []
        
        # Generate embeddings only for unique documents
        print(f"üìä {len(unique_docs)} d√∂k√ºman i√ßin embedding olu≈üturuluyor...")
        if skipped_count > 0:
            print(f"‚è≠Ô∏è {skipped_count} duplicate d√∂k√ºman atlandƒ±.")
        
        try:
            embeddings = embedding_manager.embed_texts(unique_docs)
            
            # Add via manager
            self._manager.add_documents(
                documents=unique_docs,
                embeddings=embeddings,
                metadatas=unique_metadatas,
                ids=unique_ids,
            )
            
            print(f"‚úÖ {len(unique_docs)} d√∂k√ºman eklendi")
            return unique_ids
            
        except Exception as e:
            logger.error(f"Add documents failed: {e}")
            logger.debug(traceback.format_exc())
            raise
    
    def _check_duplicate(
        self,
        content_hash: str,
        content: str,
        similarity_threshold: float = 0.98,
    ) -> bool:
        """
        D√∂k√ºmanƒ±n duplicate olup olmadƒ±ƒüƒ±nƒ± kontrol et.
        """
        try:
            # First, check by exact content hash in metadata
            existing = self._manager.get(
                where={"content_hash": content_hash},
                limit=1,
            )
            if existing and existing.get("ids"):
                return True
            
            # If no exact match, check by semantic similarity (for near-duplicates)
            if len(content) > 100:
                results = self.search_with_scores(
                    query=content[:500],
                    n_results=1,
                    score_threshold=similarity_threshold,
                )
                if results and results[0]["score"] >= similarity_threshold:
                    return True
            
            return False
        except Exception as e:
            logger.warning(f"Duplicate check failed: {e}")
            return False
    
    def search(
        self,
        query: str,
        n_results: int = 5,
        where: Optional[Dict[str, Any]] = None,
        where_document: Optional[Dict[str, Any]] = None,
        use_cache: bool = True,
    ) -> Dict[str, Any]:
        """Semantic search yap (cache destekli)."""
        self._ensure_initialized()
        
        # Use enterprise store with caching if available
        if self.enable_advanced_features and self._enterprise_store and use_cache:
            results = self._enterprise_store.search(
                query=query,
                n_results=n_results,
                where=where,
                use_cache=True,
            )
            # Convert to legacy format
            return {
                "documents": [r["document"] for r in results],
                "metadatas": [r["metadata"] for r in results],
                "distances": [1 - r["score"] for r in results],
                "ids": [r["id"] for r in results],
            }
        
        query_embedding = embedding_manager.embed_query(query)
        
        results = self._manager.query(
            query_embeddings=[query_embedding],
            n_results=n_results,
            where=where,
            include=["documents", "metadatas", "distances"],
        )
        
        return {
            "documents": results["documents"][0] if results["documents"] else [],
            "metadatas": results["metadatas"][0] if results["metadatas"] else [],
            "distances": results["distances"][0] if results["distances"] else [],
            "ids": results["ids"][0] if results["ids"] else [],
        }
    
    def search_with_scores(
        self,
        query: str,
        n_results: int = 5,
        score_threshold: float = 0.0,
        where: Optional[Dict[str, Any]] = None,
    ) -> List[Dict[str, Any]]:
        """Score ile birlikte semantic search yap."""
        results = self.search(query, n_results, where)
        
        scored_results = []
        for i, doc in enumerate(results["documents"]):
            distance = results["distances"][i]
            score = 1 - distance
            
            if score >= score_threshold:
                scored_results.append({
                    "document": doc,
                    "metadata": results["metadatas"][i] if results["metadatas"] else {},
                    "score": score,
                    "id": results["ids"][i] if results["ids"] else None,
                })
        
        return scored_results
    
    def get_document(self, doc_id: str) -> Optional[Dict[str, Any]]:
        """ID ile d√∂k√ºman getir."""
        self._ensure_initialized()
        
        try:
            result = self._manager.get(
                ids=[doc_id],
                include=["documents", "metadatas"],
            )
            
            if result["documents"]:
                return {
                    "id": doc_id,
                    "document": result["documents"][0],
                    "metadata": result["metadatas"][0] if result["metadatas"] else {},
                }
            return None
        except Exception as e:
            logger.warning(f"Get document failed: {e}")
            return None
    
    def delete_document(self, doc_id: str) -> bool:
        """D√∂k√ºman sil."""
        self._ensure_initialized()
        
        try:
            self._manager.delete(ids=[doc_id])
            return True
        except Exception as e:
            logger.error(f"Delete document failed: {e}")
            return False
    
    def delete_documents(self, doc_ids: List[str]) -> int:
        """Birden fazla d√∂k√ºman sil."""
        self._ensure_initialized()
        
        try:
            self._manager.delete(ids=doc_ids)
            return len(doc_ids)
        except Exception as e:
            logger.error(f"Delete documents failed: {e}")
            return 0
    
    def delete_by_metadata(self, where: Dict[str, Any]) -> int:
        """Metadata'ya g√∂re d√∂k√ºmanlarƒ± sil."""
        self._ensure_initialized()
        
        try:
            results = self._manager.get(where=where)
            
            if results["ids"]:
                self._manager.delete(ids=results["ids"])
                return len(results["ids"])
            
            return 0
        except Exception as e:
            logger.error(f"delete_by_metadata failed: {e}")
            return 0
    
    def update_document(
        self,
        doc_id: str,
        document: Optional[str] = None,
        metadata: Optional[Dict[str, Any]] = None,
    ) -> bool:
        """D√∂k√ºman g√ºncelle."""
        self._ensure_initialized()
        
        try:
            update_kwargs: Dict[str, Any] = {"ids": [doc_id]}
            
            if document:
                update_kwargs["documents"] = [document]
                update_kwargs["embeddings"] = [embedding_manager.embed_text(document)]
            
            if metadata:
                update_kwargs["metadatas"] = [metadata]
            
            self._manager.update(**update_kwargs)
            return True
        except Exception as e:
            logger.error(f"Update document failed: {e}")
            return False
    
    def count(self) -> int:
        """Toplam d√∂k√ºman sayƒ±sƒ±nƒ± d√∂nd√ºr."""
        self._ensure_initialized()
        
        try:
            return self._manager.count()
        except Exception as e:
            logger.error(f"Count failed: {e}")
            return 0
    
    def clear(self) -> bool:
        """T√ºm d√∂k√ºmanlarƒ± sil."""
        self._ensure_initialized()
        
        try:
            self._manager.clear()
            return True
        except Exception as e:
            logger.error(f"Clear failed: {e}")
            return False
    
    def get_stats(self) -> Dict[str, Any]:
        """Vector store istatistiklerini d√∂nd√ºr."""
        self._ensure_initialized()
        
        stats = {
            "collection_name": self.collection_name,
            "persist_directory": self.persist_directory,
            "document_count": self.count(),
            "health": self._manager.get_status().get("health", {}),
            "advanced_features_enabled": self.enable_advanced_features,
        }
        
        # Add enterprise stats if available
        if self.enable_advanced_features and self._enterprise_store:
            enterprise_stats = self._enterprise_store.get_stats()
            stats["enterprise_stats"] = enterprise_stats
            stats["analytics"] = self._enterprise_store.get_analytics()
        
        return stats
    
    def find_duplicates(self, threshold: float = 0.95) -> List[Dict[str, Any]]:
        """Mevcut duplicate'leri tespit et."""
        if self.enable_advanced_features and self._enterprise_store:
            return self._enterprise_store.find_duplicates(threshold=threshold)
        return []
    
    def remove_duplicates(self, threshold: float = 0.98, dry_run: bool = True) -> Dict[str, Any]:
        """Duplicate d√∂k√ºmanlarƒ± otomatik temizle."""
        if self.enable_advanced_features and self._enterprise_store:
            return self._enterprise_store.remove_duplicates(threshold=threshold, dry_run=dry_run)
        return {"error": "Advanced features not enabled"}
    
    def cleanup_low_quality(self, min_quality: float = 0.3, dry_run: bool = True) -> Dict[str, Any]:
        """D√º≈ü√ºk kaliteli d√∂k√ºmanlarƒ± temizle."""
        if self.enable_advanced_features and self._enterprise_store:
            return self._enterprise_store.cleanup_low_quality(min_quality=min_quality, dry_run=dry_run)
        return {"error": "Advanced features not enabled"}
    
    def export_documents(self, output_path: str) -> Dict[str, Any]:
        """D√∂k√ºmanlarƒ± JSON dosyasƒ±na export et."""
        if self.enable_advanced_features and self._enterprise_store:
            return self._enterprise_store.export_documents(output_path)
        return {"error": "Advanced features not enabled"}
    
    def import_documents(self, input_path: str, skip_duplicates: bool = True) -> Dict[str, Any]:
        """JSON dosyasƒ±ndan d√∂k√ºman import et."""
        if self.enable_advanced_features and self._enterprise_store:
            return self._enterprise_store.import_documents(input_path, skip_duplicates=skip_duplicates)
        return {"error": "Advanced features not enabled"}
    
    def find_similar(self, doc_id: str, n_results: int = 5) -> List[Dict[str, Any]]:
        """Belirli bir d√∂k√ºmana benzer d√∂k√ºmanlarƒ± bul."""
        if self.enable_advanced_features and self._enterprise_store:
            return self._enterprise_store.find_similar(doc_id, n_results=n_results)
        return []
    
    def get_search_history(self, limit: int = 50) -> List[Dict[str, Any]]:
        """Arama ge√ßmi≈üi."""
        if self.enable_advanced_features and self._enterprise_store:
            return self._enterprise_store.get_search_history(limit=limit)
        return []
    
    def clear_cache(self):
        """Sorgu √∂nbelleƒüini temizle."""
        if self.enable_advanced_features and self._enterprise_store:
            self._enterprise_store.clear_cache()
    
    def search_by_embedding(
        self,
        embedding: List[float],
        n_results: int = 5,
        where: Optional[Dict[str, Any]] = None,
    ) -> List[Dict[str, Any]]:
        """Hazƒ±r embedding ile search yap (HyDE i√ßin)."""
        self._ensure_initialized()
        
        try:
            results = self._manager.query(
                query_embeddings=[embedding],
                n_results=n_results,
                where=where,
                include=["documents", "metadatas", "distances"],
            )
            
            scored_results = []
            for i, doc in enumerate(results["documents"][0] if results["documents"] else []):
                distance = results["distances"][0][i] if results["distances"] else 1.0
                score = 1 - distance
                
                scored_results.append({
                    "document": doc,
                    "content": doc,
                    "metadata": results["metadatas"][0][i] if results["metadatas"] else {},
                    "score": score,
                    "id": results["ids"][0][i] if results["ids"] else None,
                })
            
            return scored_results
        except Exception as e:
            logger.error(f"Embedding search error: {e}")
            return []
    
    def get_by_page_number(
        self,
        page_number: int,
        source: Optional[str] = None,
    ) -> List[Dict[str, Any]]:
        """Sayfa numarasƒ±na g√∂re chunk'larƒ± getir."""
        self._ensure_initialized()
        
        try:
            where_filter: Dict[str, Any] = {"page_number": page_number}
            if source:
                where_filter = {
                    "$and": [
                        {"page_number": page_number},
                        {"source": {"$eq": source}},
                    ]
                }
            
            results = self._manager.get(
                where=where_filter,
                include=["documents", "metadatas"],
            )
            
            chunks = []
            for i, doc in enumerate(results["documents"] if results["documents"] else []):
                chunks.append({
                    "id": results["ids"][i] if results["ids"] else None,
                    "document": doc,
                    "content": doc,
                    "text": doc,
                    "metadata": results["metadatas"][i] if results["metadatas"] else {},
                    "page_number": page_number,
                    "score": 1.0,
                })
            
            return chunks
        except Exception as e:
            logger.warning(f"Page search warning: {e}")
            return []
    
    def get_by_page_numbers(
        self,
        page_numbers: List[int],
        source: Optional[str] = None,
        max_results: int = 50,
    ) -> List[Dict[str, Any]]:
        """Birden fazla sayfa numarasƒ±na g√∂re chunk'larƒ± getir."""
        all_chunks = []
        
        for page_num in page_numbers:
            chunks = self.get_by_page_number(page_num, source)
            all_chunks.extend(chunks)
        
        all_chunks.sort(key=lambda x: (
            x.get("metadata", {}).get("page_number", 0),
            x.get("metadata", {}).get("chunk_index", 0),
        ))
        
        return all_chunks[:max_results]
    
    def get_parent_chunk(self, child_id: str) -> Optional[Dict[str, Any]]:
        """Child chunk'ƒ±n parent'ƒ±nƒ± getir."""
        self._ensure_initialized()
        
        try:
            child_result = self._manager.get(
                ids=[child_id],
                include=["metadatas"],
            )
            
            if not child_result["metadatas"]:
                return None
            
            parent_id = child_result["metadatas"][0].get("parent_id")
            if not parent_id:
                return None
            
            return self.get_document(parent_id)
        except Exception as e:
            logger.error(f"Parent chunk error: {e}")
            return None
    
    def get_children_chunks(self, parent_id: str) -> List[Dict[str, Any]]:
        """Parent chunk'ƒ±n children'larƒ±nƒ± getir."""
        self._ensure_initialized()
        
        try:
            results = self._manager.get(
                where={"parent_id": parent_id},
                include=["documents", "metadatas"],
            )
            
            children = []
            for i, doc in enumerate(results["documents"] if results["documents"] else []):
                children.append({
                    "id": results["ids"][i] if results["ids"] else None,
                    "document": doc,
                    "content": doc,
                    "metadata": results["metadatas"][i] if results["metadatas"] else {},
                })
            
            children.sort(key=lambda x: x.get("metadata", {}).get("chunk_index", 0))
            return children
        except Exception as e:
            logger.error(f"Children chunks error: {e}")
            return []
    
    def get_unique_sources(self) -> List[str]:
        """Benzersiz kaynak listesini d√∂nd√ºr."""
        self._ensure_initialized()
        
        try:
            all_data = self._manager.get(include=["metadatas"])
            sources = set()
            
            for meta in all_data.get("metadatas", []):
                if meta:
                    source = meta.get("source") or meta.get("filename", "")
                    if source:
                        sources.add(source)
            
            return sorted(list(sources))
        except Exception as e:
            logger.error(f"Get sources error: {e}")
            return []
    
    def get_document_stats(self) -> Dict[str, Any]:
        """Detaylƒ± d√∂k√ºman istatistikleri."""
        self._ensure_initialized()
        
        try:
            all_data = self._manager.get(include=["metadatas"])
            
            stats: Dict[str, Any] = {
                "total_chunks": len(all_data.get("ids", [])),
                "sources": {},
                "page_count": {},
                "chunk_types": {"parent": 0, "child": 0, "standalone": 0},
            }
            
            for meta in all_data.get("metadatas", []):
                if not meta:
                    continue
                
                source = meta.get("source") or meta.get("filename", "unknown")
                if source not in stats["sources"]:
                    stats["sources"][source] = 0
                stats["sources"][source] += 1
                
                page = meta.get("page_number")
                if page:
                    page_key = f"{source}_page_{page}"
                    if page_key not in stats["page_count"]:
                        stats["page_count"][page_key] = 0
                    stats["page_count"][page_key] += 1
                
                chunk_type = meta.get("chunk_type", "standalone")
                if chunk_type in stats["chunk_types"]:
                    stats["chunk_types"][chunk_type] += 1
            
            return stats
        except Exception as e:
            logger.error(f"Document stats error: {e}")
            return {"error": str(e)}
    
    def health_check(self) -> Dict[str, Any]:
        """Saƒülƒ±k kontrol√º."""
        self._ensure_initialized()
        return self._manager.get_status()
    
    def is_healthy(self) -> bool:
        """Saƒülƒ±k durumu."""
        return self._manager.is_healthy


# Singleton instance
vector_store = VectorStore()


def initialize_vector_store() -> bool:
    """Vector store'u ba≈ülat."""
    try:
        vector_store._ensure_initialized()
        return vector_store.is_healthy()
    except Exception as e:
        logger.error(f"Vector store initialization failed: {e}")
        return False
