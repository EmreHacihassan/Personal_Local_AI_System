"""
Enterprise AI Assistant - WebSocket Module
Real-time streaming chat desteÄŸi

EndÃ¼stri standardÄ± WebSocket implementasyonu.
GerÃ§ek token-by-token streaming ile.

V2 Features:
- Intelligent Model Routing (4B/8B)
- Human-in-the-Loop Feedback
- A/B Model Comparison
- Real-time Model Badges
"""

import json
import asyncio
from datetime import datetime
from typing import Optional, AsyncGenerator, Dict, Any
from fastapi import WebSocket, WebSocketDisconnect
from core.config import settings
from core.llm_manager import llm_manager
from core.logger import get_logger
from core.model_router import (
    get_model_router,
    ModelSize,
    FeedbackType,
    FeedbackStatus,
    MODEL_CONFIG,
)
from agents.orchestrator import orchestrator

logger = get_logger("websocket")


class ConnectionManager:
    """WebSocket baÄŸlantÄ± yÃ¶neticisi."""
    
    def __init__(self):
        self.active_connections: dict[str, WebSocket] = {}
        self.cancellation_flags: dict[str, bool] = {}
    
    async def connect(self, websocket: WebSocket, client_id: str) -> None:
        """Yeni baÄŸlantÄ± kabul et."""
        await websocket.accept()
        self.active_connections[client_id] = websocket
        self.cancellation_flags[client_id] = False
    
    def disconnect(self, client_id: str) -> None:
        """BaÄŸlantÄ±yÄ± kapat."""
        if client_id in self.active_connections:
            del self.active_connections[client_id]
        if client_id in self.cancellation_flags:
            del self.cancellation_flags[client_id]
    
    def cancel_stream(self, client_id: str) -> None:
        """Streaming'i iptal et."""
        self.cancellation_flags[client_id] = True
    
    def is_cancelled(self, client_id: str) -> bool:
        """Streaming iptal edildi mi kontrol et."""
        return self.cancellation_flags.get(client_id, False)
    
    def reset_cancellation(self, client_id: str) -> None:
        """Ä°ptal bayraÄŸÄ±nÄ± sÄ±fÄ±rla."""
        self.cancellation_flags[client_id] = False
    
    async def send_message(self, client_id: str, message: dict) -> None:
        """Belirli bir client'a mesaj gÃ¶nder."""
        if client_id in self.active_connections:
            try:
                await self.active_connections[client_id].send_json(message)
            except Exception as e:
                logger.error(f"Failed to send message to {client_id}: {e}")
    
    async def broadcast(self, message: dict) -> None:
        """TÃ¼m baÄŸlÄ± client'lara mesaj gÃ¶nder."""
        for connection in self.active_connections.values():
            try:
                await connection.send_json(message)
            except Exception:
                pass


# Global connection manager
manager = ConnectionManager()


def is_simple_chat_query(message: str) -> bool:
    """
    MesajÄ±n basit sohbet mi yoksa kompleks gÃ¶rev mi olduÄŸunu belirle.
    
    NOTE: Bu fonksiyon backward compatibility iÃ§in korunuyor.
    Yeni kod ModelRouter kullanmalÄ±.
    """
    message_lower = message.lower().strip()
    
    # Basit sohbet kalÄ±plarÄ±
    simple_patterns = [
        "merhaba", "selam", "nasÄ±lsÄ±n", "teÅŸekkÃ¼r", "saÄŸol", 
        "hello", "hi", "thanks", "bye", "hey",
        "gÃ¼naydÄ±n", "iyi geceler", "iyi akÅŸamlar",
        "naber", "ne haber", "hoÅŸÃ§akal",
    ]
    
    # Kompleks gÃ¶rev kalÄ±plarÄ± (RAG/Agent gerektiren)
    complex_patterns = [
        "ara", "bul", "getir", "listele", "analiz", "yaz", "hazÄ±rla",
        "dÃ¶kÃ¼man", "dosya", "upload", "belge", "rapor",
        "search", "find", "analyze", "write", "document",
    ]
    
    # Basit sohbet mi?
    if any(pattern in message_lower for pattern in simple_patterns) and len(message.split()) <= 10:
        return True
    
    # Kompleks gÃ¶rev mi?
    if any(pattern in message_lower for pattern in complex_patterns):
        return False
    
    # Soru mu? (kÄ±sa sorular direkt streaming ile yanÄ±tlanabilir)
    if len(message.split()) <= 20 and message.endswith("?"):
        return True
    
    # Default: kompleks
    return False


async def route_and_generate(
    client_id: str,
    message: str,
    use_routing: bool = True,
) -> AsyncGenerator[Dict[str, Any], None]:
    """
    Model router ile routing yapÄ±p yanÄ±t Ã¼ret.
    
    Args:
        client_id: Client ID
        message: KullanÄ±cÄ± mesajÄ±  
        use_routing: Model routing kullanÄ±lsÄ±n mÄ±
        
    Yields:
        Streaming mesajlarÄ± (routing_info, chunk, end, etc.)
    """
    try:
        model_router = get_model_router()
        
        # 1. ROUTING - Hangi model kullanÄ±lacak?
        routing_result = await model_router.route_async(message)
        
        # Routing bilgisini gÃ¶nder
        yield {
            "type": "routing",
            "routing": routing_result.to_dict(),
            "timestamp": datetime.now().isoformat()
        }
        
        # 2. MODEL SEÃ‡Ä°MÄ°
        model_name = routing_result.model_name
        model_config = MODEL_CONFIG[routing_result.model_size]
        
        logger.info(
            f"Routing decision: {model_config['display_name']} "
            f"(confidence: {routing_result.confidence:.2f}, "
            f"source: {routing_result.decision_source.value})"
        )
        
        # 3. LLM'DEN STREAMING YANIT
        default_system = """Sen yardÄ±mcÄ± bir AI asistanÄ±sÄ±n. TÃ¼rkÃ§e yanÄ±t ver.
KullanÄ±cÄ±ya nazik ve bilgilendirici yanÄ±tlar sun."""
        
        async for chunk in llm_manager.generate_stream_async(
            prompt=message,
            system_prompt=default_system,
            temperature=0.7,
            model=model_name,  # SeÃ§ilen modeli kullan
        ):
            # Ä°ptal kontrolÃ¼
            if manager.is_cancelled(client_id):
                logger.info(f"Streaming cancelled for {client_id}")
                yield {
                    "type": "cancelled",
                    "timestamp": datetime.now().isoformat()
                }
                return
            
            yield {
                "type": "chunk",
                "content": chunk,
                "timestamp": datetime.now().isoformat()
            }
        
        # 4. BÄ°TÄ°Å - Feedback iÃ§in gerekli bilgilerle
        yield {
            "type": "end",
            "response_id": routing_result.response_id,
            "model_size": routing_result.model_size.value,
            "model_name": model_name,
            "model_icon": model_config["icon"],
            "model_display_name": model_config["display_name"],
            "confidence": routing_result.confidence,
            "decision_source": routing_result.decision_source.value,
            "attempt_number": routing_result.attempt_number,
            "streaming_type": "real",
            "timestamp": datetime.now().isoformat()
        }
        
    except Exception as e:
        logger.error(f"Route and generate error: {e}")
        yield {
            "type": "error",
            "content": f"Bir hata oluÅŸtu: {str(e)}",
            "timestamp": datetime.now().isoformat()
        }


async def stream_llm_response(
    client_id: str,
    message: str,
    system_prompt: Optional[str] = None
) -> AsyncGenerator[str, None]:
    """
    LLM'den gerÃ§ek streaming yanÄ±t al.
    Token-by-token streaming saÄŸlar.
    """
    default_system = """Sen yardÄ±mcÄ± bir AI asistanÄ±sÄ±n. TÃ¼rkÃ§e yanÄ±t ver.
KullanÄ±cÄ±ya nazik ve bilgilendirici yanÄ±tlar sun."""
    
    async for chunk in llm_manager.generate_stream_async(
        prompt=message,
        system_prompt=system_prompt or default_system,
        temperature=0.7,
    ):
        # Ä°ptal kontrolÃ¼
        if manager.is_cancelled(client_id):
            logger.info(f"Streaming cancelled for {client_id}")
            break
        yield chunk


async def handle_chat_message(
    websocket: WebSocket,
    client_id: str,
    message: str,
    session_id: Optional[str] = None,
    use_streaming: bool = True,
    use_routing: bool = True,
    force_model: Optional[str] = None,
) -> None:
    """
    Chat mesajÄ±nÄ± iÅŸle ve streaming yanÄ±t gÃ¶nder.
    
    Args:
        websocket: WebSocket baÄŸlantÄ±sÄ±
        client_id: Client ID
        message: KullanÄ±cÄ± mesajÄ±
        session_id: Session ID (opsiyonel)
        use_streaming: GerÃ§ek streaming mi kullanÄ±lsÄ±n
        use_routing: Model routing kullanÄ±lsÄ±n mÄ±
        force_model: Zorla belirli model kullan (comparison iÃ§in)
    """
    try:
        # Ä°ptal bayraÄŸÄ±nÄ± sÄ±fÄ±rla
        manager.reset_cancellation(client_id)
        
        # BaÅŸlangÄ±Ã§ mesajÄ±
        await manager.send_message(client_id, {
            "type": "start",
            "timestamp": datetime.now().isoformat()
        })
        
        # FORCED MODEL - KarÅŸÄ±laÅŸtÄ±rma modu
        if force_model:
            logger.info(f"Forced model mode: {force_model}")
            
            # Direkt belirtilen modeli kullan
            default_system = """Sen yardÄ±mcÄ± bir AI asistanÄ±sÄ±n. TÃ¼rkÃ§e yanÄ±t ver.
KullanÄ±cÄ±ya nazik ve bilgilendirici yanÄ±tlar sun."""
            
            async for chunk in llm_manager.generate_stream_async(
                prompt=message,
                system_prompt=default_system,
                temperature=0.7,
                model=force_model,
            ):
                if manager.is_cancelled(client_id):
                    break
                    
                await manager.send_message(client_id, {
                    "type": "chunk",
                    "content": chunk,
                    "timestamp": datetime.now().isoformat()
                })
            
            await manager.send_message(client_id, {
                "type": "end",
                "model_name": force_model,
                "streaming_type": "forced",
                "timestamp": datetime.now().isoformat()
            })
            return
        
        # MODEL ROUTING MODE
        if use_routing:
            logger.info(f"Using model routing for: {message[:50]}...")
            
            async for event in route_and_generate(client_id, message):
                if event["type"] == "cancelled":
                    await manager.send_message(client_id, event)
                    return
                await manager.send_message(client_id, event)
            
            return
        
        # LEGACY MODE - Basit sorgu kontrolÃ¼
        is_simple = is_simple_chat_query(message)
        
        if is_simple and use_streaming:
            # GERÃ‡EK STREAMING - Token by token
            logger.info(f"Using real streaming for: {message[:50]}...")
            
            full_content = []
            async for chunk in stream_llm_response(client_id, message):
                if manager.is_cancelled(client_id):
                    break
                    
                full_content.append(chunk)
                await manager.send_message(client_id, {
                    "type": "chunk",
                    "content": chunk,
                    "timestamp": datetime.now().isoformat()
                })
            
            # BitiÅŸ mesajÄ±
            await manager.send_message(client_id, {
                "type": "end",
                "agent": "assistant",
                "streaming_type": "real",
                "timestamp": datetime.now().isoformat()
            })
            
        else:
            # ORCHESTRATOR MODU - Kompleks gÃ¶revler iÃ§in
            logger.info(f"Using orchestrator for: {message[:50]}...")
            
            # Orchestrator iÅŸlem bilgisi
            await manager.send_message(client_id, {
                "type": "processing",
                "message": "GÃ¶rev analiz ediliyor...",
                "timestamp": datetime.now().isoformat()
            })
            
            # Orchestrator ile gÃ¶revi iÅŸle
            response = await asyncio.get_event_loop().run_in_executor(
                None, 
                lambda: orchestrator.process(message)
            )
            
            if manager.is_cancelled(client_id):
                await manager.send_message(client_id, {
                    "type": "cancelled",
                    "timestamp": datetime.now().isoformat()
                })
                return
            
            if response.success:
                content = response.content
                
                # YanÄ±tÄ± streaming olarak gÃ¶nder (ama gerÃ§ek token deÄŸil, chunk)
                chunk_size = 15  # Daha doÄŸal gÃ¶rÃ¼nÃ¼m iÃ§in
                
                for i in range(0, len(content), chunk_size):
                    if manager.is_cancelled(client_id):
                        break
                        
                    chunk = content[i:i + chunk_size]
                    await manager.send_message(client_id, {
                        "type": "chunk",
                        "content": chunk,
                        "timestamp": datetime.now().isoformat()
                    })
                    await asyncio.sleep(0.015)  # KÃ¼Ã§Ã¼k gecikme
                
                # KaynaklarÄ± gÃ¶nder
                if response.sources:
                    await manager.send_message(client_id, {
                        "type": "sources",
                        "sources": response.sources,
                        "timestamp": datetime.now().isoformat()
                    })
            else:
                await manager.send_message(client_id, {
                    "type": "error",
                    "content": response.content,
                    "timestamp": datetime.now().isoformat()
                })
            
            # BitiÅŸ mesajÄ±
            await manager.send_message(client_id, {
                "type": "end",
                "agent": response.agent if hasattr(response, 'agent') else "orchestrator",
                "streaming_type": "simulated",
                "timestamp": datetime.now().isoformat()
            })
        
    except asyncio.CancelledError:
        await manager.send_message(client_id, {
            "type": "cancelled",
            "timestamp": datetime.now().isoformat()
        })
    except Exception as e:
        logger.error(f"Chat message error: {e}")
        await manager.send_message(client_id, {
            "type": "error",
            "content": f"Bir hata oluÅŸtu: {str(e)}",
            "timestamp": datetime.now().isoformat()
        })


async def websocket_endpoint(websocket: WebSocket, client_id: str) -> None:
    """
    Ana WebSocket endpoint'i.
    
    Desteklenen mesaj tipleri:
    - chat: Normal chat mesajÄ± (model routing ile)
    - chat_legacy: Eski mod (routing olmadan)
    - compare: Model karÅŸÄ±laÅŸtÄ±rma
    - feedback: KullanÄ±cÄ± feedback'i
    - confirm: Feedback onayÄ±
    - cancel: Streaming iptal
    - ping: Heartbeat
    
    Args:
        websocket: WebSocket baÄŸlantÄ±sÄ±
        client_id: Client ID
    """
    await manager.connect(websocket, client_id)
    
    try:
        # BaÄŸlantÄ± onayÄ±
        await manager.send_message(client_id, {
            "type": "connected",
            "client_id": client_id,
            "features": {
                "real_streaming": True,
                "cancellation": True,
                "sources": True,
                "model_routing": True,
                "feedback": True,
                "comparison": True,
            },
            "models": {
                k.value: {
                    "name": v["name"],
                    "display_name": v["display_name"],
                    "icon": v["icon"],
                }
                for k, v in MODEL_CONFIG.items()
            },
            "timestamp": datetime.now().isoformat()
        })
        
        while True:
            # Mesaj bekle
            data = await websocket.receive_json()
            
            message_type = data.get("type", "chat")
            
            # =====================
            # CHAT - Model Routing ile
            # =====================
            if message_type == "chat":
                message = data.get("message", "")
                session_id = data.get("session_id")
                use_streaming = data.get("streaming", True)
                use_routing = data.get("routing", True)  # Default: routing aÃ§Ä±k
                
                if message.strip():
                    await handle_chat_message(
                        websocket,
                        client_id,
                        message,
                        session_id,
                        use_streaming,
                        use_routing=use_routing,
                    )
            
            # =====================
            # CHAT LEGACY - Eski mod
            # =====================
            elif message_type == "chat_legacy":
                message = data.get("message", "")
                session_id = data.get("session_id")
                use_streaming = data.get("streaming", True)
                
                if message.strip():
                    await handle_chat_message(
                        websocket,
                        client_id,
                        message,
                        session_id,
                        use_streaming,
                        use_routing=False,
                    )
            
            # =====================
            # COMPARE - Model karÅŸÄ±laÅŸtÄ±rma
            # =====================
            elif message_type == "compare":
                feedback_id = data.get("feedback_id")
                
                if not feedback_id:
                    await manager.send_message(client_id, {
                        "type": "error",
                        "content": "feedback_id gerekli",
                        "timestamp": datetime.now().isoformat()
                    })
                    continue
                
                try:
                    model_router = get_model_router()
                    query, comparison_result = model_router.request_comparison(feedback_id)
                    
                    # KarÅŸÄ±laÅŸtÄ±rma baÅŸlangÄ±cÄ±
                    await manager.send_message(client_id, {
                        "type": "compare_start",
                        "feedback_id": feedback_id,
                        "comparison_routing": comparison_result.to_dict(),
                        "timestamp": datetime.now().isoformat()
                    })
                    
                    # Alternatif model ile yanÄ±t Ã¼ret
                    await handle_chat_message(
                        websocket,
                        client_id,
                        query,
                        force_model=comparison_result.model_name,
                    )
                    
                except ValueError as e:
                    await manager.send_message(client_id, {
                        "type": "error",
                        "content": str(e),
                        "timestamp": datetime.now().isoformat()
                    })
            
            # =====================
            # FEEDBACK - KullanÄ±cÄ± geri bildirimi
            # =====================
            elif message_type == "feedback":
                response_id = data.get("response_id")
                feedback_type = data.get("feedback_type")  # correct, downgrade, upgrade
                
                if not response_id or not feedback_type:
                    await manager.send_message(client_id, {
                        "type": "error",
                        "content": "response_id ve feedback_type gerekli",
                        "timestamp": datetime.now().isoformat()
                    })
                    continue
                
                try:
                    # FeedbackType'a Ã§evir
                    fb_type = FeedbackType(feedback_type)
                    
                    # Suggested model
                    suggested_model = None
                    if fb_type == FeedbackType.DOWNGRADE:
                        suggested_model = ModelSize.SMALL
                    elif fb_type == FeedbackType.UPGRADE:
                        suggested_model = ModelSize.LARGE
                    
                    model_router = get_model_router()
                    feedback = model_router.submit_feedback(
                        response_id=response_id,
                        feedback_type=fb_type,
                        suggested_model=suggested_model,
                    )
                    
                    # YanÄ±t
                    requires_comparison = fb_type != FeedbackType.CORRECT
                    
                    if fb_type == FeedbackType.CORRECT:
                        message = "âœ… TeÅŸekkÃ¼rler! Tercih kaydedildi."
                    elif fb_type == FeedbackType.DOWNGRADE:
                        message = "ğŸ”„ KÃ¼Ã§Ã¼k modeli denemek iÃ§in 'Dene' butonunu kullanÄ±n."
                    else:
                        message = "ğŸ”„ BÃ¼yÃ¼k modeli denemek iÃ§in 'Dene' butonunu kullanÄ±n."
                    
                    await manager.send_message(client_id, {
                        "type": "feedback_received",
                        "feedback": feedback.to_dict(),
                        "message": message,
                        "requires_comparison": requires_comparison,
                        "timestamp": datetime.now().isoformat()
                    })
                    
                except ValueError as e:
                    await manager.send_message(client_id, {
                        "type": "error",
                        "content": str(e),
                        "timestamp": datetime.now().isoformat()
                    })
            
            # =====================
            # CONFIRM - Feedback onayÄ±
            # =====================
            elif message_type == "confirm":
                feedback_id = data.get("feedback_id")
                confirmed = data.get("confirmed", False)
                
                if not feedback_id:
                    await manager.send_message(client_id, {
                        "type": "error",
                        "content": "feedback_id gerekli",
                        "timestamp": datetime.now().isoformat()
                    })
                    continue
                
                try:
                    model_router = get_model_router()
                    feedback = model_router.confirm_feedback(
                        feedback_id=feedback_id,
                        confirmed=confirmed,
                    )
                    
                    if confirmed:
                        model_config = MODEL_CONFIG.get(feedback.final_decision, {})
                        model_name = model_config.get("display_name", "Model")
                        message = f"âœ… Tercih kaydedildi! Benzer sorgular iÃ§in {model_name} kullanÄ±lacak."
                        learning_applied = True
                    else:
                        message = "â†©ï¸ Ä°lk tercih korundu. TeÅŸekkÃ¼rler!"
                        learning_applied = False
                    
                    await manager.send_message(client_id, {
                        "type": "feedback_confirmed",
                        "feedback": feedback.to_dict(),
                        "message": message,
                        "learning_applied": learning_applied,
                        "timestamp": datetime.now().isoformat()
                    })
                    
                except ValueError as e:
                    await manager.send_message(client_id, {
                        "type": "error",
                        "content": str(e),
                        "timestamp": datetime.now().isoformat()
                    })
            
            # =====================
            # CANCEL - Streaming iptal
            # =====================
            elif message_type == "cancel":
                manager.cancel_stream(client_id)
                await manager.send_message(client_id, {
                    "type": "cancel_acknowledged",
                    "timestamp": datetime.now().isoformat()
                })
            
            # =====================
            # PING - Heartbeat
            # =====================
            elif message_type == "ping":
                await manager.send_message(client_id, {
                    "type": "pong",
                    "timestamp": datetime.now().isoformat()
                })
            
    except WebSocketDisconnect:
        manager.disconnect(client_id)
        logger.info(f"Client disconnected: {client_id}")
    except Exception as e:
        manager.disconnect(client_id)
        logger.error(f"WebSocket error for {client_id}: {e}")
        raise e
