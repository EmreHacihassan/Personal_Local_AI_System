"""
ğŸš€ Enterprise WebSocket v2 - Real-time Streaming
=================================================

MyChatbot'tan ilham alan ama onu aÅŸan enterprise-grade WebSocket.

Ã–zellikler:
- ANLIK streaming (buffering yok)
- Keepalive ping/pong (25 saniye)
- Rate limiting (10 istek/5 saniye)
- Graceful shutdown
- Stop komutu desteÄŸi
- DetaylÄ± istatistikler
- BaÄŸlantÄ± durumu takibi
- Otomatik reconnection desteÄŸi

Protocol:
    Client -> Server:
        {"type": "chat", "message": "...", "session_id": "..."}
        {"type": "stop"}  - Streaming'i durdur
        {"type": "resume", "stream_id": "...", "from_index": N}  - KaldÄ±ÄŸÄ± yerden devam et
        {"type": "ping"}  - Manuel ping
    
    Server -> Client:
        {"type": "connected", "client_id": "...", "ts": ...}
        {"type": "start", "ts": ..., "stream_id": "..."}  - YanÄ±t baÅŸladÄ± (stream_id ile)
        {"type": "token", "content": "...", "index": N}  - Her token anÄ±nda (index ile)
        {"type": "status", "message": "...", "phase": "..."}  - Durum gÃ¼ncellemesi
        {"type": "sources", "sources": [...]}  - Kaynaklar
        {"type": "end", "stats": {...}}  - TamamlandÄ±
        {"type": "error", "message": "..."}  - Hata
        {"type": "stopped", "elapsed_ms": ...}  - Durduruldu
        {"type": "pong", "ts": ...}  - Ping yanÄ±tÄ±
        {"type": "resume_data", ...}  - Resume verisi

Mimari Prensip:
    WebSocket hiÃ§bir zaman state taÅŸÄ±maz. WebSocket sadece iletir.
    Token'lar StreamBuffer'da saklanÄ±r.
    Client reconnect edince kaldÄ±ÄŸÄ± yerden devam eder.
"""

import json
import asyncio
import time
import logging
import uuid
import sys
from datetime import datetime
from typing import Optional, Dict, Any, List, Set
from dataclasses import dataclass, field
from contextlib import asynccontextmanager

# Python 3.10 ve altÄ± iÃ§in async_timeout uyumluluÄŸu - GERÃ‡EK TIMEOUT Ä°MPLEMENTASYONU
if sys.version_info < (3, 11):
    try:
        from async_timeout import timeout as asyncio_timeout
    except ImportError:
        # async_timeout yoksa, GERÃ‡EK timeout implementasyonu - BU KRÄ°TÄ°K!
        @asynccontextmanager
        async def asyncio_timeout(seconds: float):
            """
            GerÃ§ek timeout wrapper - Python 3.10 uyumlu.
            
            Bu implementasyon GERÃ‡EKTEN timeout uygular!
            Eski versiyon sadece 'yield' yapÄ±yordu ve timeout Ã‡ALIÅMIYORDU.
            """
            loop = asyncio.get_running_loop()
            task = asyncio.current_task()
            timed_out = False
            
            def timeout_handler():
                nonlocal timed_out
                timed_out = True
                if task and not task.done():
                    task.cancel()
            
            # Timeout timer'Ä± baÅŸlat
            handle = loop.call_later(seconds, timeout_handler)
            try:
                yield
            except asyncio.CancelledError:
                if timed_out:
                    raise asyncio.TimeoutError(f"Operation timed out after {seconds} seconds")
                raise
            finally:
                handle.cancel()
else:
    asyncio_timeout = asyncio.timeout

from fastapi import WebSocket, WebSocketDisconnect
from starlette.websockets import WebSocketState

from core.config import settings
from core.llm_manager import llm_manager
from core.vector_store import vector_store
from core.session_manager import session_manager
from core.stream_buffer import stream_buffer
from agents.orchestrator import orchestrator
from core.model_router import (
    get_model_router,
    ModelSize,
    FeedbackType,
    FeedbackStatus,
    MODEL_CONFIG,
)

# Intent classifier import
try:
    from core.intent_classifier import intent_classifier, QueryIntent, ResponseStrategy
except ImportError:
    intent_classifier = None
    QueryIntent = None
    ResponseStrategy = None

# Web search import
try:
    from tools.web_search_engine import get_search_engine
    web_search_engine = get_search_engine()
except ImportError:
    web_search_engine = None

# Premium modules import
try:
    from core.response_length_manager import ResponseLengthManager
    response_length_manager = ResponseLengthManager()
except ImportError:
    response_length_manager = None

try:
    from core.source_quality_scorer import SourceQualityScorer
    source_quality_scorer = SourceQualityScorer()
except ImportError:
    source_quality_scorer = None

try:
    from core.semantic_query_expander import SemanticQueryExpander
    semantic_query_expander = SemanticQueryExpander()
except ImportError:
    semantic_query_expander = None

try:
    from core.smart_title_generator import SmartTitleGenerator
    smart_title_generator = SmartTitleGenerator()
except ImportError:
    smart_title_generator = None

# === PREMIUM FULL QUALITY MODULES ===

# CRAG - Corrective RAG System
try:
    from core.crag_system import (
        CRAGPipeline, RelevanceGrader, QueryTransformer, HallucinationDetector,
        RelevanceGrade, CorrectionAction, HallucinationRisk, GradedDocument, CRAGResult
    )
    # Lazy initialization - CRAGPipeline requires retriever/generator
    crag_pipeline = None  # Will be initialized when needed
    relevance_grader = RelevanceGrader()
    query_transformer = QueryTransformer()
    hallucination_detector = HallucinationDetector()
    CRAG_AVAILABLE = True
    logging.info("âœ… CRAG System loaded")
except ImportError as e:
    crag_pipeline = None
    relevance_grader = None
    query_transformer = None
    hallucination_detector = None
    CRAG_AVAILABLE = False
    logging.warning(f"âš ï¸ CRAG System not available: {e}")

# MoE Router - Mixture of Experts
try:
    from core.moe_router import (
        MoERouter, AdaptiveMoERouter, QueryAnalyzer as MoEQueryAnalyzer,
        ExpertType, QueryComplexity as MoEComplexity, RoutingStrategy,
        RoutingDecision, RoutingResult
    )
    moe_router = AdaptiveMoERouter(strategy=RoutingStrategy.BALANCED)
    moe_query_analyzer = MoEQueryAnalyzer()
    MOE_AVAILABLE = True
    logging.info("âœ… MoE Router loaded")
except ImportError as e:
    moe_router = None
    moe_query_analyzer = None
    MOE_AVAILABLE = False
    logging.warning(f"âš ï¸ MoE Router not available: {e}")

# Multi-Agent Debate System
try:
    from core.multi_agent_debate import (
        DebateOrchestrator, DebateAgent,
        AgentRole, DebatePhase, VoteType, Argument, DebateResult,
        multi_agent_debate
    )
    # Lazy initialization - DebateOrchestrator requires llm_factory
    debate_orchestrator = None  # Will be initialized when needed
    DEBATE_AVAILABLE = True
    logging.info("âœ… Multi-Agent Debate loaded")
except ImportError as e:
    debate_orchestrator = None
    DEBATE_AVAILABLE = False
    logging.warning(f"âš ï¸ Multi-Agent Debate not available: {e}")

# MemGPT-Style Tiered Memory
try:
    from core.memgpt_memory import (
        TieredMemoryManager, CoreMemory, MemoryBlock,
        MemoryType, MemoryPriority
    )
    # Lazy initialization - TieredMemoryManager requires storage
    memory_manager = None  # Will be initialized when needed
    MEMGPT_AVAILABLE = True
    logging.info("âœ… MemGPT Memory loaded")
except ImportError as e:
    memory_manager = None
    MEMGPT_AVAILABLE = False
    logging.warning(f"âš ï¸ MemGPT Memory not available: {e}")

# RAGAS Evaluation System
try:
    from core.ragas_evaluation import (
        RAGASEvaluator, quick_evaluate,
        MetricType, EvaluationSample, EvaluationResult
    )
    ragas_evaluator = RAGASEvaluator()
    RAGAS_AVAILABLE = True
    logging.info("âœ… RAGAS Evaluation loaded")
except ImportError as e:
    ragas_evaluator = None
    RAGAS_AVAILABLE = False
    logging.warning(f"âš ï¸ RAGAS Evaluation not available: {e}")

# Feature Flags System
try:
    from core.config import FeatureFlags, feature_enabled
    FEATURE_FLAGS_AVAILABLE = True
except ImportError:
    FEATURE_FLAGS_AVAILABLE = False
    def feature_enabled(flag): return True

# Services import
try:
    from services.query_analyzer import query_analyzer, QueryComplexity, QueryType
    from services.rag_service import rag_service, EnrichedContext
    from services.websocket_service import ws_service, WSPhase, WSMessageType, WSErrorCode, MessageBuilder
    from services.routing_service import routing_service
    SERVICES_AVAILABLE = True
except ImportError as e:
    logging.warning(f"Services not available: {e}")
    query_analyzer = None
    rag_service = None
    ws_service = None
    routing_service = None
    SERVICES_AVAILABLE = False

logger = logging.getLogger(__name__)

# =============================================================================
# CONFIGURATION (from settings)
# =============================================================================

PING_INTERVAL: int = settings.WS_PING_INTERVAL      # Keepalive ping aralÄ±ÄŸÄ± (saniye)
STREAM_TIMEOUT: int = settings.WS_STREAM_TIMEOUT    # Maksimum yanÄ±t sÃ¼resi (saniye)
RAG_SEARCH_TIMEOUT: int = settings.WS_RAG_SEARCH_TIMEOUT  # RAG search timeout
WEB_SEARCH_TIMEOUT: int = settings.WS_WEB_SEARCH_TIMEOUT  # Web search timeout
MODEL_ROUTING_TIMEOUT: int = settings.WS_MODEL_ROUTING_TIMEOUT  # Model routing timeout
RATE_LIMIT_WINDOW: int = 5       # Rate limit penceresi (saniye)
RATE_LIMIT_MAX: int = 10         # Pencere iÃ§inde maksimum istek
MAX_MESSAGE_SIZE: int = settings.WS_MAX_MESSAGE_SIZE  # Maksimum mesaj boyutu
MAX_CONNECTIONS: int = 100       # Maksimum eÅŸzamanlÄ± baÄŸlantÄ±


# =============================================================================
# DATA CLASSES
# =============================================================================

@dataclass
class ClientConnection:
    """WebSocket client baÄŸlantÄ± bilgisi."""
    client_id: str
    websocket: WebSocket
    connected_at: float = field(default_factory=time.time)
    last_activity: float = field(default_factory=time.time)
    request_times: List[float] = field(default_factory=list)
    total_requests: int = 0
    total_tokens: int = 0
    is_streaming: bool = False
    stop_flag: bool = False
    session_id: Optional[str] = None
    current_stream_id: Optional[str] = None  # Aktif stream ID
    active_agents: Dict[str, Any] = field(default_factory=dict)  # Aktif agent'lar
    
    @property
    def connection_duration(self) -> float:
        return time.time() - self.connected_at


@dataclass
class StreamStats:
    """Streaming istatistikleri."""
    start_time: float
    end_time: Optional[float] = None
    token_count: int = 0
    char_count: int = 0
    error: Optional[str] = None
    was_stopped: bool = False
    
    @property
    def duration_ms(self) -> int:
        end = self.end_time or time.time()
        return int((end - self.start_time) * 1000)
    
    @property
    def tokens_per_second(self) -> float:
        duration = (self.end_time or time.time()) - self.start_time
        return round(self.token_count / duration, 1) if duration > 0 else 0
    
    def to_dict(self) -> dict:
        return {
            "duration_ms": self.duration_ms,
            "tokens": self.token_count,
            "chars": self.char_count,
            "tokens_per_second": self.tokens_per_second,
            "was_stopped": self.was_stopped,
        }


# =============================================================================
# CONNECTION MANAGER v2
# =============================================================================

class WebSocketManagerV2:
    """
    Enterprise WebSocket Connection Manager.
    
    MyChatbot'tan daha geliÅŸmiÅŸ:
    - Connection pooling
    - DetaylÄ± metriks
    - Broadcast desteÄŸi
    - Room/group desteÄŸi
    """
    
    def __init__(self):
        self._connections: Dict[str, ClientConnection] = {}
        self._rooms: Dict[str, Set[str]] = {}  # room_id -> client_ids
        self._lock = asyncio.Lock()
        self._stats = {
            "total_connections": 0,
            "total_messages": 0,
            "total_errors": 0,
        }
    
    @property
    def active_count(self) -> int:
        return len(self._connections)
    
    async def connect(self, websocket: WebSocket, client_id: str) -> ClientConnection:
        """Yeni baÄŸlantÄ± kabul et."""
        async with self._lock:
            # Maksimum baÄŸlantÄ± kontrolÃ¼
            if len(self._connections) >= MAX_CONNECTIONS:
                await websocket.close(code=1013, reason="Server overloaded")
                raise ConnectionError("Maximum connections reached")
            
            # Mevcut baÄŸlantÄ± varsa kapat
            if client_id in self._connections:
                old_conn = self._connections[client_id]
                try:
                    await old_conn.websocket.close(code=1000, reason="New connection")
                except Exception:
                    pass  # Ignore errors when closing old connection
            
            # BaÄŸlantÄ±yÄ± kabul et
            await websocket.accept()
            
            # Client connection oluÅŸtur
            conn = ClientConnection(
                client_id=client_id,
                websocket=websocket
            )
            self._connections[client_id] = conn
            self._stats["total_connections"] += 1
            
            logger.info(f"WebSocket connected: {client_id}, total: {self.active_count}")
            return conn
    
    async def disconnect(self, client_id: str) -> Optional[ClientConnection]:
        """BaÄŸlantÄ±yÄ± kapat ve temizle."""
        async with self._lock:
            if client_id in self._connections:
                conn = self._connections.pop(client_id)
                
                # Room'lardan Ã§Ä±kar
                for room_clients in self._rooms.values():
                    room_clients.discard(client_id)
                
                logger.info(
                    f"WebSocket disconnected: {client_id}, "
                    f"duration: {conn.connection_duration:.1f}s, "
                    f"requests: {conn.total_requests}"
                )
                return conn
            return None
    
    def get_connection(self, client_id: str) -> Optional[ClientConnection]:
        """Client baÄŸlantÄ±sÄ±nÄ± al."""
        return self._connections.get(client_id)
    
    async def send(self, client_id: str, data: dict) -> bool:
        """Belirli bir client'a mesaj gÃ¶nder."""
        conn = self._connections.get(client_id)
        if not conn or conn.websocket.client_state != WebSocketState.CONNECTED:
            return False
        
        try:
            await conn.websocket.send_json(data)
            conn.last_activity = time.time()
            self._stats["total_messages"] += 1
            return True
        except Exception as e:
            logger.debug(f"Send error to {client_id}: {e}")
            self._stats["total_errors"] += 1
            return False
    
    async def broadcast(self, data: dict, exclude: Optional[Set[str]] = None) -> int:
        """TÃ¼m baÄŸlÄ± client'lara mesaj gÃ¶nder."""
        exclude = exclude or set()
        sent = 0
        for client_id in list(self._connections.keys()):
            if client_id not in exclude:
                if await self.send(client_id, data):
                    sent += 1
        return sent
    
    async def send_to_room(self, room_id: str, data: dict) -> int:
        """Room'daki tÃ¼m client'lara mesaj gÃ¶nder."""
        if room_id not in self._rooms:
            return 0
        sent = 0
        for client_id in self._rooms[room_id]:
            if await self.send(client_id, data):
                sent += 1
        return sent
    
    def join_room(self, client_id: str, room_id: str) -> None:
        """Client'Ä± room'a ekle."""
        if room_id not in self._rooms:
            self._rooms[room_id] = set()
        self._rooms[room_id].add(client_id)
    
    def leave_room(self, client_id: str, room_id: str) -> None:
        """Client'Ä± room'dan Ã§Ä±kar."""
        if room_id in self._rooms:
            self._rooms[room_id].discard(client_id)
    
    def check_rate_limit(self, client_id: str) -> bool:
        """Rate limiting kontrolÃ¼."""
        conn = self._connections.get(client_id)
        if not conn:
            return False
        
        now = time.time()
        # Eski istekleri temizle
        conn.request_times = [t for t in conn.request_times if now - t < RATE_LIMIT_WINDOW]
        
        if len(conn.request_times) >= RATE_LIMIT_MAX:
            return False
        
        conn.request_times.append(now)
        return True
    
    def get_stats(self) -> dict:
        """Manager istatistiklerini al."""
        return {
            "active_connections": self.active_count,
            "total_connections": self._stats["total_connections"],
            "total_messages": self._stats["total_messages"],
            "total_errors": self._stats["total_errors"],
            "rooms": len(self._rooms),
        }
    
    def get_clients_info(self) -> List[dict]:
        """TÃ¼m client'larÄ±n bilgisini al."""
        return [
            {
                "client_id": conn.client_id,
                "connected_at": datetime.fromtimestamp(conn.connected_at).isoformat(),
                "duration_seconds": int(conn.connection_duration),
                "total_requests": conn.total_requests,
                "is_streaming": conn.is_streaming,
                "session_id": conn.session_id,
            }
            for conn in self._connections.values()
        ]


# Global manager instance
ws_manager = WebSocketManagerV2()


# =============================================================================
# WEBSOCKET HANDLER v2
# =============================================================================

class WebSocketHandlerV2:
    """
    WebSocket mesaj iÅŸleyici.
    
    Her baÄŸlantÄ± iÃ§in ayrÄ± bir handler instance'Ä±.
    """
    
    def __init__(self, conn: ClientConnection, manager: WebSocketManagerV2):
        self.conn = conn
        self.manager = manager
        self._ping_task: Optional[asyncio.Task] = None
        self._stream_task: Optional[asyncio.Task] = None
    
    async def start(self) -> None:
        """Handler'Ä± baÅŸlat."""
        # HoÅŸ geldin mesajÄ±
        await self._send({
            "type": "connected",
            "client_id": self.conn.client_id,
            "ts": int(time.time() * 1000),
            "server_time": datetime.now().isoformat(),
        })
        
        # Keepalive baÅŸlat
        self._ping_task = asyncio.create_task(self._keepalive_loop())
    
    async def stop(self) -> None:
        """Handler'Ä± durdur."""
        self.conn.stop_flag = True
        
        # Task'larÄ± iptal et
        for task in [self._stream_task, self._ping_task]:
            if task and not task.done():
                task.cancel()
                try:
                    await task
                except asyncio.CancelledError:
                    pass
    
    async def handle_message(self, data: dict) -> None:
        """Gelen mesajÄ± iÅŸle."""
        msg_type = data.get("type", "chat")
        
        if msg_type == "ping":
            await self._send({
                "type": "pong",
                "ts": int(time.time() * 1000)
            })
        
        elif msg_type == "pong":
            # Client'tan gelen pong - keepalive onayÄ±, sessizce yoksay
            self.conn.last_activity = time.time()
        
        elif msg_type == "stop":
            await self._handle_stop()
        
        elif msg_type == "resume":
            await self._handle_resume(data)
        
        elif msg_type == "chat":
            await self._handle_chat(data)
        
        elif msg_type == "message":
            # Routing destekli mesaj (frontend'den gelen format)
            await self._handle_routed_message(data)
        
        elif msg_type == "agent":
            # Autonomous Agent modu
            await self._handle_agent_task(data)
        
        elif msg_type == "feedback":
            await self._handle_feedback(data)
        
        elif msg_type == "compare":
            await self._handle_compare(data)
        
        elif msg_type == "confirm":
            await self._handle_confirm(data)
        
        # Bilinmeyen mesaj tipleri sessizce yoksayÄ±lÄ±r (hata gÃ¶nderme!)
    
    async def _handle_resume(self, data: dict) -> None:
        """
        Resume komutunu iÅŸle - kaldÄ±ÄŸÄ± yerden devam et.
        
        Client reconnect ettikten sonra eksik token'larÄ± alÄ±r.
        """
        stream_id = data.get("stream_id")
        from_index = data.get("from_index", 0)
        
        if not stream_id:
            # Session'Ä±n aktif stream'ini bul
            session_id = data.get("session_id") or self.conn.session_id
            if session_id:
                stream = stream_buffer.get_active_stream(session_id)
                if stream:
                    stream_id = stream.stream_id
        
        if not stream_id:
            await self._send({
                "type": "error",
                "code": "no_stream",
                "message": "Devam edilecek stream bulunamadÄ±"
            })
            return
        
        # Resume verisini al
        resume_data = stream_buffer.get_resume_data(stream_id, from_index)
        
        if "error" in resume_data:
            await self._send({
                "type": "error",
                "code": "stream_not_found",
                "message": "Stream bulunamadÄ±"
            })
            return
        
        # Resume verisini gÃ¶nder
        await self._send({
            "type": "resume_data",
            **resume_data
        })
        
        # EÄŸer stream hala aktifse, yeni token'larÄ± canlÄ± olarak gÃ¶ndermeye devam et
        stream = stream_buffer.get_stream(stream_id)
        if stream and stream.is_active:
            self.conn.current_stream_id = stream_id
            # Stream'i takip et
            asyncio.create_task(self._follow_stream(stream_id, stream.token_count))
    
    async def _follow_stream(self, stream_id: str, last_sent_index: int) -> None:
        """
        Aktif bir stream'i takip et ve yeni token'larÄ± gÃ¶nder.
        
        Resume sonrasÄ± veya reconnect durumunda kullanÄ±lÄ±r.
        """
        while True:
            stream = stream_buffer.get_stream(stream_id)
            if not stream:
                break
            
            # Yeni token'lar var mÄ±?
            if stream.token_count > last_sent_index:
                new_tokens = stream.get_tokens_from(last_sent_index)
                for token in new_tokens:
                    await self._send({
                        "type": "token",
                        "content": token.content,
                        "index": token.index
                    })
                last_sent_index = stream.token_count
            
            # Stream tamamlandÄ± mÄ±?
            if not stream.is_active:
                if stream.status == "completed":
                    await self._send({
                        "type": "end",
                        "stats": {
                            "duration_ms": stream.duration_ms,
                            "tokens": stream.token_count,
                        }
                    })
                elif stream.status == "stopped":
                    await self._send({
                        "type": "stopped",
                        "elapsed_ms": stream.duration_ms,
                        "tokens": stream.token_count,
                    })
                elif stream.status == "error":
                    await self._send({
                        "type": "error",
                        "message": stream.error or "Stream hatasÄ±"
                    })
                break
            
            # KÄ±sa bekle
            await asyncio.sleep(0.05)
    
    async def _handle_stop(self) -> None:
        """Stop komutunu iÅŸle - o ana kadar yazÄ±lanlarÄ± koru."""
        self.conn.stop_flag = True
        
        # Stream buffer'a stop isteÄŸi gÃ¶nder
        if self.conn.session_id:
            stream_buffer.request_stop(self.conn.session_id)
        
        # Task'Ä± cancel etmiyoruz - streaming loop stop_flag'i kontrol edecek
        # ve graceful olarak duracak, bÃ¶ylece o ana kadar yazÄ±lanlar korunur
        # Stream task kendi "stopped" mesajÄ±nÄ± gÃ¶nderecek
        
        # Sadece stream aktif deÄŸilse burada "stopped" gÃ¶nder
        if not self._stream_task or self._stream_task.done():
            await self._send({"type": "stopped", "ts": int(time.time() * 1000)})
    
    async def _handle_chat(self, data: dict) -> None:
        """Chat mesajÄ±nÄ± iÅŸle."""
        # Rate limiting
        if not self.manager.check_rate_limit(self.conn.client_id):
            await self._send({
                "type": "error",
                "code": "rate_limited",
                "message": f"Ã‡ok fazla istek. {RATE_LIMIT_WINDOW} saniye bekleyin.",
                "retry_after": RATE_LIMIT_WINDOW
            })
            return
        
        message = data.get("message", "").strip()
        if not message:
            await self._send({
                "type": "error",
                "code": "empty_message",
                "message": "Mesaj boÅŸ olamaz"
            })
            return
        
        session_id = data.get("session_id") or str(uuid.uuid4())
        self.conn.session_id = session_id
        
        # ğŸ” DEBUG: Log incoming chat request
        logger.info(f"ğŸ“¨ [CHAT DEBUG] Received chat request:")
        logger.info(f"   - session_id from frontend: {data.get('session_id')}")
        logger.info(f"   - effective session_id: {session_id}")
        logger.info(f"   - message preview: {message[:50]}...")
        logger.info(f"   - use_routing: {data.get('use_routing')}")
        
        # Model routing modu - frontend'den gelen use_routing parametresini kontrol et
        use_routing = data.get("use_routing", False)
        force_model = data.get("force_model")
        
        # Web search modu
        web_search = data.get("web_search", False)
        response_mode = data.get("response_mode", "normal")
        complexity_level = data.get("complexity_level", "auto")  # auto, simple, moderate, advanced, comprehensive
        
        # Ã–nceki stream'i iptal et
        if self._stream_task and not self._stream_task.done():
            self._stream_task.cancel()
            try:
                await self._stream_task
            except asyncio.CancelledError:
                pass
        
        # Yeni stream baÅŸlat
        self.conn.stop_flag = False
        self.conn.is_streaming = True
        self.conn.total_requests += 1
        
        # use_routing=true ise model routing kullan, yoksa normal stream
        if use_routing:
            self._stream_task = asyncio.create_task(
                self._stream_routed_response(
                    message, session_id, use_routing, force_model,
                    web_search, complexity_level, response_mode
                )
            )
        else:
            self._stream_task = asyncio.create_task(
                self._stream_response(message, session_id, web_search, response_mode, complexity_level)
            )
    
    async def _handle_routed_message(self, data: dict) -> None:
        """
        Model routing destekli mesaj iÅŸleyici.
        
        Frontend'den gelen format:
        {
            "type": "message",
            "content": "...",
            "use_routing": true,
            "session_id": "..."
        }
        """
        # Rate limiting
        if not self.manager.check_rate_limit(self.conn.client_id):
            await self._send({
                "type": "error",
                "code": "rate_limited",
                "message": f"Ã‡ok fazla istek. {RATE_LIMIT_WINDOW} saniye bekleyin.",
                "retry_after": RATE_LIMIT_WINDOW
            })
            return
        
        message = data.get("content", "").strip()
        if not message:
            await self._send({
                "type": "error",
                "code": "empty_message",
                "message": "Mesaj boÅŸ olamaz"
            })
            return
        
        session_id = data.get("session_id") or str(uuid.uuid4())
        self.conn.session_id = session_id
        use_routing = data.get("use_routing", True)
        force_model = data.get("force_model")
        
        # Yeni parametreler
        web_search = data.get("web_search", False)
        complexity_level = data.get("complexity_level", "auto")
        response_mode = data.get("response_mode", "normal")
        
        # Ã–nceki stream'i iptal et
        if self._stream_task and not self._stream_task.done():
            self._stream_task.cancel()
            try:
                await self._stream_task
            except asyncio.CancelledError:
                pass
        
        # Yeni stream baÅŸlat
        self.conn.stop_flag = False
        self.conn.is_streaming = True
        self.conn.total_requests += 1
        
        self._stream_task = asyncio.create_task(
            self._stream_routed_response(
                message, session_id, use_routing, force_model,
                web_search, complexity_level, response_mode
            )
        )
    
    async def _handle_feedback(self, data: dict) -> None:
        """
        KullanÄ±cÄ± feedback'ini iÅŸle.
        
        {
            "type": "feedback",
            "response_id": "...",
            "feedback_type": "correct" | "downgrade" | "upgrade"
        }
        """
        response_id = data.get("response_id")
        feedback_type = data.get("feedback_type")
        
        if not response_id or not feedback_type:
            await self._send({
                "type": "error",
                "code": "missing_fields",
                "message": "response_id ve feedback_type gerekli"
            })
            return
        
        try:
            fb_type = FeedbackType(feedback_type)
            
            # Suggested model
            suggested_model = None
            if fb_type == FeedbackType.DOWNGRADE:
                suggested_model = ModelSize.SMALL
            elif fb_type == FeedbackType.UPGRADE:
                suggested_model = ModelSize.LARGE
            
            model_router = get_model_router()
            feedback = model_router.submit_feedback(
                response_id=response_id,
                feedback_type=fb_type,
                suggested_model=suggested_model,
            )
            
            # YanÄ±t
            requires_comparison = fb_type != FeedbackType.CORRECT
            
            if fb_type == FeedbackType.CORRECT:
                message_text = "âœ… TeÅŸekkÃ¼rler! Tercih kaydedildi."
            elif fb_type == FeedbackType.DOWNGRADE:
                message_text = "ğŸ”„ KÃ¼Ã§Ã¼k modeli denemek iÃ§in 'Dene' butonunu kullanÄ±n."
            else:
                message_text = "ğŸ”„ BÃ¼yÃ¼k modeli denemek iÃ§in 'Dene' butonunu kullanÄ±n."
            
            await self._send({
                "type": "feedback_received",
                "feedback": feedback.to_dict(),
                "message": message_text,
                "requires_comparison": requires_comparison,
                "status": feedback.status.value,
                "timestamp": datetime.now().isoformat()
            })
            
        except ValueError as e:
            await self._send({
                "type": "error",
                "code": "invalid_feedback",
                "message": str(e)
            })
    
    async def _handle_compare(self, data: dict) -> None:
        """
        Model karÅŸÄ±laÅŸtÄ±rma isteÄŸini iÅŸle.
        
        {
            "type": "compare",
            "response_id": "...",
            "query": "..."
        }
        """
        response_id = data.get("response_id")
        query = data.get("query", "")
        
        if not response_id:
            await self._send({
                "type": "error",
                "code": "missing_fields",
                "message": "response_id gerekli"
            })
            return
        
        try:
            model_router = get_model_router()
            
            # Response'dan original model bilgisini al
            response = model_router.storage.get_response(response_id)
            if not response:
                await self._send({
                    "type": "error",
                    "code": "response_not_found",
                    "message": "Response bulunamadÄ±"
                })
                return
            
            original_model_size = ModelSize(response["model_size"])
            
            # Alternatif modeli belirle
            if original_model_size == ModelSize.LARGE:
                comparison_model = MODEL_CONFIG[ModelSize.SMALL]["name"]
                comparison_display = MODEL_CONFIG[ModelSize.SMALL]["display_name"]
                comparison_icon = MODEL_CONFIG[ModelSize.SMALL]["icon"]
            else:
                comparison_model = MODEL_CONFIG[ModelSize.LARGE]["name"]
                comparison_display = MODEL_CONFIG[ModelSize.LARGE]["display_name"]
                comparison_icon = MODEL_CONFIG[ModelSize.LARGE]["icon"]
            
            # KarÅŸÄ±laÅŸtÄ±rma baÅŸlangÄ±cÄ±
            await self._send({
                "type": "compare_start",
                "response_id": response_id,
                "model": comparison_model,
                "model_display_name": comparison_display,
                "model_icon": comparison_icon,
                "timestamp": datetime.now().isoformat()
            })
            
            # Query'yi al
            if not query:
                query = response.get("query", "Merhaba")
            
            # Alternatif modelden yanÄ±t Ã¼ret
            from core.system_knowledge import SELF_KNOWLEDGE_PROMPT
            system_prompt = SELF_KNOWLEDGE_PROMPT
            
            async for chunk in llm_manager.generate_stream_async(
                prompt=query,
                system_prompt=system_prompt,
                temperature=0.7,
                model=comparison_model,
            ):
                if self.conn.stop_flag:
                    break
                
                await self._send({
                    "type": "compare_chunk",
                    "content": chunk,
                    "timestamp": datetime.now().isoformat()
                })
            
            await self._send({
                "type": "compare_end",
                "response_id": response_id,
                "model": comparison_model,
                "timestamp": datetime.now().isoformat()
            })
            
        except Exception as e:
            logger.error(f"Compare error: {e}")
            await self._send({
                "type": "error",
                "code": "compare_failed",
                "message": str(e)[:200]
            })
    
    async def _handle_confirm(self, data: dict) -> None:
        """
        Feedback onayÄ±nÄ± iÅŸle.
        
        {
            "type": "confirm",
            "feedback_id": "...",
            "confirmed": true/false,
            "selected_model": "small"/"large"
        }
        """
        feedback_id = data.get("feedback_id")
        confirmed = data.get("confirmed", False)
        selected_model = data.get("selected_model")
        
        if not feedback_id:
            await self._send({
                "type": "error",
                "code": "missing_fields",
                "message": "feedback_id gerekli"
            })
            return
        
        try:
            model_router = get_model_router()
            
            # Final decision'Ä± belirle
            final_decision = None
            if selected_model:
                final_decision = ModelSize.SMALL if selected_model == "small" else ModelSize.LARGE
            
            feedback = model_router.confirm_feedback(
                feedback_id=feedback_id,
                confirmed=confirmed,
                final_decision=final_decision,
            )
            
            if confirmed:
                model_config = MODEL_CONFIG.get(feedback.final_decision, {})
                model_name = model_config.get("display_name", "Model")
                message_text = f"âœ… Tercih kaydedildi! Benzer sorgular iÃ§in {model_name} kullanÄ±lacak."
                learning_applied = True
            else:
                message_text = "â†©ï¸ Ä°lk tercih korundu. TeÅŸekkÃ¼rler!"
                learning_applied = False
            
            await self._send({
                "type": "feedback_confirmed",
                "feedback": feedback.to_dict(),
                "message": message_text,
                "learning_applied": learning_applied,
                "timestamp": datetime.now().isoformat()
            })
            
        except ValueError as e:
            await self._send({
                "type": "error",
                "code": "confirm_failed",
                "message": str(e)
            })
    
    async def _handle_agent_task(self, data: dict) -> None:
        """
        Autonomous Agent gÃ¶revini WebSocket Ã¼zerinden iÅŸle.
        
        Mesaj formatÄ±:
        {
            "type": "agent",
            "action": "create" | "start" | "respond" | "cancel",
            "task_id": str (optional),
            "goal": str (create iÃ§in),
            "context": dict (optional),
            "response": str (respond iÃ§in),
            "intervention_type": str (respond iÃ§in)
        }
        """
        from agents.autonomous_agent import (
            StreamingAutonomousAgent, AgentTask, HumanIntervention, InterventionType
        )
        
        action = data.get("action", "create")
        task_id = data.get("task_id")
        
        try:
            if action == "create":
                # Yeni gÃ¶rev oluÅŸtur
                goal = data.get("goal", "")
                context = data.get("context", {})
                
                if not goal:
                    await self._send({
                        "type": "agent_error",
                        "code": "missing_goal",
                        "message": "GÃ¶rev hedefi belirtilmedi"
                    })
                    return
                
                # Streaming agent oluÅŸtur
                agent = StreamingAutonomousAgent(max_steps=10, max_retries=3)
                
                # GÃ¶rev oluÅŸtur
                task = await agent.create_task(goal, context)
                
                # Agent'Ä± sakla (session bazlÄ±)
                if not hasattr(self.conn, 'active_agents'):
                    self.conn.active_agents = {}
                self.conn.active_agents[task.id] = agent
                
                # GÃ¶rev oluÅŸturuldu bilgisi
                await self._send({
                    "type": "agent_task_created",
                    "task": {
                        "task_id": task.id,
                        "goal": task.user_request,
                        "status": task.status.value,
                        "created_at": task.created_at.isoformat()
                    },
                    "message": f"ğŸ¯ GÃ¶rev oluÅŸturuldu: {task.user_request[:100]}..."
                })
                
                # Otomatik olarak gÃ¶revi baÅŸlat ve planla
                await self._send({
                    "type": "agent_planning",
                    "task_id": task.id,
                    "message": "ğŸ“‹ GÃ¶rev planlanÄ±yor..."
                })
                
                # GÃ¶revi planla
                plan = await agent.plan_task(task)
                
                # Plan bilgisini gÃ¶nder
                await self._send({
                    "type": "agent_plan_ready",
                    "task_id": task.id,
                    "plan": {
                        "total_steps": plan.total_steps,
                        "steps": [
                            {
                                "step_number": step.step_number,
                                "description": step.description,
                                "tool_name": step.tool_name
                            }
                            for step in plan.steps
                        ]
                    },
                    "message": f"âœ… Plan hazÄ±r: {plan.total_steps} adÄ±m"
                })
                
                # GÃ¶revi stream olarak Ã§alÄ±ÅŸtÄ±r
                await self._execute_agent_task_streaming(agent, task)
                
            elif action == "start":
                # Bekleyen gÃ¶revi baÅŸlat
                if not task_id or task_id not in getattr(self.conn, 'active_agents', {}):
                    await self._send({
                        "type": "agent_error",
                        "code": "task_not_found",
                        "message": "GÃ¶rev bulunamadÄ±"
                    })
                    return
                
                agent = self.conn.active_agents[task_id]
                task = agent._tasks.get(task_id)
                
                if task:
                    await self._execute_agent_task_streaming(agent, task)
                    
            elif action == "respond":
                # Ä°nsan mÃ¼dahalesine yanÄ±t
                if not task_id or task_id not in getattr(self.conn, 'active_agents', {}):
                    await self._send({
                        "type": "agent_error",
                        "code": "task_not_found",
                        "message": "GÃ¶rev bulunamadÄ±"
                    })
                    return
                
                agent = self.conn.active_agents[task_id]
                response = data.get("response", "")
                
                # YanÄ±tÄ± iÅŸle
                if hasattr(agent, 'pending_intervention') and agent.pending_intervention:
                    intervention = agent.pending_intervention
                    intervention.response = response
                    intervention.responded = True
                    agent.pending_intervention = None
                    
                    await self._send({
                        "type": "agent_intervention_response",
                        "task_id": task_id,
                        "response": response,
                        "message": "âœ… YanÄ±t alÄ±ndÄ±, devam ediliyor..."
                    })
                    
            elif action == "cancel":
                # GÃ¶revi iptal et
                if task_id and task_id in getattr(self.conn, 'active_agents', {}):
                    agent = self.conn.active_agents[task_id]
                    task = agent._tasks.get(task_id)
                    if task:
                        from agents.autonomous_agent import TaskStatus
                        task.status = TaskStatus.CANCELLED
                        
                    del self.conn.active_agents[task_id]
                    
                    await self._send({
                        "type": "agent_task_cancelled",
                        "task_id": task_id,
                        "message": "âŒ GÃ¶rev iptal edildi"
                    })
                    
        except Exception as e:
            logger.error(f"Agent task error: {e}")
            await self._send({
                "type": "agent_error",
                "code": "agent_failed",
                "message": str(e)
            })
    
    async def _execute_agent_task_streaming(self, agent, task) -> None:
        """Agent gÃ¶revini streaming olarak Ã§alÄ±ÅŸtÄ±r."""
        from agents.autonomous_agent import TaskStatus, StepStatus
        
        task_id = task.id
        
        try:
            # Plan kontrolÃ¼
            if not task.plan or not task.plan.steps:
                await self._send({
                    "type": "agent_error",
                    "task_id": task_id,
                    "code": "no_plan",
                    "message": "âŒ GÃ¶rev planÄ± bulunamadÄ±"
                })
                return
            
            # GÃ¶revi baÅŸlat
            await self._send({
                "type": "agent_executing",
                "task_id": task_id,
                "message": "ğŸš€ GÃ¶rev Ã§alÄ±ÅŸtÄ±rÄ±lÄ±yor..."
            })
            
            # Her adÄ±mÄ± Ã§alÄ±ÅŸtÄ±r
            for step in task.plan.steps:
                if task.status == TaskStatus.CANCELLED:
                    break
                    
                # AdÄ±m baÅŸladÄ±
                await self._send({
                    "type": "agent_step_start",
                    "task_id": task_id,
                    "step": {
                        "step_number": step.step_number,
                        "description": step.description,
                        "tool_name": step.tool_name
                    },
                    "message": f"âš™ï¸ AdÄ±m {step.step_number}/{task.plan.total_steps}: {step.description}"
                })
                
                # AdÄ±mÄ± Ã§alÄ±ÅŸtÄ±r
                try:
                    success, result = await agent.execute_step(task, step)
                    
                    # AdÄ±m tamamlandÄ±
                    await self._send({
                        "type": "agent_step_complete",
                        "task_id": task_id,
                        "step": {
                            "step_number": step.step_number,
                            "status": step.status.value,
                            "success": success,
                            "result": str(result)[:500] if result else None
                        },
                        "message": f"âœ… AdÄ±m {step.step_number} tamamlandÄ±"
                    })
                    
                except Exception as step_error:
                    # AdÄ±m baÅŸarÄ±sÄ±z
                    await self._send({
                        "type": "agent_step_failed",
                        "task_id": task_id,
                        "step": {
                            "step_number": step.step_number,
                            "error": str(step_error)
                        },
                        "message": f"âŒ AdÄ±m {step.step_number} baÅŸarÄ±sÄ±z: {step_error}"
                    })
                    
                    # Self-correction dene
                    if step.attempts < step.max_attempts:
                        await self._send({
                            "type": "agent_self_correction",
                            "task_id": task_id,
                            "retry": step.attempts,
                            "message": f"ğŸ”„ DÃ¼zeltme deneniyor ({step.attempts}/{step.max_attempts})..."
                        })
                
                # KÃ¼Ã§Ã¼k gecikme
                await asyncio.sleep(0.1)
            
            # GÃ¶rev tamamlandÄ±
            task.status = TaskStatus.COMPLETED
            task.completed_at = datetime.now()
            
            # Sonucu oluÅŸtur
            await agent._generate_summary(task)
            final_result = task.final_result or "GÃ¶rev tamamlandÄ±"
            
            progress = task.plan.get_progress() if task.plan else {"completed": 0}
            await self._send({
                "type": "agent_task_complete",
                "task_id": task_id,
                "result": final_result,
                "stats": {
                    "total_steps": task.plan.total_steps if task.plan else 0,
                    "completed_steps": progress.get("completed", 0),
                    "duration": (task.completed_at - task.created_at).total_seconds() if task.completed_at else 0
                },
                "message": "ğŸ‰ GÃ¶rev baÅŸarÄ±yla tamamlandÄ±!"
            })
            
        except Exception as e:
            task.status = TaskStatus.FAILED
            task.error = str(e)
            
            await self._send({
                "type": "agent_task_failed",
                "task_id": task_id,
                "error": str(e),
                "message": f"ğŸ’¥ GÃ¶rev baÅŸarÄ±sÄ±z: {e}"
            })
    
    async def _stream_routed_response(
        self,
        message: str,
        session_id: str,
        use_routing: bool = True,
        force_model: Optional[str] = None,
        web_search: bool = False,
        complexity_level: str = "auto",
        response_mode: str = "normal",
    ) -> None:
        """
        Model routing ile streaming yanÄ±t Ã¼ret.
        
        Args:
            message: KullanÄ±cÄ± mesajÄ±
            session_id: Session ID
            use_routing: Model routing kullanÄ±lsÄ±n mÄ±
            force_model: Zorla belirli model kullan
            web_search: Web aramasÄ± yapÄ±lsÄ±n mÄ±
            complexity_level: simple/normal/comprehensive/research
            response_mode: normal/analytical/creative/technical
        """
        stats = StreamStats(start_time=time.time())
        
        # Stream buffer'da yeni stream oluÅŸtur
        stream = stream_buffer.create_stream(session_id, message)
        stream_id = stream.stream_id
        self.conn.current_stream_id = stream_id
        
        try:
            model_router = get_model_router()
            
            # BaÅŸlangÄ±Ã§ mesajÄ±
            await self._send({
                "type": "start",
                "ts": int(time.time() * 1000),
                "session_id": session_id,
                "stream_id": stream_id,
            })
            
            # âš¡ Basit sorgu tespiti - ROUTING'DEN Ã–NCE kontrol et (query_analyzer service)
            is_simple_query = query_analyzer.is_simple_query(message)
            
            # Complexity override - simple mode forced
            if complexity_level == "simple":
                is_simple_query = True
            elif complexity_level in ["comprehensive", "research"]:
                is_simple_query = False
            
            # === PHASE 1: ROUTING ===
            await self._send({
                "type": "status",
                "message": "âš¡ HÄ±zlÄ± yanÄ±t..." if is_simple_query else "Model seÃ§iliyor...",
                "phase": "routing"
            })
            
            if force_model:
                # Zorla belirtilen model - 'small' veya 'large' string olarak gelebilir
                if force_model == "small":
                    actual_model = MODEL_CONFIG[ModelSize.SMALL]["name"]
                    model_size = "small"
                elif force_model == "large":
                    actual_model = MODEL_CONFIG[ModelSize.LARGE]["name"]
                    model_size = "large"
                else:
                    actual_model = force_model
                    model_size = "unknown"
                    
                routing_info = {
                    "model_name": actual_model,
                    "model_size": model_size,
                    "model_icon": MODEL_CONFIG.get(ModelSize.SMALL if model_size == "small" else ModelSize.LARGE, {}).get("icon", "ğŸ¤–"),
                    "model_display_name": MODEL_CONFIG.get(ModelSize.SMALL if model_size == "small" else ModelSize.LARGE, {}).get("display_name", actual_model),
                    "decision_source": "forced",
                    "confidence": 1.0,
                    "response_id": str(uuid.uuid4()),
                }
            elif is_simple_query:
                # âš¡ Basit sorgular iÃ§in routing atla, direkt small model
                routing_info = {
                    "model_name": MODEL_CONFIG[ModelSize.SMALL]["name"],
                    "model_size": "small",
                    "model_icon": MODEL_CONFIG[ModelSize.SMALL]["icon"],
                    "model_display_name": MODEL_CONFIG[ModelSize.SMALL]["display_name"],
                    "decision_source": "simple_query_bypass",
                    "confidence": 1.0,
                    "response_id": str(uuid.uuid4()),
                    "reasoning": "Basit sorgu - hÄ±zlÄ± yanÄ±t modu"
                }
            elif use_routing:
                # Model router kullan
                routing_result = await model_router.route_async(message)
                routing_info = routing_result.to_dict()
            else:
                # Default small model
                routing_info = {
                    "model_name": MODEL_CONFIG[ModelSize.SMALL]["name"],
                    "model_size": "small",
                    "decision_source": "default",
                    "confidence": 1.0,
                    "response_id": str(uuid.uuid4()),
                }
            
            # Routing bilgisini gÃ¶nder
            await self._send({
                "type": "routing",
                "routing_info": routing_info,
                "timestamp": datetime.now().isoformat()
            })
            
            model_name = routing_info["model_name"]
            response_id = routing_info.get("response_id", str(uuid.uuid4()))
            
            # === RAG ARAÅI (KarmaÅŸÄ±k sorgular iÃ§in) ===
            knowledge_context = ""
            sources = []
            web_context = ""
            web_sources = []
            
            # === PREMIUM: MoE Router - Sorgu Analizi ===
            moe_analysis = None
            selected_expert = None
            if MOE_AVAILABLE and moe_query_analyzer and not is_simple_query and feature_enabled('moe_router'):
                try:
                    moe_analysis = moe_query_analyzer.analyze(message)
                    routing_decision = moe_router.route(message)
                    selected_expert = routing_decision.selected_expert
                    
                    # MoE bilgisini gÃ¶nder
                    await self._send({
                        "type": "moe_routing",
                        "complexity": moe_analysis.complexity.value if moe_analysis else "unknown",
                        "expert": selected_expert.value if selected_expert else "default",
                        "confidence": routing_decision.confidence if routing_decision else 0.5,
                        "reasoning": routing_decision.reasoning if routing_decision else ""
                    })
                    logger.info(f"ğŸ”€ MoE: {moe_analysis.complexity.value if moe_analysis else 'N/A'} -> {selected_expert}")
                except Exception as e:
                    logger.warning(f"MoE analysis error: {e}")
            
            # === PREMIUM: MemGPT Memory - KonuÅŸma GeÃ§miÅŸi ===
            memory_context = ""
            if MEMGPT_AVAILABLE and memory_manager and session_id and feature_enabled('memgpt_memory'):
                try:
                    # Session iÃ§in bellek getir
                    relevant_memories = memory_manager.recall(
                        query=message,
                        session_id=session_id,
                        limit=5
                    )
                    if relevant_memories:
                        memory_parts = []
                        for mem in relevant_memories:
                            memory_parts.append(f"[Ã–nceki KonuÅŸma]: {mem.content[:300]}")
                        memory_context = "\n".join(memory_parts)
                        logger.info(f"ğŸ§  MemGPT: {len(relevant_memories)} ilgili bellek bulundu")
                except Exception as e:
                    logger.warning(f"MemGPT recall error: {e}")
            
            if is_simple_query:
                # === PHASE 2: SEARCH (skipped) ===
                await self._send({
                    "type": "status",
                    "message": "âš¡ AtlandÄ±",
                    "phase": "search"
                })
                # === PHASE 3: ANALYZE (skipped) ===
                await self._send({
                    "type": "status",
                    "message": "âš¡ AtlandÄ±",
                    "phase": "analyze"
                })
            else:
                # === PHASE 2: SEARCH ===
                await self._send({
                    "type": "status",
                    "message": "Bilgi tabanÄ± aranÄ±yor...",
                    "phase": "search"
                })
                
                # === PREMIUM CRAG: Query Transformation ===
                search_message = message
                query_variations = [message]
                crag_metadata = {}
                
                if CRAG_AVAILABLE and query_transformer and complexity_level in ["comprehensive", "research"] and feature_enabled('crag_full'):
                    try:
                        # Sorguyu analiz et ve dÃ¶nÃ¼ÅŸtÃ¼r
                        transformed = query_transformer.transform(message)
                        if transformed and transformed.reformulated != message:
                            search_message = transformed.reformulated
                            query_variations = [message, transformed.reformulated]
                            if transformed.sub_queries:
                                query_variations.extend(transformed.sub_queries[:3])
                            
                            crag_metadata["original_query"] = message
                            crag_metadata["transformed_query"] = transformed.reformulated
                            crag_metadata["sub_queries"] = transformed.sub_queries[:3] if transformed.sub_queries else []
                            
                            await self._send({
                                "type": "crag_transform",
                                "original": message,
                                "transformed": transformed.reformulated,
                                "sub_queries": transformed.sub_queries[:3] if transformed.sub_queries else []
                            })
                            logger.info(f"ğŸ”„ CRAG: Query transformed -> {len(query_variations)} variations")
                    except Exception as e:
                        logger.warning(f"CRAG transform error: {e}")
                
                # RAG Search - rag_service ile CRAG entegrasyonu
                try:
                    # Arama sonuÃ§ sayÄ±sÄ±nÄ± complexity'ye gÃ¶re ayarla - PREMIUM: Daha fazla sonuÃ§
                    n_results = 5 if complexity_level == "normal" else 10 if complexity_level == "comprehensive" else 15
                    score_threshold = 0.2 if complexity_level in ["comprehensive", "research"] else 0.3
                    
                    # Use rag_service if available (with CRAG), fallback to direct vector_store
                    if rag_service and SERVICES_AVAILABLE:
                        # Use CRAG for comprehensive/research queries
                        if complexity_level in ["comprehensive", "research"]:
                            enriched_context = await rag_service.search_with_crag(
                                query=search_message,
                                include_web=web_search,
                            )
                        else:
                            enriched_context = await rag_service.search(
                                query=search_message,
                                include_documents=True,
                                include_web=False,
                                n_results=n_results,
                                score_threshold=score_threshold,
                            )
                        
                        # Extract results from enriched context
                        if enriched_context.document_context:
                            knowledge_context = enriched_context.document_context
                        
                        # Convert to frontend sources format
                        for src in enriched_context.document_sources:
                            sources.append(src.to_frontend_format())
                        
                        # Web sources (if CRAG included them)
                        for src in enriched_context.web_sources:
                            web_sources.append(src.to_frontend_format())
                        
                        if enriched_context.web_context:
                            web_context = enriched_context.web_context
                        
                        # === PREMIUM CRAG: Relevance Grading ===
                        if CRAG_AVAILABLE and relevance_grader and sources and feature_enabled('crag_full'):
                            try:
                                graded_sources = []
                                high_relevance_count = 0
                                
                                for src in sources:
                                    grade = relevance_grader.grade(
                                        query=message,
                                        document=src.get("snippet", ""),
                                        metadata=src
                                    )
                                    src["relevance_grade"] = grade.grade.value
                                    src["relevance_score"] = grade.score
                                    graded_sources.append(src)
                                    
                                    if grade.grade in [RelevanceGrade.HIGHLY_RELEVANT, RelevanceGrade.RELEVANT]:
                                        high_relevance_count += 1
                                
                                sources = sorted(graded_sources, key=lambda x: x.get("relevance_score", 0), reverse=True)
                                crag_metadata["graded_sources"] = len(graded_sources)
                                crag_metadata["high_relevance"] = high_relevance_count
                                
                                logger.info(f"â­ CRAG Grading: {high_relevance_count}/{len(sources)} highly relevant")
                            except Exception as e:
                                logger.warning(f"CRAG grading error: {e}")
                    else:
                        # Fallback: use direct vector_store with multiple queries
                        all_results = []
                        seen_docs = set()
                        
                        for q_var in query_variations[:3]:
                            results = vector_store.search_with_scores(
                                query=q_var, 
                                n_results=n_results, 
                                score_threshold=score_threshold
                            )
                            if results:
                                for r in results:
                                    doc_id = hash(r.get('document', '')[:100])
                                    if doc_id not in seen_docs:
                                        seen_docs.add(doc_id)
                                        all_results.append(r)
                        
                        if all_results:
                            # En iyi 30 sonucu al
                            all_results = sorted(all_results, key=lambda x: x.get('score', 0), reverse=True)[:30]
                            
                            knowledge_context = "\n\n".join([
                                f"[Kaynak: {r.get('metadata', {}).get('filename', 'unknown')}]\n{r.get('document', '')}"
                                for r in all_results
                            ])
                            
                            # Frontend iÃ§in sources formatÄ±
                            for r in all_results:
                                meta = r.get('metadata', {})
                                doc_text = r.get('document', '')[:200]
                                sources.append({
                                    "title": meta.get('filename', 'Kaynak'),
                                    "url": meta.get('source', '#'),
                                    "domain": "ğŸ“„ Yerel Dosya",
                                    "snippet": doc_text,
                                    "type": "document",
                                    "reliability": r.get('score', 0.5),
                                })
                except Exception as e:
                    logger.warning(f"RAG search error: {e}")
                
                # === WEB SEARCH (if enabled) ===
                if web_search and web_search_engine:
                    await self._send({
                        "type": "status",
                        "message": "ğŸŒ Web'de aranÄ±yor...",
                        "phase": "search"
                    })
                    
                    try:
                        import asyncio
                        
                        # === PREMIUM: Semantic Query Expansion ===
                        search_queries = [message]  # Ana sorgu
                        if semantic_query_expander and feature_enabled('semantic_expansion'):
                            try:
                                expansion_result = await semantic_query_expander.expand_query(message, max_variations=5)
                                if expansion_result and expansion_result.expanded_queries:
                                    search_queries = [eq.query for eq in expansion_result.expanded_queries[:5]]
                                    if message not in search_queries:
                                        search_queries.insert(0, message)
                                    logger.info(f"ğŸ”„ Query expanded to {len(search_queries)} variations")
                            except Exception as e:
                                logger.warning(f"Query expansion error: {e}")
                        
                        # TÃ¼m sorgular iÃ§in paralel search
                        all_web_results = []
                        seen_urls = set()
                        
                        for query in search_queries:
                            loop = asyncio.get_event_loop()
                            web_response = await loop.run_in_executor(
                                None, 
                                lambda q=query: web_search_engine.search(
                                    query=q,
                                    num_results=15 if len(search_queries) > 1 else 30,
                                    extract_content=True,
                                    include_wikipedia=True
                                )
                            )
                            
                            if web_response and web_response.results:
                                for wr in web_response.results:
                                    if wr.url not in seen_urls:
                                        seen_urls.add(wr.url)
                                        all_web_results.append(wr)
                        
                        if all_web_results:
                            # Web context oluÅŸtur - Premium: 30 kaynak
                            web_parts = []
                            for i, wr in enumerate(all_web_results[:30]):
                                # Her kaynak iÃ§in daha fazla content
                                content_preview = wr.full_content[:1500] if wr.full_content else wr.snippet or ''
                                web_parts.append(f"[Web Kaynak {i+1}: {wr.title}]\nURL: {wr.url}\n{content_preview}")
                                
                                # Frontend iÃ§in web source formatÄ±
                                web_sources.append({
                                    "title": wr.title,
                                    "url": wr.url,
                                    "domain": f"ğŸŒ {wr.domain}",
                                    "snippet": wr.snippet or wr.full_content[:200] if wr.full_content else "",
                                    "type": "web",
                                    "reliability": wr.reliability_score if hasattr(wr, 'reliability_score') else 0.6,
                                })
                            
                            web_context = "\n\n".join(web_parts)
                            logger.info(f"ğŸŒ Web search: {len(all_web_results)} sonuÃ§ bulundu ({len(search_queries)} sorgu)")
                            
                            # === PREMIUM: Smart Title Generator ===
                            if smart_title_generator and web_sources and feature_enabled('smart_titles'):
                                try:
                                    improved_sources = []
                                    for src in web_sources:
                                        result = smart_title_generator.generate_title_sync(
                                            url=src["url"],
                                            raw_title=src["title"],
                                            query=message,
                                            content_snippet=src.get("snippet", "")
                                        )
                                        src["title"] = result.smart_title
                                        improved_sources.append(src)
                                    web_sources = improved_sources
                                    logger.info(f"ğŸ“ Titles improved for {len(web_sources)} sources")
                                except Exception as e:
                                    logger.warning(f"Smart title generator error: {e}")
                            
                            # === PREMIUM: Source Quality Scorer ===
                            if source_quality_scorer and web_sources and feature_enabled('source_scoring'):
                                try:
                                    scored_sources = source_quality_scorer.rank_sources(
                                        sources=web_sources,
                                        query=message,
                                        top_k=50
                                    )
                                    web_sources = scored_sources
                                    logger.info(f"â­ Sources ranked by quality")
                                except Exception as e:
                                    logger.warning(f"Source quality scorer error: {e}")
                    except Exception as e:
                        logger.warning(f"Web search error: {e}")
                
                # TÃ¼m kaynaklarÄ± birleÅŸtir
                all_sources = sources + web_sources
                
                # === PHASE 3: ANALYZE ===
                if all_sources:
                    doc_count = len(sources)
                    web_count = len(web_sources)
                    status_msg = []
                    if doc_count > 0:
                        status_msg.append(f"ğŸ“„ {doc_count} dosya")
                    if web_count > 0:
                        status_msg.append(f"ğŸŒ {web_count} web")
                    
                    await self._send({
                        "type": "status",
                        "message": f"{' + '.join(status_msg)} bulundu, analiz ediliyor...",
                        "phase": "analyze"
                    })
                    
                    # KaynaklarÄ± buffer'a ve frontend'e gÃ¶nder
                    stream_buffer.set_sources(stream_id, all_sources[:50])
                    await self._send({
                        "type": "sources",
                        "sources": all_sources[:50]
                    })
                else:
                    await self._send({
                        "type": "status",
                        "message": "Genel bilgi kullanÄ±lacak",
                        "phase": "analyze"
                    })
            
            # === PHASE 4: CONTEXT ===
            await self._send({
                "type": "status",
                "message": "âš¡ HazÄ±r" if is_simple_query else "BaÄŸlam hazÄ±rlanÄ±yor...",
                "phase": "context"
            })
            
            # === PHASE 5: GENERATE ===
            await self._send({
                "type": "status",
                "message": "âš¡ HÄ±zlÄ± yanÄ±t..." if is_simple_query else "YanÄ±t Ã¼retiliyor...",
                "phase": "generate"
            })
            
            # LLM'DEN STREAMING YANIT
            from core.system_knowledge import SELF_KNOWLEDGE_PROMPT
            
            # Sistem hakkÄ±nda mÄ± soruyor kontrolÃ¼ (query_analyzer service)
            is_about_system = query_analyzer.analyze(message).is_system_query
            
            # System prompt oluÅŸtur - PREMIUM EDUCATOR MODE
            if is_simple_query and not is_about_system:
                system_prompt = """Sen yardÄ±mcÄ± ve Ã¶ÄŸretici bir AI asistanÄ±sÄ±n. KÄ±sa ama bilgilendirici yanÄ±tlar ver."""
            elif is_about_system:
                system_prompt = SELF_KNOWLEDGE_PROMPT
            else:
                # KarmaÅŸÄ±k sorgular iÃ§in PREMIUM EDUCATOR PROMPT
                system_prompt = """Sen dÃ¼nya standartlarÄ±nda bir AI EÄŸitmenisin. GÃ¶revin kullanÄ±cÄ±ya konuyu GERÃ‡EKTEN Ã–ÄRETMEK.

## TEMEL PRENSÄ°PLER:
1. **Derinlemesine AÃ§Ä±klama**: Her kavramÄ± "neden" ve "nasÄ±l" boyutlarÄ±yla aÃ§Ä±kla
2. **Pratik Ã–rnekler**: Soyut kavramlarÄ± somut Ã¶rneklerle destekle
3. **Kod Ã–ÄŸretimi**: Sadece kod gÃ¶sterme - her satÄ±rÄ± aÃ§Ä±kla, alternatiflerini sun
4. **Kritik Noktalar**: YaygÄ±n hatalar, best practice'ler ve edge case'leri vurgula
5. **BaÄŸlam**: Konunun bÃ¼yÃ¼k resimde nereye oturduÄŸunu aÃ§Ä±kla

## YANITLAMA FORMATI:
### ğŸ“š Konu BaÅŸlÄ±ÄŸÄ±
- Konunun tanÄ±mÄ± ve Ã¶nemi
- Neden Ã¶ÄŸrenilmeli?

### ğŸ¯ Temel Kavramlar
- Her kavram detaylÄ± aÃ§Ä±klama ile
- GerÃ§ek dÃ¼nya analojileri

### ğŸ’» Kod Ã–rnekleri (varsa)
```language
# Her satÄ±r iÃ§in yorum
code_line  # Bu ne yapÄ±yor ve NEDEN
```
**Kod AÃ§Ä±klamasÄ±:**
- SatÄ±r satÄ±r ne yaptÄ±ÄŸÄ±nÄ± aÃ§Ä±kla
- Alternatif yaklaÅŸÄ±mlarÄ± belirt
- YaygÄ±n hatalarÄ± ve Ã§Ã¶zÃ¼mlerini gÃ¶ster

### âš ï¸ Dikkat Edilmesi Gerekenler
- YaygÄ±n hatalar ve nasÄ±l kaÃ§Ä±nÄ±lÄ±r
- Best practice'ler
- Edge case'ler

### ğŸ”— Ä°liÅŸkili Konular
- Bu konuyla baÄŸlantÄ±lÄ± kavramlar
- Sonraki Ã¶ÄŸrenme adÄ±mlarÄ±

### ğŸ“ Ã–zet
- Kilit noktalarÄ±n listesi
"""
                
                # Response mode'a gÃ¶re ek talimatlar
                if response_mode == "analytical":
                    system_prompt += "\n\n## EXTRA: ANALÄ°TÄ°K MOD\n- KarÅŸÄ±laÅŸtÄ±rmalÄ± analiz yap\n- Avantaj/dezavantaj tablolarÄ± kullan\n- Metrikler ve Ã¶lÃ§Ã¼tlerle destekle"
                elif response_mode == "creative":
                    system_prompt += "\n\n## EXTRA: YARATICI MOD\n- FarklÄ± perspektifler sun\n- Ä°lham verici Ã¶rnekler kullan\n- Benzersiz Ã§Ã¶zÃ¼mler Ã¶ner"
                elif response_mode == "technical":
                    system_prompt += "\n\n## EXTRA: TEKNÄ°K MOD\n- Low-level detaylar ver\n- Performans optimizasyonlarÄ±nÄ± aÃ§Ä±kla\n- Mimari kararlarÄ± tartÄ±ÅŸ"
                elif response_mode == "debate":
                    system_prompt += "\n\n## EXTRA: TARTIÅMA MOD\n- FarklÄ± bakÄ±ÅŸ aÃ§Ä±larÄ±nÄ± sun\n- Her gÃ¶rÃ¼ÅŸÃ¼n gÃ¼Ã§lÃ¼/zayÄ±f yanlarÄ±nÄ± analiz et\n- Sonunda dengeli bir sonuÃ§ Ã§Ä±kar"
            
            # === PREMIUM: Multi-Agent Debate (Research Mode) ===
            debate_result = None
            if DEBATE_AVAILABLE and debate_orchestrator and complexity_level == "research" and response_mode in ["debate", "analytical"] and feature_enabled('multi_agent_debate'):
                try:
                    await self._send({
                        "type": "status",
                        "message": "ğŸ¤– Multi-agent tartÄ±ÅŸma baÅŸlÄ±yor...",
                        "phase": "debate"
                    })
                    
                    # LLM factory for debate agents
                    def create_debate_llm():
                        def llm_fn(prompt):
                            import asyncio
                            loop = asyncio.get_event_loop()
                            result = loop.run_until_complete(
                                llm_manager.generate_async(prompt, system_prompt="You are a debate participant.", temperature=0.7)
                            )
                            return result
                        return llm_fn
                    
                    # Run multi-agent debate
                    from core.multi_agent_debate import multi_agent_debate
                    debate_result = await multi_agent_debate(
                        question=message,
                        llm=create_debate_llm(),
                        context=knowledge_context + "\n\n" + web_context if (knowledge_context or web_context) else "",
                        num_agents=3,
                        max_rounds=2
                    )
                    
                    if debate_result:
                        # Debate sonuÃ§larÄ±nÄ± system prompt'a ekle
                        system_prompt += f"""

## ğŸ¤– MULTI-AGENT DEBATE SONUÃ‡LARI
**KonsensÃ¼s Seviyesi:** {debate_result.consensus_level:.0%}
**Kazanan Pozisyon:** {debate_result.winning_position}
**GÃ¼ven:** {debate_result.confidence:.0%}

Bu debate sonuÃ§larÄ±nÄ± kullanarak kapsamlÄ± ve dengeli bir yanÄ±t oluÅŸtur.
FarklÄ± perspektifleri de yansÄ±t."""
                        
                        await self._send({
                            "type": "debate_result",
                            "consensus": debate_result.consensus_level,
                            "winning_position": debate_result.winning_position,
                            "confidence": debate_result.confidence,
                            "dissenting_views": debate_result.dissenting_views if hasattr(debate_result, 'dissenting_views') else []
                        })
                        logger.info(f"ğŸ¤– Debate completed: consensus={debate_result.consensus_level:.0%}")
                except Exception as e:
                    logger.warning(f"Multi-agent debate error: {e}")
            
            # === PREMIUM: Response Length Manager ===
            if response_length_manager and not is_simple_query and feature_enabled('response_length'):
                try:
                    # Sorgu iÃ§in uygun yanÄ±t modunu belirle
                    source_count = len(sources) + len(web_sources) if 'sources' in dir() else 0
                    suggested_mode = response_length_manager.suggest_mode_for_query(message, source_count)
                    
                    # System prompt'a uzunluk talimatlarÄ± ekle
                    length_enhancement = response_length_manager.get_system_prompt_enhancement(suggested_mode, source_count)
                    system_prompt += f"\n\n{length_enhancement}"
                    logger.info(f"ğŸ“ Response mode: {suggested_mode}, sources: {source_count}")
                except Exception as e:
                    logger.warning(f"Response length manager error: {e}")
            
            # RAG context ve web context'i prompt'a ekle
            final_prompt = message
            has_doc_sources = bool(knowledge_context)
            has_web_sources = bool(web_context)
            
            if has_doc_sources or has_web_sources:
                context_parts = []
                
                if has_doc_sources:
                    context_parts.append(f"""=== ğŸ“„ DOSYA KAYNAKLARI ===
{knowledge_context}
=== DOSYA KAYNAKLARI SONU ===""")
                
                if has_web_sources:
                    context_parts.append(f"""=== ğŸŒ WEB KAYNAKLARI ===
{web_context}
=== WEB KAYNAKLARI SONU ===""")
                
                combined_context = "\n\n".join(context_parts)
                
                final_prompt = f"""## ğŸ“š ARAÅTIRMA KAYNAKLARI
{combined_context}

## â“ KULLANICI SORUSU
{message}

## ğŸ“ YANITLAMA TALÄ°MATLARI

### Kaynak KullanÄ±mÄ±:
- YukarÄ±daki kaynaklardan BÄ°LGÄ° SENTEZÄ° yap - sadece kopyalama deÄŸil
- Her kaynaktan aldÄ±ÄŸÄ±n bilgiyi kendi cÃ¼mlelerinle aÃ§Ä±kla
- FarklÄ± kaynaklardan gelen bilgileri birleÅŸtirerek kapsamlÄ± yanÄ±t oluÅŸtur

### Format Gereksinimleri:
1. **GiriÅŸ**: Konunun ne olduÄŸunu ve neden Ã¶nemli olduÄŸunu aÃ§Ä±kla
2. **Ana Ä°Ã§erik**: Konuyu sistematik ve detaylÄ± iÅŸle
   - Her kavramÄ± derinlemesine aÃ§Ä±kla (sadece tanÄ±m deÄŸil, NEDEN ve NASIL)
   - Kod varsa: Her satÄ±rÄ± aÃ§Ä±kla, alternatiflerini gÃ¶ster, yaygÄ±n hatalarÄ± belirt
   - Pratik Ã¶rnekler ve analojiler kullan
3. **Kritik Noktalar**: Dikkat edilmesi gerekenler, yaygÄ±n hatalar, best practice'ler
4. **Ã–zet**: Kilit noktalarÄ± listele

### Uzunluk:
- KAPSAMLI ve DETAYLI yanÄ±t ver - kÄ±sa kesme
- Her Ã¶nemli kavramÄ± tam olarak aÃ§Ä±kla, yÃ¼zeysel geÃ§me
- Minimum 1500 kelime hedefle (karmaÅŸÄ±k konularda daha fazla)

### Kaynak GÃ¶sterimi:
- YanÄ±tÄ±n sonunda kullanÄ±lan web kaynaklarÄ±nÄ± listele:
  ğŸ”— **Kaynaklar:**
  - [Kaynak AdÄ±](URL)"""
            elif not is_simple_query:
                # Kaynak yok ama karmaÅŸÄ±k sorgu - genel bilgi kullan
                final_prompt = f"""{message}

NOT: Bu soru iÃ§in bilgi tabanÄ±nda veya web'de spesifik kaynak bulunamadÄ±. 
LÃ¼tfen genel bilginle kapsamlÄ± yanÄ±t ver ve yanÄ±tÄ±nÄ±n baÅŸÄ±na "ğŸ’¡ Genel Bilgi:" ekle."""
            
            full_response = ""
            token_index = 0
            
            # DEBUG: LLM streaming baÅŸlÄ±yor
            logger.info(f"ğŸš€ LLM STREAMING BAÅLIYOR: model={model_name}, prompt_len={len(final_prompt)}")
            
            thinking_content = ""  # AI dÃ¼ÅŸÃ¼nce sÃ¼recini biriktir
            
            async with asyncio_timeout(STREAM_TIMEOUT):
                async for chunk_data in llm_manager.generate_stream_async(
                    prompt=final_prompt,
                    system_prompt=system_prompt,
                    temperature=0.7 if response_mode != "analytical" else 0.4,
                    model=model_name,
                ):
                    # DEBUG: Her chunk'ta log
                    if token_index == 0:
                        logger.info(f"âœ… Ä°LK CHUNK GELDÄ°: {type(chunk_data)}")
                    
                    # Stop kontrolÃ¼
                    if self.conn.stop_flag or stream_buffer.is_stop_requested(stream_id):
                        stats.was_stopped = True
                        stream_buffer.stop_stream(stream_id)
                        break
                    
                    if chunk_data:
                        # Dict format: {"type": "content"|"thinking", "content": "..."}
                        if isinstance(chunk_data, dict):
                            chunk_type = chunk_data.get("type", "content")
                            chunk_content = chunk_data.get("content", "")
                            
                            if chunk_type == "thinking":
                                # Thinking content - ayrÄ± mesaj olarak gÃ¶nder
                                thinking_content += chunk_content
                                await self._send({
                                    "type": "thinking",
                                    "content": chunk_content,
                                    "index": token_index
                                })
                                token_index += 1
                            else:
                                # Normal content
                                stats.token_count += 1
                                stats.char_count += len(chunk_content)
                                full_response += chunk_content
                                
                                # Token'Ä± buffer'a kaydet
                                token = stream_buffer.add_token(stream_id, chunk_content)
                                
                                # ANLIK gÃ¶nder
                                await self._send({
                                    "type": "chunk",
                                    "content": chunk_content,
                                    "index": token.index if token else token_index
                                })
                                token_index += 1
                        else:
                            # Backward compatibility - string
                            chunk = str(chunk_data)
                            stats.token_count += 1
                            stats.char_count += len(chunk)
                            full_response += chunk
                            
                            token = stream_buffer.add_token(stream_id, chunk)
                            await self._send({
                                "type": "chunk",
                                "content": chunk,
                                "index": token.index if token else token_index
                            })
                            token_index += 1
            
            stats.end_time = time.time()
            self.conn.total_tokens += stats.token_count
            
            # === PREMIUM: Hallucination Detection ===
            hallucination_risk = "low"
            if CRAG_AVAILABLE and hallucination_detector and full_response and (sources or web_sources) and feature_enabled('crag_full'):
                try:
                    all_contexts = []
                    for src in sources[:10]:
                        all_contexts.append(src.get("snippet", ""))
                    for src in web_sources[:10]:
                        all_contexts.append(src.get("snippet", ""))
                    
                    if all_contexts:
                        risk = hallucination_detector.detect(
                            answer=full_response,
                            contexts=all_contexts,
                            query=message
                        )
                        hallucination_risk = risk.risk_level.value if risk else "unknown"
                        
                        if risk and risk.risk_level in [HallucinationRisk.HIGH, HallucinationRisk.CRITICAL]:
                            await self._send({
                                "type": "hallucination_warning",
                                "risk": hallucination_risk,
                                "confidence": risk.confidence if risk else 0,
                                "message": "âš ï¸ YanÄ±t kaynaklarla tam uyumlu olmayabilir"
                            })
                        logger.info(f"ğŸ” Hallucination check: {hallucination_risk}")
                except Exception as e:
                    logger.warning(f"Hallucination detection error: {e}")
            
            # === PREMIUM: RAGAS Evaluation ===
            ragas_score = None
            if RAGAS_AVAILABLE and ragas_evaluator and full_response and not is_simple_query and feature_enabled('ragas_evaluation'):
                try:
                    contexts = [src.get("snippet", "") for src in (sources + web_sources)[:15]]
                    if contexts:
                        eval_result = await ragas_evaluator.evaluate_async(
                            question=message,
                            answer=full_response,
                            contexts=contexts
                        )
                        if eval_result:
                            ragas_score = eval_result.overall_score
                            await self._send({
                                "type": "quality_score",
                                "overall": ragas_score,
                                "faithfulness": eval_result.metrics.get(MetricType.FAITHFULNESS, {}).score if hasattr(eval_result, 'metrics') else None,
                                "relevancy": eval_result.metrics.get(MetricType.ANSWER_RELEVANCY, {}).score if hasattr(eval_result, 'metrics') else None
                            })
                            logger.info(f"ğŸ“Š RAGAS score: {ragas_score:.2f}")
                except Exception as e:
                    logger.warning(f"RAGAS evaluation error: {e}")
            
            # === PREMIUM: MemGPT Memory - KonuÅŸmayÄ± Kaydet ===
            if MEMGPT_AVAILABLE and memory_manager and session_id and full_response and feature_enabled('memgpt_memory'):
                try:
                    # KonuÅŸmayÄ± working memory'ye kaydet
                    memory_manager.store(
                        content=f"User: {message}\nAssistant: {full_response[:500]}",
                        session_id=session_id,
                        memory_type=MemoryType.WORKING,
                        priority=MemoryPriority.MEDIUM
                    )
                    
                    # Ã–nemli bilgileri archival memory'ye taÅŸÄ± (uzun yanÄ±tlar iÃ§in)
                    if len(full_response) > 1000 and not is_simple_query:
                        memory_manager.consolidate(session_id=session_id)
                    
                    logger.info(f"ğŸ§  MemGPT: Conversation stored")
                except Exception as e:
                    logger.warning(f"MemGPT store error: {e}")
            
            # === PHASE 6: COMPLETE ===
            await self._send({
                "type": "status",
                "message": "TamamlandÄ±",
                "phase": "complete"
            })
            
            # BÄ°TÄ°Å
            if stats.was_stopped:
                stream_buffer.stop_stream(stream_id)
                await self._send({
                    "type": "stopped",
                    "elapsed_ms": stats.duration_ms,
                    "tokens": stats.token_count,
                    "stream_id": stream_id,
                })
            else:
                stream_buffer.complete_stream(stream_id)
                await self._send({
                    "type": "end",
                    "response_id": response_id,
                    "model_info": routing_info,
                    "stats": stats.to_dict(),
                    "ts": int(time.time() * 1000),
                    "stream_id": stream_id,
                })
            
            # Session'a kaydet
            logger.info(f"ğŸ” [SESSION DEBUG] About to save session:")
            logger.info(f"   - session_id: {session_id}")
            logger.info(f"   - full_response length: {len(full_response) if full_response else 0}")
            logger.info(f"   - message preview: {message[:50]}...")
            
            if full_response:
                try:
                    # Session yoksa oluÅŸtur - create_session_with_id kullan
                    logger.info(f"   - Checking if session exists...")
                    session = session_manager.get_session(session_id)
                    logger.info(f"   - Session found: {session is not None}")
                    
                    if not session:
                        # Yeni session oluÅŸtur - belirli ID ile
                        title = message[:50] if len(message) > 50 else message
                        session = session_manager.create_session_with_id(session_id, title=title)
                        logger.info(f"ğŸ“ Created new session with ID: {session_id}")
                    
                    logger.info(f"   - Adding user message...")
                    session_manager.add_message(session_id, "user", message)
                    saved_response = full_response
                    if stats.was_stopped:
                        saved_response += "\n\n*[YanÄ±t durduruldu]*"
                    logger.info(f"   - Adding assistant message...")
                    session_manager.add_message(session_id, "assistant", saved_response)
                    
                    # Verify save
                    updated_session = session_manager.get_session(session_id)
                    logger.info(f"âœ… Session saved: {session_id}, messages: {len(updated_session.messages) if updated_session else 'N/A'}")
                    
                    # Check file exists
                    from core.config import settings
                    file_path = settings.DATA_DIR / "sessions" / f"{session_id}.json"
                    logger.info(f"   - File exists: {file_path.exists()}")
                    
                except Exception as e:
                    logger.error(f"âŒ Session save error: {e}")
                    import traceback
                    logger.error(traceback.format_exc())
            else:
                logger.warning(f"âš ï¸ No full_response to save!")
            
        except asyncio.TimeoutError:
            stats.error = "timeout"
            stream_buffer.error_stream(stream_id, "timeout")
            await self._send({
                "type": "error",
                "code": "timeout",
                "message": f"YanÄ±t {STREAM_TIMEOUT} saniye iÃ§inde tamamlanamadÄ±",
                "elapsed_ms": stats.duration_ms,
                "stream_id": stream_id,
            })
        
        except asyncio.CancelledError:
            stats.was_stopped = True
            stats.end_time = time.time()
            stream_buffer.stop_stream(stream_id)
            logger.debug(f"Stream cancelled: {self.conn.client_id}")
            try:
                await self._send({
                    "type": "stopped",
                    "elapsed_ms": stats.duration_ms,
                    "tokens": stats.token_count,
                    "stream_id": stream_id,
                })
            except Exception:
                pass
        
        except Exception as e:
            stats.error = str(e)
            logger.exception(f"Routed stream error: {self.conn.client_id}")
            await self._send({
                "type": "error",
                "code": "stream_failed",
                "message": str(e)[:300],
                "elapsed_ms": stats.duration_ms,
            })
        
        finally:
            self.conn.is_streaming = False

    async def _stream_response(
        self, 
        message: str, 
        session_id: str,
        web_search: bool = False,
        response_mode: str = "normal",
        complexity_level: str = "auto"
    ) -> None:
        """
        Streaming yanÄ±t Ã¼ret ve gÃ¶nder.
        
        Token'lar StreamBuffer'da saklanÄ±r - WebSocket kopsa bile kaybolmaz.
        Client reconnect edince kaldÄ±ÄŸÄ± yerden devam edebilir.
        
        Args:
            complexity_level: "auto", "simple", "moderate", "advanced", "comprehensive"
        """
        stats = StreamStats(start_time=time.time())
        
        # Stream buffer'da yeni stream oluÅŸtur
        stream = stream_buffer.create_stream(session_id, message)
        stream_id = stream.stream_id
        self.conn.current_stream_id = stream_id
        
        # BaÅŸlangÄ±Ã§ mesajÄ± - stream_id ile
        await self._send({
            "type": "start",
            "ts": int(time.time() * 1000),
            "session_id": session_id,
            "stream_id": stream_id,  # Client bu ID ile resume yapabilir
        })
        
        try:
            # === PHASE 1: ROUTING ===
            await self._send({
                "type": "status",
                "message": "Sorgu analiz ediliyor...",
                "phase": "routing"
            })
            
            # Basit sorgu tespiti (query_analyzer service kullanÄ±yor)
            is_simple_greeting = query_analyzer.is_simple_query(message)
            is_short_query = len(message) < 25
            
            # Auto modda basit sorgular iÃ§in otomatik simple mod kullan
            if complexity_level == "auto" and (is_simple_greeting or is_short_query):
                complexity_level = "simple"
            
            # âš¡ SIMPLE MOD: Ultra hÄ±zlÄ± - RAG aramasÄ± yapma, direkt LLM
            skip_rag = complexity_level == "simple"
            
            async with asyncio_timeout(STREAM_TIMEOUT):
                knowledge_context = ""
                sources = []
                
                # Simple modda RAG'Ä± atla - maksimum hÄ±z
                if not skip_rag:
                    # === PHASE 2: SEARCH ===
                    await self._send({
                        "type": "status",
                        "message": "Bilgi tabanÄ± aranÄ±yor...",
                        "phase": "search"
                    })
                    
                    # RAG search - use rag_service if available
                    try:
                        if rag_service and SERVICES_AVAILABLE:
                            enriched_context = await rag_service.search(
                                query=message,
                                include_documents=True,
                                include_web=False,
                                n_results=5,
                                score_threshold=0.3,
                            )
                            
                            if enriched_context.document_context:
                                knowledge_context = enriched_context.document_context
                            
                            for src in enriched_context.document_sources:
                                sources.append(src.to_frontend_format())
                        else:
                            # Fallback to direct vector_store
                            results = vector_store.search_with_scores(query=message, n_results=5, score_threshold=0.3)
                            if results:
                                knowledge_context = "\n\n".join([
                                    f"[Kaynak: {r.get('metadata', {}).get('filename', 'unknown')}]\n{r.get('document', '')}"
                                    for r in results[:3]
                                ])
                                for r in results:
                                    meta = r.get('metadata', {})
                                    doc_text = r.get('document', '')[:200]
                                    sources.append({
                                        "title": meta.get('filename', 'Kaynak'),
                                        "url": meta.get('source', '#'),
                                        "domain": "ğŸ“„ Yerel Dosya",
                                        "snippet": doc_text,
                                        "type": "document",
                                        "reliability": r.get('score', 0.5),
                                    })
                    except Exception as e:
                        logger.warning(f"RAG search error: {e}")
                    
                # KaynaklarÄ± buffer'a kaydet
                if sources:
                    # === PHASE 3: ANALYZE ===
                    await self._send({
                        "type": "status",
                        "message": "Kaynaklar analiz ediliyor...",
                        "phase": "analyze"
                    })
                    
                    stream_buffer.set_sources(stream_id, sources[:30])
                    await self._send({
                        "type": "sources",
                        "sources": sources[:30]
                    })
                
                # === PHASE 4: CONTEXT ===
                await self._send({
                    "type": "status",
                    "message": "BaÄŸlam hazÄ±rlanÄ±yor...",
                    "phase": "context"
                })
                
                # === PHASE 5: GENERATE ===
                await self._send({
                    "type": "status",
                    "message": "YanÄ±t oluÅŸturuluyor..." if not skip_rag else "âš¡ HÄ±zlÄ± yanÄ±t...",
                    "phase": "generate"
                })
                
                # System prompt oluÅŸtur
                from core.system_knowledge import SELF_KNOWLEDGE_PROMPT
                
                # Sistem hakkÄ±nda mÄ± soruyor kontrolÃ¼ (query_analyzer service)
                is_about_system = query_analyzer.analyze(message).is_system_query
                
                if skip_rag and not is_about_system:
                    # Basit sorular iÃ§in minimal sistem prompt
                    system_prompt = "Sen yardÄ±mcÄ± bir AI asistanÄ±sÄ±n. Samimi ve kÄ±sa yanÄ±t ver."
                elif is_about_system:
                    system_prompt = SELF_KNOWLEDGE_PROMPT
                else:
                    system_prompt = "Sen Enterprise AI Asistan'sÄ±n. KullanÄ±cÄ±nÄ±n sorusuna odaklan ve yardÄ±mcÄ± ol."
                
                if knowledge_context:
                    system_prompt += f"\n\nğŸ“š Ä°lgili Bilgiler:\n{knowledge_context}"
                
                # Complexity level'a gÃ¶re prompt ayarla
                complexity_instructions = {
                    "simple": "\n\nâš¡ YANITLAMA STÄ°LÄ°: Ã‡OK KISA yanÄ±t ver. Sadece 1-2 cÃ¼mle. Gereksiz detay VERME.",
                    "moderate": "\n\nğŸ“ YANITLAMA STÄ°LÄ°: Orta uzunlukta, dengeli yanÄ±t ver. Ana noktalarÄ± aÃ§Ä±kla.",
                    "advanced": "\n\nğŸ“Š YANITLAMA STÄ°LÄ°: DetaylÄ± analiz yap. Ã–rnekler ve aÃ§Ä±klamalar ekle.",
                    "comprehensive": "\n\nğŸ“š YANITLAMA STÄ°LÄ°: KapsamlÄ± ve derinlemesine yanÄ±t ver. TÃ¼m yÃ¶nleri ele al, kaynaklar ve Ã¶rneklerle destekle.",
                }
                if complexity_level in complexity_instructions:
                    system_prompt += complexity_instructions[complexity_level]
                
                # âš¡ Simple modda kÃ¼Ã§Ã¼k model kullan - ultra hÄ±zlÄ±
                selected_model = None
                if skip_rag:
                    # Basit sorgular iÃ§in kÃ¼Ã§Ã¼k model (qwen3:4b veya benzeri)
                    selected_model = MODEL_CONFIG[ModelSize.SMALL]["name"]
                
                # Streaming response - token'lar buffer'a kaydedilir
                full_response = ""
                token_index = 0
                async for chunk in llm_manager.generate_stream_async(
                    prompt=message,
                    system_prompt=system_prompt,
                    model=selected_model,  # Simple modda kÃ¼Ã§Ã¼k model
                ):
                    # Stop kontrolÃ¼ - hem local flag hem buffer'dan
                    if self.conn.stop_flag or stream_buffer.is_stop_requested(stream_id):
                        stats.was_stopped = True
                        stream_buffer.stop_stream(stream_id)
                        break
                    
                    if chunk:
                        stats.token_count += 1
                        stats.char_count += len(chunk)
                        full_response += chunk
                        
                        # Token'Ä± buffer'a kaydet
                        token = stream_buffer.add_token(stream_id, chunk)
                        
                        # ANLIK gÃ¶nder - index ile (resume iÃ§in gerekli)
                        await self._send({
                            "type": "token",
                            "content": chunk,
                            "index": token.index if token else token_index
                        })
                        token_index += 1
                
                stats.end_time = time.time()
                self.conn.total_tokens += stats.token_count
                
                # === PHASE 6: COMPLETE ===
                await self._send({
                    "type": "status",
                    "message": "TamamlandÄ±",
                    "phase": "complete"
                })
                
                # BitiÅŸ mesajÄ± ve buffer gÃ¼ncelle
                if stats.was_stopped:
                    stream_buffer.stop_stream(stream_id)
                    await self._send({
                        "type": "stopped",
                        "elapsed_ms": stats.duration_ms,
                        "tokens": stats.token_count,
                        "stream_id": stream_id,
                    })
                else:
                    stream_buffer.complete_stream(stream_id)
                    await self._send({
                        "type": "end",
                        "stats": stats.to_dict(),
                        "ts": int(time.time() * 1000),
                        "stream_id": stream_id,
                    })
                
                # Session'a kaydet (durdurulsa bile kÄ±smi yanÄ±tÄ± kaydet)
                if full_response:
                    try:
                        # Session yoksa oluÅŸtur
                        if not session_manager.get_session(session_id):
                            title = message[:50] if len(message) > 50 else message
                            session_manager.create_session_with_id(session_id, title=title)
                            logger.info(f"ğŸ“ Created new session: {session_id}")
                        
                        session_manager.add_message(session_id, "user", message)
                        # Durdurulduysa yanÄ±ta iÅŸaret ekle
                        saved_response = full_response
                        if stats.was_stopped:
                            saved_response += "\n\n*[YanÄ±t durduruldu]*"
                        session_manager.add_message(session_id, "assistant", saved_response)
                        logger.info(f"âœ… Session saved: {session_id}")
                    except Exception as e:
                        logger.warning(f"Session save error: {e}")
                
        except asyncio.TimeoutError:
            stats.error = "timeout"
            stream_buffer.error_stream(stream_id, "timeout")
            await self._send({
                "type": "error",
                "code": "timeout",
                "message": f"YanÄ±t {STREAM_TIMEOUT} saniye iÃ§inde tamamlanamadÄ±",
                "elapsed_ms": stats.duration_ms,
                "stream_id": stream_id,
            })
        
        except asyncio.CancelledError:
            # KullanÄ±cÄ± durduÄŸunda, o ana kadar yazÄ±lanlarÄ± koru
            stats.was_stopped = True
            stats.end_time = time.time()
            stream_buffer.stop_stream(stream_id)
            logger.debug(f"Stream cancelled: {self.conn.client_id}, tokens: {stats.token_count}")
            # Frontend'e "stopped" mesajÄ± gÃ¶nder - o ana kadar yazÄ±lanlar korunacak
            try:
                await self._send({
                    "type": "stopped",
                    "elapsed_ms": stats.duration_ms,
                    "tokens": stats.token_count,
                    "partial": True,
                    "stream_id": stream_id,
                })
            except Exception:
                pass  # BaÄŸlantÄ± kapanmÄ±ÅŸ olabilir
        
        except Exception as e:
            stats.error = str(e)
            logger.exception(f"Stream error: {self.conn.client_id}")
            await self._send({
                "type": "error",
                "code": "stream_failed",
                "message": str(e)[:300],
                "elapsed_ms": stats.duration_ms,
            })
        
        finally:
            self.conn.is_streaming = False
    
    async def _keepalive_loop(self) -> None:
        """Keepalive ping gÃ¶nder."""
        try:
            while True:
                await asyncio.sleep(PING_INTERVAL)
                await self._send({
                    "type": "ping",
                    "ts": int(time.time() * 1000)
                })
        except asyncio.CancelledError:
            pass
        except Exception as e:
            logger.debug(f"Keepalive error: {e}")
    
    async def _send(self, data: dict) -> bool:
        """Mesaj gÃ¶nder."""
        return await self.manager.send(self.conn.client_id, data)


# =============================================================================
# FASTAPI WEBSOCKET ENDPOINT
# =============================================================================

async def websocket_endpoint_v2(websocket: WebSocket, client_id: str) -> None:
    """
    Enterprise WebSocket endpoint v2.
    
    Args:
        websocket: WebSocket baÄŸlantÄ±sÄ±
        client_id: Client ID
    """
    handler: Optional[WebSocketHandlerV2] = None
    
    try:
        # BaÄŸlantÄ±yÄ± kabul et
        conn = await ws_manager.connect(websocket, client_id)
        
        # Handler oluÅŸtur ve baÅŸlat
        handler = WebSocketHandlerV2(conn, ws_manager)
        await handler.start()
        
        # Mesaj dÃ¶ngÃ¼sÃ¼
        while True:
            try:
                # Mesaj bekle
                data = await websocket.receive_json()
                
                # Mesaj boyutu kontrolÃ¼
                if len(json.dumps(data)) > MAX_MESSAGE_SIZE:
                    await ws_manager.send(client_id, {
                        "type": "error",
                        "code": "message_too_large",
                        "message": f"Maksimum mesaj boyutu: {MAX_MESSAGE_SIZE} byte"
                    })
                    continue
                
                # MesajÄ± iÅŸle
                await handler.handle_message(data)
                
            except json.JSONDecodeError:
                await ws_manager.send(client_id, {
                    "type": "error",
                    "code": "invalid_json",
                    "message": "GeÃ§ersiz JSON formatÄ±"
                })
    
    except WebSocketDisconnect:
        logger.debug(f"WebSocket disconnect: {client_id}")
    
    except ConnectionError as e:
        logger.warning(f"Connection error: {client_id}, {e}")
    
    except Exception as e:
        logger.exception(f"WebSocket error: {client_id}")
    
    finally:
        # Temizlik
        if handler:
            await handler.stop()
        await ws_manager.disconnect(client_id)


# =============================================================================
# EXPORTS
# =============================================================================

__all__ = [
    'ws_manager',
    'websocket_endpoint_v2',
    'WebSocketManagerV2',
    'WebSocketHandlerV2',
    'ClientConnection',
    'StreamStats',
]
