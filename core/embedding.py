"""
Enterprise AI Assistant - Embedding Manager
EndÃ¼stri StandartlarÄ±nda Kurumsal AI Ã‡Ã¶zÃ¼mÃ¼

Ollama tabanlÄ± embedding Ã¼retimi - dÃ¶kÃ¼man ve sorgu vektÃ¶rizasyonu.

ENTERPRISE FEATURES:
- TRUE Batch Processing (single API call per batch)
- Thread-safe LRU Caching (2000 entries)
- Parallel embedding for large document sets
- Performance metrics and monitoring
- Automatic retry on failure
"""

import hashlib
import threading
import time
from typing import List, Optional, Dict, Tuple
from collections import OrderedDict
from concurrent.futures import ThreadPoolExecutor, as_completed
import ollama
import numpy as np

from .config import settings


class EmbeddingCache:
    """
    Thread-safe LRU embedding cache.
    
    Embedding hesaplamasÄ± pahalÄ± bir iÅŸlem olduÄŸundan,
    sÄ±k kullanÄ±lan sorgu/dokÃ¼manlar iÃ§in cache tutulur.
    """
    
    def __init__(self, max_size: int = 1000):
        self.max_size = max_size
        self._cache: OrderedDict[str, List[float]] = OrderedDict()
        self._lock = threading.Lock()
        self._hits = 0
        self._misses = 0
    
    def _hash_text(self, text: str) -> str:
        """Metin iÃ§in unique hash oluÅŸtur."""
        return hashlib.sha256(text.encode('utf-8')).hexdigest()[:32]
    
    def get(self, text: str) -> Optional[List[float]]:
        """Cache'den embedding al."""
        key = self._hash_text(text)
        with self._lock:
            if key in self._cache:
                # LRU: Move to end
                self._cache.move_to_end(key)
                self._hits += 1
                return self._cache[key]
            self._misses += 1
            return None
    
    def set(self, text: str, embedding: List[float]) -> None:
        """Embedding'i cache'e ekle."""
        key = self._hash_text(text)
        with self._lock:
            if key in self._cache:
                self._cache.move_to_end(key)
            else:
                if len(self._cache) >= self.max_size:
                    # Remove oldest
                    self._cache.popitem(last=False)
                self._cache[key] = embedding
    
    def clear(self) -> None:
        """Cache'i temizle."""
        with self._lock:
            self._cache.clear()
            self._hits = 0
            self._misses = 0
    
    def get_stats(self) -> Dict[str, any]:
        """Cache istatistiklerini dÃ¶ndÃ¼r."""
        with self._lock:
            total = self._hits + self._misses
            hit_rate = (self._hits / total * 100) if total > 0 else 0
            return {
                "size": len(self._cache),
                "max_size": self.max_size,
                "hits": self._hits,
                "misses": self._misses,
                "hit_rate": f"{hit_rate:.1f}%"
            }


class EmbeddingManager:
    """
    Embedding yÃ¶netim sÄ±nÄ±fÄ± - EndÃ¼stri standartlarÄ±na uygun.
    
    ENTERPRISE FEATURES:
    - Ollama embedding modeli desteÄŸi
    - TRUE Batch processing (parallel API calls)
    - L2 Normalization
    - Thread-safe LRU Caching
    - Performance metrics
    - Automatic retry on failure
    """
    
    # Cache sabitleri
    CACHE_MAX_SIZE = 2000  # Maksimum cache giriÅŸi
    
    # Parallel processing
    MAX_WORKERS = 4  # Parallel thread sayÄ±sÄ±
    
    def __init__(
        self,
        model_name: Optional[str] = None,
        base_url: Optional[str] = None,
        enable_cache: bool = True,
    ):
        self.model_name = model_name or settings.OLLAMA_EMBEDDING_MODEL
        self.base_url = base_url or settings.OLLAMA_BASE_URL
        self.client = ollama.Client(host=self.base_url)
        self.dimension = settings.EMBEDDING_DIMENSION
        
        # Cache
        self._cache_enabled = enable_cache
        self._cache = EmbeddingCache(max_size=self.CACHE_MAX_SIZE) if enable_cache else None
        
        # Thread pool for parallel processing
        self._executor = ThreadPoolExecutor(max_workers=self.MAX_WORKERS)
        
        # Performance metrics
        self._metrics = {
            "total_embeddings": 0,
            "cache_hits": 0,
            "cache_misses": 0,
            "total_latency_ms": 0,
            "errors": 0,
            "batch_calls": 0,
        }
    
    def check_model_available(self) -> bool:
        """Embedding model'in mevcut olup olmadÄ±ÄŸÄ±nÄ± kontrol et."""
        try:
            result = self.client.list()
            # Handle both old (dict) and new (object) API formats
            if hasattr(result, 'models'):
                # New API: result.models is a list of Model objects
                available_models = [m.model for m in result.models]
            elif isinstance(result, dict):
                # Old API: result is a dict with 'models' key
                available_models = [m["name"] for m in result.get("models", [])]
            else:
                available_models = []
            
            return any(
                self.model_name in m or m.startswith(self.model_name.split(":")[0])
                for m in available_models
            )
        except Exception:
            return False
    
    def pull_model(self) -> bool:
        """Embedding model'ini indir."""
        try:
            print(f"ğŸ“¥ Embedding model indiriliyor: {self.model_name}")
            self.client.pull(self.model_name)
            print(f"âœ… Embedding model indirildi: {self.model_name}")
            return True
        except Exception as e:
            print(f"âŒ Embedding model indirilemedi: {e}")
            return False
    
    def ensure_model(self) -> bool:
        """Model'in mevcut olduÄŸundan emin ol."""
        if not self.check_model_available():
            return self.pull_model()
        return True
    
    def embed_text(self, text: str, use_cache: bool = True, max_retries: int = 3) -> List[float]:
        """
        Tek bir metin iÃ§in embedding Ã¼ret.
        
        Args:
            text: Embedding yapÄ±lacak metin
            use_cache: Cache kullanÄ±lsÄ±n mÄ±
            max_retries: Hata durumunda tekrar deneme sayÄ±sÄ±
            
        Returns:
            Embedding vektÃ¶rÃ¼ (float listesi)
        """
        start_time = time.time()
        
        # Cache kontrolÃ¼
        if use_cache and self._cache_enabled and self._cache:
            cached = self._cache.get(text)
            if cached is not None:
                self._metrics["cache_hits"] += 1
                return cached
        
        self._metrics["cache_misses"] += 1
        
        # Retry mekanizmasÄ±
        last_error = None
        for attempt in range(max_retries):
            try:
                response = self.client.embeddings(
                    model=self.model_name,
                    prompt=text,
                )
                embedding = response["embedding"]
                
                # Metrics gÃ¼ncelle
                self._metrics["total_embeddings"] += 1
                self._metrics["total_latency_ms"] += (time.time() - start_time) * 1000
                
                # Cache'e ekle
                if use_cache and self._cache_enabled and self._cache:
                    self._cache.set(text, embedding)
                
                return embedding
            except Exception as e:
                last_error = e
                self._metrics["errors"] += 1
                if attempt < max_retries - 1:
                    time.sleep(0.5 * (attempt + 1))  # Exponential backoff
        
        print(f"âŒ Embedding hatasÄ± ({max_retries} deneme sonrasÄ±): {last_error}")
        raise last_error
    
    def _embed_single_for_batch(self, text: str) -> Tuple[str, List[float]]:
        """Batch processing iÃ§in tek metin embed et."""
        embedding = self.embed_text(text, use_cache=False, max_retries=2)
        return (text, embedding)
    
    def embed_texts(
        self,
        texts: List[str],
        batch_size: int = 32,
        use_cache: bool = True,
        parallel: bool = True
    ) -> List[List[float]]:
        """
        Birden fazla metin iÃ§in embedding Ã¼ret.
        
        TRUE BATCH PROCESSING:
        - Parallel API calls ile hÄ±zlÄ± iÅŸleme
        - Cache hit'ler ayrÄ± iÅŸlenir
        - Thread pool kullanarak paralel embedding
        
        Args:
            texts: Embedding yapÄ±lacak metinler
            batch_size: Batch boyutu (parallel Ã§aÄŸrÄ± sayÄ±sÄ±)
            use_cache: Cache kullanÄ±lsÄ±n mÄ±
            parallel: Parallel processing kullanÄ±lsÄ±n mÄ±
            
        Returns:
            Embedding vektÃ¶rleri listesi
        """
        if not texts:
            return []
        
        start_time = time.time()
        embeddings = [None] * len(texts)  # SÄ±rayÄ± korumak iÃ§in
        texts_to_embed = []  # Cache'de olmayan metinler
        text_indices = []  # Orijinal index'leri
        
        # 1. Cache kontrolÃ¼ - hit'leri ayÄ±r
        for i, text in enumerate(texts):
            if use_cache and self._cache_enabled and self._cache:
                cached = self._cache.get(text)
                if cached is not None:
                    embeddings[i] = cached
                    self._metrics["cache_hits"] += 1
                    continue
            
            texts_to_embed.append(text)
            text_indices.append(i)
            self._metrics["cache_misses"] += 1
        
        # 2. Cache'de olmayanlarÄ± embed et
        if texts_to_embed:
            self._metrics["batch_calls"] += 1
            
            if parallel and len(texts_to_embed) > 1:
                # PARALLEL PROCESSING - ThreadPoolExecutor ile
                try:
                    futures = {}
                    for idx, text in zip(text_indices, texts_to_embed):
                        future = self._executor.submit(self._embed_single_for_batch, text)
                        futures[future] = idx
                    
                    for future in as_completed(futures):
                        idx = futures[future]
                        try:
                            text, embedding = future.result(timeout=30)
                            embeddings[idx] = embedding
                            
                            # Cache'e ekle
                            if use_cache and self._cache_enabled and self._cache:
                                self._cache.set(text, embedding)
                        except Exception as e:
                            print(f"âš ï¸ Parallel embedding error at index {idx}: {e}")
                            # Fallback: zero vector
                            embeddings[idx] = [0.0] * self.dimension
                except Exception as e:
                    print(f"âš ï¸ Parallel processing failed, falling back to sequential: {e}")
                    parallel = False
            
            if not parallel:
                # SEQUENTIAL PROCESSING
                for i, (idx, text) in enumerate(zip(text_indices, texts_to_embed)):
                    try:
                        embedding = self.embed_text(text, use_cache=use_cache)
                        embeddings[idx] = embedding
                    except Exception as e:
                        print(f"âš ï¸ Embedding error at index {idx}: {e}")
                        embeddings[idx] = [0.0] * self.dimension
                    
                    # Progress indicator (her 10 metin iÃ§in)
                    if (i + 1) % 10 == 0:
                        elapsed = time.time() - start_time
                        rate = (i + 1) / elapsed if elapsed > 0 else 0
                        print(f"ğŸ“Š Embedding progress: {i + 1}/{len(texts_to_embed)} ({rate:.1f}/s)")
        
        # 3. SonuÃ§ kontrolÃ¼
        total_time = time.time() - start_time
        print(f"âœ… Embedded {len(texts)} texts in {total_time:.2f}s (cache hits: {len(texts) - len(texts_to_embed)})")
        
        return embeddings
    
    def embed_query(self, query: str) -> List[float]:
        """
        Sorgu iÃ§in embedding Ã¼ret (arama optimizasyonu).
        Cache'lenir Ã§Ã¼nkÃ¼ aynÄ± sorgular tekrarlanabilir.
        
        Args:
            query: Arama sorgusu
            
        Returns:
            Query embedding vektÃ¶rÃ¼
        """
        return self.embed_text(query, use_cache=True)
    
    def embed_document(self, document: str, use_cache: bool = False) -> List[float]:
        """
        DÃ¶kÃ¼man iÃ§in embedding Ã¼ret (indexing optimizasyonu).
        Default olarak cache'lenmez Ã§Ã¼nkÃ¼ dokÃ¼manlar genelde tek sefer iÅŸlenir.
        
        Args:
            document: DÃ¶kÃ¼man iÃ§eriÄŸi
            use_cache: Cache kullanÄ±lsÄ±n mÄ±
            
        Returns:
            Document embedding vektÃ¶rÃ¼
        """
        return self.embed_text(document, use_cache=use_cache)
    
    @staticmethod
    def normalize(embedding: List[float]) -> List[float]:
        """Embedding vektÃ¶rÃ¼nÃ¼ normalize et (L2 normalization)."""
        arr = np.array(embedding)
        norm = np.linalg.norm(arr)
        if norm > 0:
            arr = arr / norm
        return arr.tolist()
    
    @staticmethod
    def cosine_similarity(vec1: List[float], vec2: List[float]) -> float:
        """Ä°ki vektÃ¶r arasÄ±ndaki cosine similarity hesapla."""
        arr1 = np.array(vec1)
        arr2 = np.array(vec2)
        
        dot_product = np.dot(arr1, arr2)
        norm1 = np.linalg.norm(arr1)
        norm2 = np.linalg.norm(arr2)
        
        if norm1 == 0 or norm2 == 0:
            return 0.0
        
        return float(dot_product / (norm1 * norm2))
    
    def get_status(self) -> dict:
        """Embedding manager durumunu dÃ¶ndÃ¼r."""
        avg_latency = (
            self._metrics["total_latency_ms"] / self._metrics["total_embeddings"]
            if self._metrics["total_embeddings"] > 0 else 0
        )
        total_cache_ops = self._metrics["cache_hits"] + self._metrics["cache_misses"]
        cache_hit_rate = (
            self._metrics["cache_hits"] / total_cache_ops * 100
            if total_cache_ops > 0 else 0
        )
        
        status = {
            "model_name": self.model_name,
            "base_url": self.base_url,
            "dimension": self.dimension,
            "model_available": self.check_model_available(),
            "cache_enabled": self._cache_enabled,
            "metrics": {
                "total_embeddings": self._metrics["total_embeddings"],
                "cache_hits": self._metrics["cache_hits"],
                "cache_hit_rate": f"{cache_hit_rate:.1f}%",
                "avg_latency_ms": f"{avg_latency:.1f}",
                "batch_calls": self._metrics["batch_calls"],
                "errors": self._metrics["errors"],
            }
        }
        
        if self._cache_enabled and self._cache:
            status["cache_stats"] = self._cache.get_stats()
        
        return status
    
    def get_metrics(self) -> Dict:
        """Performance metrics dÃ¶ndÃ¼r."""
        return self._metrics.copy()
    
    def reset_metrics(self) -> None:
        """Metrics'i sÄ±fÄ±rla."""
        for key in self._metrics:
            self._metrics[key] = 0
    
    def clear_cache(self) -> None:
        """Embedding cache'ini temizle."""
        if self._cache:
            self._cache.clear()
    
    def get_cache_stats(self) -> Optional[Dict]:
        """Cache istatistiklerini dÃ¶ndÃ¼r."""
        if self._cache:
            return self._cache.get_stats()
        return None


# Singleton instance
embedding_manager = EmbeddingManager()
