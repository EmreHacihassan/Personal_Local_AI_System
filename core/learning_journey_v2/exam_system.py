"""
ðŸŽ“ Exam System with Feynman Technique
AI-Powered SÄ±nav ve DeÄŸerlendirme Sistemi

Bu modÃ¼l ÅŸunlarÄ± iÃ§erir:
1. Feynman TekniÄŸi SÄ±navlarÄ± - KullanÄ±cÄ± anlatÄ±r, AI deÄŸerlendirir
2. Ã‡oktan SeÃ§meli Testler
3. Problem Ã‡Ã¶zme SÄ±navlarÄ±
4. Oral Presentation deÄŸerlendirmesi
5. Concept Mapping
6. Peer Review
"""

import asyncio
import json
from datetime import datetime
from typing import Dict, List, Any, Optional, Tuple
from dataclasses import dataclass, field

from .models import (
    Exam, ExamType, ExamQuestion, Package, Stage,
    DifficultyLevel
)


# ==================== EVALUATION RESULTS ====================

@dataclass
class EvaluationCriteria:
    """DeÄŸerlendirme kriteri"""
    name: str
    max_score: float
    weight: float = 1.0
    description: str = ""

@dataclass
class CriteriaScore:
    """Kriter puanÄ±"""
    criteria_name: str
    score: float
    max_score: float
    feedback: str

@dataclass
class ExamResult:
    """SÄ±nav sonucu"""
    exam_id: str
    user_id: str
    exam_type: ExamType
    total_score: float
    max_possible_score: float
    percentage: float
    passed: bool
    attempt_number: int
    criteria_scores: List[CriteriaScore] = field(default_factory=list)
    detailed_feedback: str = ""
    suggestions: List[str] = field(default_factory=list)
    time_taken_seconds: int = 0
    submitted_at: str = field(default_factory=lambda: datetime.now().isoformat())
    ai_evaluator_model: str = ""
    
    @property
    def grade(self) -> str:
        if self.percentage >= 90:
            return "A+"
        elif self.percentage >= 85:
            return "A"
        elif self.percentage >= 80:
            return "B+"
        elif self.percentage >= 75:
            return "B"
        elif self.percentage >= 70:
            return "C+"
        elif self.percentage >= 65:
            return "C"
        elif self.percentage >= 60:
            return "D"
        else:
            return "F"
    
    def to_dict(self) -> Dict[str, Any]:
        return {
            "exam_id": self.exam_id,
            "user_id": self.user_id,
            "exam_type": self.exam_type.value,
            "total_score": self.total_score,
            "max_possible_score": self.max_possible_score,
            "percentage": round(self.percentage, 1),
            "grade": self.grade,
            "passed": self.passed,
            "attempt_number": self.attempt_number,
            "criteria_scores": [
                {
                    "name": c.criteria_name,
                    "score": c.score,
                    "max_score": c.max_score,
                    "feedback": c.feedback
                } for c in self.criteria_scores
            ],
            "detailed_feedback": self.detailed_feedback,
            "suggestions": self.suggestions,
            "time_taken_seconds": self.time_taken_seconds,
            "submitted_at": self.submitted_at
        }


# ==================== FEYNMAN EXAM EVALUATOR ====================

class FeynmanExamEvaluator:
    """
    Feynman TekniÄŸi SÄ±nav DeÄŸerlendiricisi
    
    Richard Feynman'Ä±n Ã¶ÄŸrenme tekniÄŸi:
    1. Konuyu seÃ§
    2. Bir Ã§ocuÄŸa anlatÄ±r gibi basit bir dille aÃ§Ä±kla
    3. Eksik kaldÄ±ÄŸÄ±n yerleri tespit et
    4. Geri dÃ¶n, Ã¶ÄŸren, basitleÅŸtir
    
    Bu sÄ±nav tÃ¼rÃ¼nde:
    - KullanÄ±cÄ± konuyu kendi cÃ¼mleleriyle anlatÄ±r
    - AI, anlatÄ±mÄ± deÄŸerlendirir
    - Eksik/yanlÄ±ÅŸ kavramlarÄ± tespit eder
    - Geri bildirim verir
    """
    
    EVALUATION_CRITERIA = [
        EvaluationCriteria(
            name="concept_accuracy",
            max_score=30,
            weight=1.5,
            description="KavramlarÄ±n doÄŸruluÄŸu ve eksiksizliÄŸi"
        ),
        EvaluationCriteria(
            name="simplicity",
            max_score=20,
            weight=1.0,
            description="Basit ve anlaÅŸÄ±lÄ±r dil kullanÄ±mÄ±"
        ),
        EvaluationCriteria(
            name="examples",
            max_score=15,
            weight=1.0,
            description="Uygun Ã¶rnekler kullanÄ±mÄ±"
        ),
        EvaluationCriteria(
            name="logical_flow",
            max_score=15,
            weight=1.0,
            description="MantÄ±ksal akÄ±ÅŸ ve tutarlÄ±lÄ±k"
        ),
        EvaluationCriteria(
            name="completeness",
            max_score=20,
            weight=1.2,
            description="Konunun eksiksiz iÅŸlenmesi"
        )
    ]
    
    def __init__(self, llm_service=None):
        self.llm_service = llm_service
    
    async def evaluate(
        self, 
        exam: Exam, 
        user_explanation: str,
        user_id: str,
        attempt_number: int = 1,
        audio_transcript: Optional[str] = None
    ) -> ExamResult:
        """
        Feynman sÄ±navÄ±nÄ± deÄŸerlendir
        
        Args:
            exam: Feynman sÄ±navÄ±
            user_explanation: KullanÄ±cÄ±nÄ±n yazÄ±lÄ± aÃ§Ä±klamasÄ±
            user_id: KullanÄ±cÄ± ID
            attempt_number: Deneme numarasÄ±
            audio_transcript: Sesli anlatÄ±m transkripsiyonu (opsiyonel)
        
        Returns:
            ExamResult
        """
        
        config = exam.feynman_config or {}
        topic = config.get("topic", "Konu")
        subtopics = config.get("subtopics", [])
        required_concepts = config.get("required_concepts", [])
        min_words = config.get("min_explanation_words", 100)
        
        # Metin birleÅŸtir
        full_text = user_explanation
        if audio_transcript:
            full_text = f"{user_explanation}\n\nSesli AnlatÄ±m:\n{audio_transcript}"
        
        # Kelime sayÄ±sÄ± kontrolÃ¼
        word_count = len(full_text.split())
        word_penalty = 0
        if word_count < min_words:
            word_penalty = (min_words - word_count) / min_words * 20
        
        # AI ile deÄŸerlendirme
        if self.llm_service:
            evaluation = await self._evaluate_with_llm(
                topic=topic,
                subtopics=subtopics,
                required_concepts=required_concepts,
                user_text=full_text
            )
        else:
            # Mock deÄŸerlendirme (LLM olmadan)
            evaluation = self._mock_evaluation(
                topic=topic,
                subtopics=subtopics,
                required_concepts=required_concepts,
                user_text=full_text
            )
        
        # Kriterlere gÃ¶re puanlama
        criteria_scores = []
        total_score = 0
        max_score = 0
        
        for criteria in self.EVALUATION_CRITERIA:
            score = evaluation.get(criteria.name, {}).get("score", 0)
            feedback = evaluation.get(criteria.name, {}).get("feedback", "")
            
            # Kelime cezasÄ± uygula
            if criteria.name == "completeness":
                score = max(0, score - word_penalty)
            
            weighted_score = score * criteria.weight
            weighted_max = criteria.max_score * criteria.weight
            
            criteria_scores.append(CriteriaScore(
                criteria_name=criteria.name,
                score=round(weighted_score, 1),
                max_score=weighted_max,
                feedback=feedback
            ))
            
            total_score += weighted_score
            max_score += weighted_max
        
        percentage = (total_score / max_score * 100) if max_score > 0 else 0
        
        # Ã–neriler oluÅŸtur
        suggestions = evaluation.get("suggestions", [])
        if not suggestions:
            suggestions = self._generate_suggestions(evaluation, topic, subtopics)
        
        # DetaylÄ± geri bildirim
        detailed_feedback = evaluation.get("overall_feedback", "")
        if not detailed_feedback:
            detailed_feedback = self._generate_detailed_feedback(
                evaluation, topic, percentage
            )
        
        return ExamResult(
            exam_id=exam.id,
            user_id=user_id,
            exam_type=ExamType.FEYNMAN,
            total_score=round(total_score, 1),
            max_possible_score=round(max_score, 1),
            percentage=round(percentage, 1),
            passed=percentage >= exam.passing_score,
            attempt_number=attempt_number,
            criteria_scores=criteria_scores,
            detailed_feedback=detailed_feedback,
            suggestions=suggestions,
            ai_evaluator_model="gpt-4o" if self.llm_service else "mock"
        )
    
    async def _evaluate_with_llm(
        self,
        topic: str,
        subtopics: List[str],
        required_concepts: List[str],
        user_text: str
    ) -> Dict[str, Any]:
        """LLM ile deÄŸerlendirme"""
        
        prompt = f"""Sen bir Feynman TekniÄŸi deÄŸerlendirmecisisin. Ã–ÄŸrencinin aÅŸaÄŸÄ±daki konu hakkÄ±ndaki aÃ§Ä±klamasÄ±nÄ± deÄŸerlendir.

**Konu:** {topic}
**Alt Konular:** {', '.join(subtopics)}
**Gerekli Kavramlar:** {', '.join(required_concepts)}

**Ã–ÄŸrencinin AÃ§Ä±klamasÄ±:**
{user_text}

**DeÄŸerlendirme Kriterleri:**
1. concept_accuracy (0-30): KavramlarÄ±n doÄŸruluÄŸu
2. simplicity (0-20): Basit dil kullanÄ±mÄ±
3. examples (0-15): Ã–rnek kullanÄ±mÄ±
4. logical_flow (0-15): MantÄ±ksal akÄ±ÅŸ
5. completeness (0-20): Konunun eksiksizliÄŸi

YanÄ±tÄ±nÄ± ÅŸu JSON formatÄ±nda ver:
{{
    "concept_accuracy": {{"score": X, "feedback": "..."}},
    "simplicity": {{"score": X, "feedback": "..."}},
    "examples": {{"score": X, "feedback": "..."}},
    "logical_flow": {{"score": X, "feedback": "..."}},
    "completeness": {{"score": X, "feedback": "..."}},
    "missing_concepts": ["..."],
    "incorrect_concepts": ["..."],
    "suggestions": ["..."],
    "overall_feedback": "..."
}}"""

        try:
            response = await self.llm_service.generate(prompt, json_mode=True)
            return json.loads(response)
        except:
            return self._mock_evaluation(topic, subtopics, required_concepts, user_text)
    
    def _mock_evaluation(
        self,
        topic: str,
        subtopics: List[str],
        required_concepts: List[str],
        user_text: str
    ) -> Dict[str, Any]:
        """Mock deÄŸerlendirme (test iÃ§in)"""
        
        word_count = len(user_text.split())
        base_score = min(80, word_count / 5)  # Her 5 kelime iÃ§in 1 puan
        
        # Konu kelimelerinin geÃ§ip geÃ§mediÄŸini kontrol et
        topic_mentions = sum(1 for t in subtopics if t.lower() in user_text.lower())
        topic_bonus = topic_mentions * 2
        
        return {
            "concept_accuracy": {
                "score": min(30, base_score * 0.3 + topic_bonus),
                "feedback": f"Konuyu genel olarak doÄŸru anlatmÄ±ÅŸsÄ±n. {topic_mentions}/{len(subtopics)} alt konuya deÄŸindin."
            },
            "simplicity": {
                "score": min(20, base_score * 0.2),
                "feedback": "Dil kullanÄ±mÄ±n anlaÅŸÄ±lÄ±r."
            },
            "examples": {
                "score": min(15, base_score * 0.15),
                "feedback": "Ã–rnekler eklemen anlatÄ±mÄ± gÃ¼Ã§lendirir."
            },
            "logical_flow": {
                "score": min(15, base_score * 0.15),
                "feedback": "AnlatÄ±mÄ±n mantÄ±ksal bir akÄ±ÅŸ izliyor."
            },
            "completeness": {
                "score": min(20, base_score * 0.2),
                "feedback": f"Kelime sayÄ±sÄ±: {word_count}. Daha fazla detay ekleyebilirsin."
            },
            "missing_concepts": [s for s in subtopics if s.lower() not in user_text.lower()],
            "incorrect_concepts": [],
            "suggestions": [
                "Daha fazla Ã¶rnek ekle",
                "KavramlarÄ± basit kelimelerle aÃ§Ä±kla",
                "Konu arasÄ±ndaki baÄŸlantÄ±larÄ± gÃ¶ster"
            ],
            "overall_feedback": f"{topic} konusunu kendi cÃ¼mlelerinle anlatmaya Ã§alÄ±ÅŸmÄ±ÅŸsÄ±n. Devam et!"
        }
    
    def _generate_suggestions(
        self, 
        evaluation: Dict[str, Any],
        topic: str,
        subtopics: List[str]
    ) -> List[str]:
        """Ã–neriler oluÅŸtur"""
        suggestions = []
        
        missing = evaluation.get("missing_concepts", [])
        if missing:
            suggestions.append(f"Åžu konulara deÄŸinmeyi unuttun: {', '.join(missing[:3])}")
        
        incorrect = evaluation.get("incorrect_concepts", [])
        if incorrect:
            suggestions.append(f"Åžu kavramlarÄ± gÃ¶zden geÃ§ir: {', '.join(incorrect[:3])}")
        
        if evaluation.get("examples", {}).get("score", 0) < 10:
            suggestions.append("GÃ¼nlÃ¼k hayattan Ã¶rnekler ekleyerek konuyu somutlaÅŸtÄ±r")
        
        if evaluation.get("simplicity", {}).get("score", 0) < 15:
            suggestions.append("Teknik terimleri daha basit kelimelerle aÃ§Ä±kla")
        
        if not suggestions:
            suggestions.append("Harika iÅŸ! Åžimdi farklÄ± bir kitleye anlatmayÄ± dene")
        
        return suggestions
    
    def _generate_detailed_feedback(
        self,
        evaluation: Dict[str, Any],
        topic: str,
        percentage: float
    ) -> str:
        """DetaylÄ± geri bildirim oluÅŸtur"""
        
        if percentage >= 90:
            intro = f"MÃ¼kemmel! {topic} konusunu Ã§ok iyi anlatmÄ±ÅŸsÄ±n. ðŸŒŸ"
        elif percentage >= 75:
            intro = f"GÃ¼zel iÅŸ! {topic} konusunu iyi kavramÄ±ÅŸsÄ±n. ðŸ‘"
        elif percentage >= 60:
            intro = f"Ä°yi baÅŸlangÄ±Ã§! {topic} konusunda geliÅŸim gÃ¶steriyorsun. ðŸ’ª"
        else:
            intro = f"{topic} konusunu tekrar gÃ¶zden geÃ§irmeni Ã¶neririm. ðŸ“š"
        
        strengths = []
        weaknesses = []
        
        for key in ["concept_accuracy", "simplicity", "examples", "logical_flow", "completeness"]:
            criteria = evaluation.get(key, {})
            score = criteria.get("score", 0)
            max_scores = {"concept_accuracy": 30, "simplicity": 20, "examples": 15, "logical_flow": 15, "completeness": 20}
            
            ratio = score / max_scores.get(key, 1)
            
            if ratio >= 0.8:
                strengths.append(key.replace("_", " ").title())
            elif ratio < 0.5:
                weaknesses.append(key.replace("_", " ").title())
        
        feedback = intro + "\n\n"
        
        if strengths:
            feedback += f"**GÃ¼Ã§lÃ¼ YÃ¶nlerin:** {', '.join(strengths)}\n"
        
        if weaknesses:
            feedback += f"**GeliÅŸtirilebilir YÃ¶nlerin:** {', '.join(weaknesses)}\n"
        
        missing = evaluation.get("missing_concepts", [])
        if missing:
            feedback += f"\n**Eksik Kalan Konular:** {', '.join(missing[:5])}"
        
        return feedback


# ==================== MULTIPLE CHOICE EVALUATOR ====================

class MultipleChoiceEvaluator:
    """Ã‡oktan seÃ§meli sÄ±nav deÄŸerlendiricisi"""
    
    def evaluate(
        self,
        exam: Exam,
        answers: Dict[str, str],  # question_id -> selected_answer
        user_id: str,
        attempt_number: int = 1,
        time_taken_seconds: int = 0
    ) -> ExamResult:
        """Ã‡oktan seÃ§meli sÄ±navÄ± deÄŸerlendir"""
        
        correct_count = 0
        total_points = 0
        earned_points = 0
        criteria_scores = []
        
        for question in exam.questions:
            total_points += question.points
            user_answer = answers.get(question.id, "")
            
            is_correct = user_answer.upper() == question.correct_answer.upper()
            if is_correct:
                correct_count += 1
                earned_points += question.points
            
            criteria_scores.append(CriteriaScore(
                criteria_name=f"soru_{question.id[:8]}",
                score=question.points if is_correct else 0,
                max_score=question.points,
                feedback=question.explanation if not is_correct else "DoÄŸru!"
            ))
        
        percentage = (earned_points / total_points * 100) if total_points > 0 else 0
        
        return ExamResult(
            exam_id=exam.id,
            user_id=user_id,
            exam_type=ExamType.MULTIPLE_CHOICE,
            total_score=earned_points,
            max_possible_score=total_points,
            percentage=percentage,
            passed=percentage >= exam.passing_score,
            attempt_number=attempt_number,
            criteria_scores=criteria_scores,
            detailed_feedback=f"{correct_count}/{len(exam.questions)} soru doÄŸru.",
            time_taken_seconds=time_taken_seconds
        )


# ==================== PROBLEM SOLVING EVALUATOR ====================

class ProblemSolvingEvaluator:
    """Problem Ã§Ã¶zme sÄ±navÄ± deÄŸerlendiricisi"""
    
    def __init__(self, llm_service=None):
        self.llm_service = llm_service
    
    async def evaluate(
        self,
        exam: Exam,
        solutions: Dict[str, str],  # question_id -> solution_text
        user_id: str,
        attempt_number: int = 1,
        time_taken_seconds: int = 0
    ) -> ExamResult:
        """Problem Ã§Ã¶zme sÄ±navÄ±nÄ± deÄŸerlendir"""
        
        total_points = 0
        earned_points = 0
        criteria_scores = []
        
        for question in exam.questions:
            total_points += question.points
            solution = solutions.get(question.id, "")
            
            if self.llm_service:
                score, feedback = await self._evaluate_solution_with_llm(
                    question, solution
                )
            else:
                score, feedback = self._mock_evaluate_solution(question, solution)
            
            earned_points += score
            criteria_scores.append(CriteriaScore(
                criteria_name=f"problem_{question.id[:8]}",
                score=score,
                max_score=question.points,
                feedback=feedback
            ))
        
        percentage = (earned_points / total_points * 100) if total_points > 0 else 0
        
        return ExamResult(
            exam_id=exam.id,
            user_id=user_id,
            exam_type=ExamType.PROBLEM_SOLVING,
            total_score=earned_points,
            max_possible_score=total_points,
            percentage=percentage,
            passed=percentage >= exam.passing_score,
            attempt_number=attempt_number,
            criteria_scores=criteria_scores,
            detailed_feedback=f"Toplam {earned_points}/{total_points} puan aldÄ±n.",
            time_taken_seconds=time_taken_seconds
        )
    
    async def _evaluate_solution_with_llm(
        self, 
        question: ExamQuestion, 
        solution: str
    ) -> Tuple[float, str]:
        """LLM ile Ã§Ã¶zÃ¼m deÄŸerlendir"""
        
        prompt = f"""AÅŸaÄŸÄ±daki matematik probleminin Ã§Ã¶zÃ¼mÃ¼nÃ¼ deÄŸerlendir.

**Problem:**
{question.question}

**Ã–ÄŸrenci Ã‡Ã¶zÃ¼mÃ¼:**
{solution}

**Rubrik:**
{json.dumps(question.rubric, ensure_ascii=False) if question.rubric else "Genel deÄŸerlendirme"}

**Maksimum Puan:** {question.points}

YanÄ±tÄ±nÄ± ÅŸu formatta ver:
{{
    "score": X,
    "feedback": "DeÄŸerlendirme ve aÃ§Ä±klama..."
}}"""

        try:
            response = await self.llm_service.generate(prompt, json_mode=True)
            data = json.loads(response)
            return data.get("score", 0), data.get("feedback", "")
        except:
            return self._mock_evaluate_solution(question, solution)
    
    def _mock_evaluate_solution(
        self, 
        question: ExamQuestion, 
        solution: str
    ) -> Tuple[float, str]:
        """Mock Ã§Ã¶zÃ¼m deÄŸerlendirmesi"""
        
        # Basit heuristik: kelime sayÄ±sÄ±na gÃ¶re puan
        word_count = len(solution.split())
        
        if word_count < 10:
            score = question.points * 0.2
            feedback = "Ã‡Ã¶zÃ¼mÃ¼n Ã§ok kÄ±sa. AdÄ±mlarÄ± detaylÄ± gÃ¶ster."
        elif word_count < 30:
            score = question.points * 0.5
            feedback = "Ä°yi baÅŸlangÄ±Ã§ ama daha fazla aÃ§Ä±klama gerekiyor."
        elif word_count < 50:
            score = question.points * 0.75
            feedback = "GÃ¼zel Ã§Ã¶zÃ¼m! BirkaÃ§ detay eksik kalmÄ±ÅŸ."
        else:
            score = question.points * 0.9
            feedback = "KapsamlÄ± ve detaylÄ± bir Ã§Ã¶zÃ¼m. Harika!"
        
        return round(score, 1), feedback


# ==================== TEACH BACK EVALUATOR ====================

class TeachBackEvaluator:
    """
    Teach-Back (Ã–ÄŸreterek Ã–ÄŸrenme) DeÄŸerlendiricisi
    
    KullanÄ±cÄ± konuyu baÅŸka birine Ã¶ÄŸretiyormuÅŸ gibi anlatÄ±r.
    AI, bir Ã¶ÄŸrenci gibi sorular sorabilir.
    """
    
    def __init__(self, llm_service=None):
        self.llm_service = llm_service
    
    async def evaluate(
        self,
        exam: Exam,
        teaching_content: str,
        user_id: str,
        attempt_number: int = 1,
        qa_history: Optional[List[Dict[str, str]]] = None
    ) -> ExamResult:
        """Teach-back sÄ±navÄ±nÄ± deÄŸerlendir"""
        
        # Feynman benzeri deÄŸerlendirme + etkileÅŸim puanÄ±
        base_evaluator = FeynmanExamEvaluator(self.llm_service)
        result = await base_evaluator.evaluate(
            exam, teaching_content, user_id, attempt_number
        )
        
        # Soru-cevap etkileÅŸimi varsa ekstra puan
        if qa_history and len(qa_history) > 0:
            interaction_bonus = min(10, len(qa_history) * 2)
            result.total_score += interaction_bonus
            result.percentage = (result.total_score / result.max_possible_score * 100)
            result.suggestions.append(
                f"Harika! {len(qa_history)} soruya cevap verdin."
            )
        
        result.exam_type = ExamType.TEACH_BACK
        return result


# ==================== CONCEPT MAP EVALUATOR ====================

class ConceptMapEvaluator:
    """
    Kavram HaritasÄ± DeÄŸerlendiricisi
    
    KullanÄ±cÄ± kavramlar arasÄ± iliÅŸkileri gÃ¶rselleÅŸtirir.
    """
    
    def __init__(self, llm_service=None):
        self.llm_service = llm_service
    
    async def evaluate(
        self,
        exam: Exam,
        concept_map: Dict[str, Any],  # nodes ve edges iÃ§eren yapÄ±
        user_id: str,
        attempt_number: int = 1
    ) -> ExamResult:
        """Kavram haritasÄ±nÄ± deÄŸerlendir"""
        
        nodes = concept_map.get("nodes", [])
        edges = concept_map.get("edges", [])
        
        # Puanlama kriterleri
        node_count = len(nodes)
        edge_count = len(edges)
        
        # DÃ¼ÄŸÃ¼m sayÄ±sÄ± puanÄ±
        node_score = min(30, node_count * 3)
        
        # BaÄŸlantÄ± sayÄ±sÄ± puanÄ±
        edge_score = min(30, edge_count * 2)
        
        # BaÄŸlantÄ±/dÃ¼ÄŸÃ¼m oranÄ± (iyi bir haritada her dÃ¼ÄŸÃ¼mÃ¼n en az 1-2 baÄŸlantÄ±sÄ± olmalÄ±)
        ratio = edge_count / max(1, node_count)
        ratio_score = min(20, ratio * 10)
        
        # Etiketli baÄŸlantÄ± puanÄ±
        labeled_edges = sum(1 for e in edges if e.get("label"))
        label_score = min(20, (labeled_edges / max(1, edge_count)) * 20)
        
        total_score = node_score + edge_score + ratio_score + label_score
        max_score = 100
        percentage = total_score
        
        criteria_scores = [
            CriteriaScore("node_coverage", node_score, 30, f"{node_count} kavram eklendi"),
            CriteriaScore("connections", edge_score, 30, f"{edge_count} baÄŸlantÄ± oluÅŸturuldu"),
            CriteriaScore("interconnection", ratio_score, 20, f"BaÄŸlantÄ± oranÄ±: {ratio:.1f}"),
            CriteriaScore("labeled_relations", label_score, 20, f"{labeled_edges} etiketli baÄŸlantÄ±")
        ]
        
        return ExamResult(
            exam_id=exam.id,
            user_id=user_id,
            exam_type=ExamType.CONCEPT_MAP,
            total_score=total_score,
            max_possible_score=max_score,
            percentage=percentage,
            passed=percentage >= exam.passing_score,
            attempt_number=attempt_number,
            criteria_scores=criteria_scores,
            detailed_feedback=f"Kavram haritanda {node_count} kavram ve {edge_count} baÄŸlantÄ± var.",
            suggestions=[
                "Daha fazla kavram ekle" if node_count < 8 else "Kavram sayÄ±sÄ± yeterli",
                "Kavramlar arasÄ± daha fazla baÄŸlantÄ± kur" if ratio < 1.5 else "BaÄŸlantÄ±lar iyi",
                "BaÄŸlantÄ±lara aÃ§Ä±klayÄ±cÄ± etiketler ekle" if labeled_edges < edge_count * 0.5 else "Etiketler iyi"
            ]
        )


# ==================== TRUE/FALSE EVALUATOR ====================

class TrueFalseEvaluator:
    """
    DoÄŸru/YanlÄ±ÅŸ SÄ±nav DeÄŸerlendiricisi
    
    Basit doÄŸru/yanlÄ±ÅŸ ifadelerini deÄŸerlendirir.
    """
    
    def evaluate(
        self,
        exam: Exam,
        answers: Dict[str, bool],  # question_id -> True/False
        user_id: str,
        attempt_number: int = 1,
        time_taken_seconds: int = 0
    ) -> ExamResult:
        """DoÄŸru/YanlÄ±ÅŸ sÄ±navÄ±nÄ± deÄŸerlendir"""
        
        correct_count = 0
        total_points = 0
        earned_points = 0
        criteria_scores = []
        
        for question in exam.questions:
            total_points += question.points
            user_answer = answers.get(question.id)
            
            # correct_answer "True", "False", "DoÄŸru", "YanlÄ±ÅŸ" olabilir
            correct_val = question.correct_answer
            if isinstance(correct_val, str):
                correct_val = correct_val.lower() in ["true", "doÄŸru", "evet", "1"]
            
            is_correct = user_answer == correct_val
            if is_correct:
                correct_count += 1
                earned_points += question.points
            
            criteria_scores.append(CriteriaScore(
                criteria_name=f"tf_{question.id[:8]}",
                score=question.points if is_correct else 0,
                max_score=question.points,
                feedback="DoÄŸru!" if is_correct else f"YanlÄ±ÅŸ. DoÄŸru cevap: {'DoÄŸru' if correct_val else 'YanlÄ±ÅŸ'}"
            ))
        
        percentage = (earned_points / total_points * 100) if total_points > 0 else 0
        
        return ExamResult(
            exam_id=exam.id,
            user_id=user_id,
            exam_type=ExamType.TRUE_FALSE,
            total_score=earned_points,
            max_possible_score=total_points,
            percentage=percentage,
            passed=percentage >= exam.passing_score,
            attempt_number=attempt_number,
            criteria_scores=criteria_scores,
            detailed_feedback=f"{correct_count}/{len(exam.questions)} ifade doÄŸru.",
            time_taken_seconds=time_taken_seconds
        )


# ==================== FILL IN THE BLANK EVALUATOR ====================

class FillBlankEvaluator:
    """
    BoÅŸluk Doldurma SÄ±nav DeÄŸerlendiricisi
    
    Fuzzy matching ile boÅŸluk doldurma sorularÄ±nÄ± deÄŸerlendirir.
    """
    
    def __init__(self, llm_service=None):
        self.llm_service = llm_service
    
    def _normalize(self, text: str) -> str:
        """Metni normalize et - karÅŸÄ±laÅŸtÄ±rma iÃ§in"""
        import re
        text = text.lower().strip()
        text = re.sub(r'[^\w\s]', '', text)  # Noktalama kaldÄ±r
        text = re.sub(r'\s+', ' ', text)  # Ã‡oklu boÅŸluklarÄ± tek boÅŸluÄŸa Ã§evir
        return text
    
    def _fuzzy_match(self, user_answer: str, correct_answers: List[str], threshold: float = 0.8) -> Tuple[bool, float]:
        """Fuzzy matching ile cevap kontrolÃ¼"""
        user_norm = self._normalize(user_answer)
        
        best_score = 0.0
        for correct in correct_answers:
            correct_norm = self._normalize(correct)
            
            # Tam eÅŸleÅŸme kontrolÃ¼
            if user_norm == correct_norm:
                return True, 1.0
            
            # Basit benzerlik hesabÄ± (Jaccard benzeri)
            user_words = set(user_norm.split())
            correct_words = set(correct_norm.split())
            
            if not correct_words:
                continue
            
            intersection = len(user_words & correct_words)
            union = len(user_words | correct_words)
            similarity = intersection / union if union > 0 else 0
            
            best_score = max(best_score, similarity)
        
        return best_score >= threshold, best_score
    
    def evaluate(
        self,
        exam: Exam,
        answers: Dict[str, str],  # question_id -> user's answer
        user_id: str,
        attempt_number: int = 1,
        time_taken_seconds: int = 0
    ) -> ExamResult:
        """BoÅŸluk doldurma sÄ±navÄ±nÄ± deÄŸerlendir"""
        
        total_points = 0
        earned_points = 0
        criteria_scores = []
        
        for question in exam.questions:
            total_points += question.points
            user_answer = answers.get(question.id, "")
            
            # DoÄŸru cevaplar listesi (birden fazla kabul edilebilir cevap olabilir)
            correct_answers = [question.correct_answer]
            if question.hints:  # hints'i alternatif cevaplar olarak kullan
                correct_answers.extend(question.hints)
            
            is_correct, score_ratio = self._fuzzy_match(user_answer, correct_answers)
            
            if is_correct:
                score = question.points * score_ratio
            elif score_ratio >= 0.5:  # KÄ±smi puan
                score = question.points * score_ratio * 0.5
            else:
                score = 0
            
            earned_points += score
            
            criteria_scores.append(CriteriaScore(
                criteria_name=f"blank_{question.id[:8]}",
                score=round(score, 1),
                max_score=question.points,
                feedback="DoÄŸru!" if is_correct else f"Beklenen: {question.correct_answer}"
            ))
        
        percentage = (earned_points / total_points * 100) if total_points > 0 else 0
        
        return ExamResult(
            exam_id=exam.id,
            user_id=user_id,
            exam_type=ExamType.FILL_BLANK,
            total_score=round(earned_points, 1),
            max_possible_score=total_points,
            percentage=round(percentage, 1),
            passed=percentage >= exam.passing_score,
            attempt_number=attempt_number,
            criteria_scores=criteria_scores,
            detailed_feedback=f"Toplam {round(earned_points, 1)}/{total_points} puan.",
            time_taken_seconds=time_taken_seconds
        )


# ==================== SHORT ANSWER EVALUATOR ====================

class ShortAnswerEvaluator:
    """
    KÄ±sa Cevap SÄ±nav DeÄŸerlendiricisi
    
    LLM ile semantik deÄŸerlendirme + anahtar kelime kontrolÃ¼.
    """
    
    def __init__(self, llm_service=None):
        self.llm_service = llm_service
    
    async def evaluate(
        self,
        exam: Exam,
        answers: Dict[str, str],  # question_id -> answer text
        user_id: str,
        attempt_number: int = 1,
        time_taken_seconds: int = 0
    ) -> ExamResult:
        """KÄ±sa cevap sÄ±navÄ±nÄ± deÄŸerlendir"""
        
        total_points = 0
        earned_points = 0
        criteria_scores = []
        
        for question in exam.questions:
            total_points += question.points
            user_answer = answers.get(question.id, "")
            
            if self.llm_service:
                score, feedback = await self._evaluate_with_llm(question, user_answer)
            else:
                score, feedback = self._keyword_evaluate(question, user_answer)
            
            earned_points += score
            criteria_scores.append(CriteriaScore(
                criteria_name=f"short_{question.id[:8]}",
                score=round(score, 1),
                max_score=question.points,
                feedback=feedback
            ))
        
        percentage = (earned_points / total_points * 100) if total_points > 0 else 0
        
        return ExamResult(
            exam_id=exam.id,
            user_id=user_id,
            exam_type=ExamType.SHORT_ANSWER,
            total_score=round(earned_points, 1),
            max_possible_score=total_points,
            percentage=round(percentage, 1),
            passed=percentage >= exam.passing_score,
            attempt_number=attempt_number,
            criteria_scores=criteria_scores,
            detailed_feedback=f"Toplam {round(earned_points, 1)}/{total_points} puan.",
            time_taken_seconds=time_taken_seconds
        )
    
    async def _evaluate_with_llm(self, question: ExamQuestion, answer: str) -> Tuple[float, str]:
        """LLM ile kÄ±sa cevabÄ± deÄŸerlendir"""
        prompt = f"""AÅŸaÄŸÄ±daki kÄ±sa cevabÄ± deÄŸerlendir.

**Soru:** {question.question}
**DoÄŸru Cevap:** {question.correct_answer}
**Ã–ÄŸrenci CevabÄ±:** {answer}
**Maksimum Puan:** {question.points}

YanÄ±tÄ±nÄ± ÅŸu formatta ver:
{{"score": X, "feedback": "DeÄŸerlendirme..."}}"""
        
        try:
            response = await self.llm_service.generate(prompt, json_mode=True)
            data = json.loads(response)
            return min(data.get("score", 0), question.points), data.get("feedback", "")
        except:
            return self._keyword_evaluate(question, answer)
    
    def _keyword_evaluate(self, question: ExamQuestion, answer: str) -> Tuple[float, str]:
        """Anahtar kelime bazlÄ± deÄŸerlendirme"""
        answer_lower = answer.lower()
        correct_lower = question.correct_answer.lower()
        
        # Anahtar kelimeler
        correct_words = set(correct_lower.split())
        answer_words = set(answer_lower.split())
        
        matching_words = correct_words & answer_words
        match_ratio = len(matching_words) / max(len(correct_words), 1)
        
        score = question.points * match_ratio
        
        if match_ratio >= 0.8:
            feedback = "Harika! CevabÄ±n doÄŸru."
        elif match_ratio >= 0.5:
            feedback = "KÄ±smen doÄŸru. BazÄ± anahtar kavramlar eksik."
        else:
            feedback = f"YanlÄ±ÅŸ veya eksik. Beklenen: {question.correct_answer}"
        
        return round(score, 1), feedback


# ==================== ESSAY EVALUATOR ====================

class EssayEvaluator:
    """
    Kompozisyon (Essay) DeÄŸerlendiricisi
    
    Rubric bazlÄ± LLM deÄŸerlendirme ile kapsamlÄ± essay kontrolÃ¼.
    """
    
    EVALUATION_CRITERIA = [
        EvaluationCriteria("content", 30, 1.0, "Ä°Ã§erik ve tema uygunluÄŸu"),
        EvaluationCriteria("organization", 25, 1.0, "YapÄ± ve organizasyon"),
        EvaluationCriteria("language", 25, 1.0, "Dil kullanÄ±mÄ± ve gramer"),
        EvaluationCriteria("creativity", 20, 1.0, "YaratÄ±cÄ±lÄ±k ve Ã¶zgÃ¼nlÃ¼k"),
    ]
    
    def __init__(self, llm_service=None):
        self.llm_service = llm_service
    
    async def evaluate(
        self,
        exam: Exam,
        essay_text: str,
        user_id: str,
        attempt_number: int = 1,
        time_taken_seconds: int = 0
    ) -> ExamResult:
        """Kompozisyonu deÄŸerlendir"""
        
        if self.llm_service:
            evaluation = await self._evaluate_with_llm(exam, essay_text)
        else:
            evaluation = self._mock_evaluation(essay_text)
        
        criteria_scores = []
        total_score = 0
        max_score = 100
        
        for criteria in self.EVALUATION_CRITERIA:
            score = evaluation.get(criteria.name, {}).get("score", 0)
            feedback = evaluation.get(criteria.name, {}).get("feedback", "")
            
            criteria_scores.append(CriteriaScore(
                criteria_name=criteria.name,
                score=score,
                max_score=criteria.max_score,
                feedback=feedback
            ))
            total_score += score
        
        percentage = (total_score / max_score * 100)
        
        return ExamResult(
            exam_id=exam.id,
            user_id=user_id,
            exam_type=ExamType.ESSAY,
            total_score=total_score,
            max_possible_score=max_score,
            percentage=percentage,
            passed=percentage >= exam.passing_score,
            attempt_number=attempt_number,
            criteria_scores=criteria_scores,
            detailed_feedback=evaluation.get("overall_feedback", ""),
            suggestions=evaluation.get("suggestions", []),
            time_taken_seconds=time_taken_seconds
        )
    
    async def _evaluate_with_llm(self, exam: Exam, essay_text: str) -> Dict[str, Any]:
        """LLM ile kompozisyon deÄŸerlendir"""
        prompt = f"""AÅŸaÄŸÄ±daki kompozisyonu deÄŸerlendir.

**Konu:** {exam.title}
**Kompozisyon:**
{essay_text}

**DeÄŸerlendirme Kriterleri:**
1. content (0-30): Ä°Ã§erik ve tema uygunluÄŸu
2. organization (0-25): YapÄ± ve organizasyon
3. language (0-25): Dil kullanÄ±mÄ± ve gramer
4. creativity (0-20): YaratÄ±cÄ±lÄ±k ve Ã¶zgÃ¼nlÃ¼k

YanÄ±tÄ±nÄ± ÅŸu JSON formatÄ±nda ver:
{{
    "content": {{"score": X, "feedback": "..."}},
    "organization": {{"score": X, "feedback": "..."}},
    "language": {{"score": X, "feedback": "..."}},
    "creativity": {{"score": X, "feedback": "..."}},
    "overall_feedback": "Genel deÄŸerlendirme...",
    "suggestions": ["Ã–neri 1", "Ã–neri 2"]
}}"""
        
        try:
            response = await self.llm_service.generate(prompt, json_mode=True)
            return json.loads(response)
        except:
            return self._mock_evaluation(essay_text)
    
    def _mock_evaluation(self, essay_text: str) -> Dict[str, Any]:
        """Mock deÄŸerlendirme"""
        word_count = len(essay_text.split())
        paragraph_count = len([p for p in essay_text.split('\n\n') if p.strip()])
        
        base_score = min(70, word_count / 10)
        
        return {
            "content": {"score": min(30, base_score * 0.3), "feedback": "Ä°Ã§erik deÄŸerlendirildi."},
            "organization": {"score": min(25, paragraph_count * 5), "feedback": f"{paragraph_count} paragraf."},
            "language": {"score": min(25, base_score * 0.25), "feedback": "Dil kullanÄ±mÄ± uygun."},
            "creativity": {"score": min(20, base_score * 0.2), "feedback": "YaratÄ±cÄ±lÄ±k deÄŸerlendirildi."},
            "overall_feedback": f"Kompozisyonun {word_count} kelime iÃ§eriyor.",
            "suggestions": ["Daha fazla Ã¶rnek ekle", "ParagraflarÄ± geliÅŸtir"]
        }


# ==================== CODE CHALLENGE EVALUATOR ====================

class CodeChallengeEvaluator:
    """
    Kod Yazma SÄ±nav DeÄŸerlendiricisi
    
    Syntax kontrolÃ¼ + test case Ã§alÄ±ÅŸtÄ±rma + Ã§Ä±ktÄ± karÅŸÄ±laÅŸtÄ±rma.
    """
    
    def __init__(self, llm_service=None):
        self.llm_service = llm_service
    
    async def evaluate(
        self,
        exam: Exam,
        code_submissions: Dict[str, str],  # question_id -> code
        user_id: str,
        attempt_number: int = 1,
        time_taken_seconds: int = 0
    ) -> ExamResult:
        """Kod sÄ±navÄ±nÄ± deÄŸerlendir"""
        
        total_points = 0
        earned_points = 0
        criteria_scores = []
        
        for question in exam.questions:
            total_points += question.points
            code = code_submissions.get(question.id, "")
            
            # Syntax kontrolÃ¼
            syntax_ok, syntax_error = self._check_syntax(code)
            
            if not syntax_ok:
                criteria_scores.append(CriteriaScore(
                    criteria_name=f"code_{question.id[:8]}",
                    score=0,
                    max_score=question.points,
                    feedback=f"Syntax hatasÄ±: {syntax_error}"
                ))
                continue
            
            # Test case Ã§alÄ±ÅŸtÄ±rma (gÃ¼venli sandbox gerekir - burada mock)
            if self.llm_service:
                score, feedback = await self._evaluate_with_llm(question, code)
            else:
                score, feedback = self._mock_evaluate(question, code)
            
            earned_points += score
            criteria_scores.append(CriteriaScore(
                criteria_name=f"code_{question.id[:8]}",
                score=round(score, 1),
                max_score=question.points,
                feedback=feedback
            ))
        
        percentage = (earned_points / total_points * 100) if total_points > 0 else 0
        
        return ExamResult(
            exam_id=exam.id,
            user_id=user_id,
            exam_type=ExamType.CODE_CHALLENGE,
            total_score=round(earned_points, 1),
            max_possible_score=total_points,
            percentage=round(percentage, 1),
            passed=percentage >= exam.passing_score,
            attempt_number=attempt_number,
            criteria_scores=criteria_scores,
            detailed_feedback=f"Toplam {round(earned_points, 1)}/{total_points} puan.",
            time_taken_seconds=time_taken_seconds
        )
    
    def _check_syntax(self, code: str) -> Tuple[bool, str]:
        """Python syntax kontrolÃ¼"""
        try:
            import ast
            ast.parse(code)
            return True, ""
        except SyntaxError as e:
            return False, str(e)
    
    async def _evaluate_with_llm(self, question: ExamQuestion, code: str) -> Tuple[float, str]:
        """LLM ile kod deÄŸerlendir"""
        prompt = f"""AÅŸaÄŸÄ±daki Python kodunu deÄŸerlendir.

**Problem:** {question.question}
**Beklenen Ã‡Ã¶zÃ¼m Konsepti:** {question.correct_answer if question.correct_answer else "Genel Ã§Ã¶zÃ¼m"}
**Ã–ÄŸrenci Kodu:**
```python
{code}
```
**Maksimum Puan:** {question.points}

DeÄŸerlendirme kriterleri:
1. Kod doÄŸru Ã§alÄ±ÅŸÄ±yor mu?
2. Algoritma doÄŸru mu?
3. Kod kalitesi (okunabilirlik, verimlilik)

YanÄ±tÄ±nÄ± ÅŸu formatta ver:
{{"score": X, "feedback": "DeÄŸerlendirme..."}}"""
        
        try:
            response = await self.llm_service.generate(prompt, json_mode=True)
            data = json.loads(response)
            return min(data.get("score", 0), question.points), data.get("feedback", "")
        except:
            return self._mock_evaluate(question, code)
    
    def _mock_evaluate(self, question: ExamQuestion, code: str) -> Tuple[float, str]:
        """Mock kod deÄŸerlendirmesi"""
        lines = len([l for l in code.split('\n') if l.strip()])
        
        if lines < 3:
            return question.points * 0.2, "Kod Ã§ok kÄ±sa. Daha fazla detay gerekli."
        elif lines < 10:
            return question.points * 0.6, "Kod mantÄ±klÄ± gÃ¶rÃ¼nÃ¼yor. Ä°yileÅŸtirilebilir."
        else:
            return question.points * 0.85, "KapsamlÄ± bir Ã§Ã¶zÃ¼m. Harika!"


# ==================== SELF ASSESSMENT EVALUATOR ====================

class SelfAssessmentEvaluator:
    """
    Ã–z DeÄŸerlendirme SÄ±nav DeÄŸerlendiricisi
    
    KullanÄ±cÄ± kendini puanlar + gÃ¼ven dÃ¼zeyi analizi.
    """
    
    def evaluate(
        self,
        exam: Exam,
        self_ratings: Dict[str, Dict[str, Any]],  # question_id -> {rating, confidence, reflection}
        user_id: str,
        attempt_number: int = 1
    ) -> ExamResult:
        """Ã–z deÄŸerlendirmeyi analiz et"""
        
        total_questions = len(exam.questions)
        total_rating = 0
        total_confidence = 0
        criteria_scores = []
        reflections = []
        
        for question in exam.questions:
            rating_data = self_ratings.get(question.id, {})
            rating = rating_data.get("rating", 0)  # 1-5 arasÄ±
            confidence = rating_data.get("confidence", 50)  # 0-100
            reflection = rating_data.get("reflection", "")
            
            total_rating += rating
            total_confidence += confidence
            
            if reflection:
                reflections.append(reflection)
            
            # Rating'i puana Ã§evir (1-5 -> 0-20)
            score = (rating / 5) * 20
            
            criteria_scores.append(CriteriaScore(
                criteria_name=f"self_{question.id[:8]}",
                score=score,
                max_score=20,
                feedback=f"Ã–zgÃ¼ven: %{confidence}"
            ))
        
        avg_rating = total_rating / max(total_questions, 1)
        avg_confidence = total_confidence / max(total_questions, 1)
        
        # Genel puan hesaplama
        total_score = (avg_rating / 5) * 100
        max_score = 100
        percentage = total_score
        
        # GÃ¼ven analizi
        if avg_confidence > 80 and avg_rating < 3:
            confidence_insight = "DÃ¼ÅŸÃ¼k performansta yÃ¼ksek gÃ¼ven - konularÄ± gÃ¶zden geÃ§ir."
        elif avg_confidence < 40 and avg_rating > 3:
            confidence_insight = "Kendine daha Ã§ok gÃ¼venebilirsin!"
        else:
            confidence_insight = "Ã–z farkÄ±ndalÄ±ÄŸÄ±n dengeli gÃ¶rÃ¼nÃ¼yor."
        
        return ExamResult(
            exam_id=exam.id,
            user_id=user_id,
            exam_type=ExamType.SELF_ASSESSMENT,
            total_score=round(total_score, 1),
            max_possible_score=max_score,
            percentage=round(percentage, 1),
            passed=True,  # Ã–z deÄŸerlendirmede herkes geÃ§er
            attempt_number=attempt_number,
            criteria_scores=criteria_scores,
            detailed_feedback=f"Ortalama deÄŸerlendirme: {avg_rating:.1f}/5, Ortalama gÃ¼ven: %{avg_confidence:.0f}. {confidence_insight}",
            suggestions=[
                "ZayÄ±f olduÄŸunu dÃ¼ÅŸÃ¼ndÃ¼ÄŸÃ¼n konularÄ± tekrar et",
                "GÃ¼ven dÃ¼zeyini artÄ±rmak iÃ§in daha fazla pratik yap"
            ] + reflections[:3]
        )


# ==================== EXAM SYSTEM ORCHESTRATOR ====================


class ExamSystem:
    """
    SÄ±nav Sistemi OrkestratÃ¶rÃ¼
    
    TÃ¼m sÄ±nav tÃ¼rlerini yÃ¶netir.
    """
    
    def __init__(self, llm_service=None):
        self.llm_service = llm_service
        self.feynman_evaluator = FeynmanExamEvaluator(llm_service)
        self.mc_evaluator = MultipleChoiceEvaluator()
        self.problem_evaluator = ProblemSolvingEvaluator(llm_service)
        self.teach_back_evaluator = TeachBackEvaluator(llm_service)
        self.concept_map_evaluator = ConceptMapEvaluator(llm_service)
        # Yeni evaluator'lar
        self.true_false_evaluator = TrueFalseEvaluator()
        self.fill_blank_evaluator = FillBlankEvaluator(llm_service)
        self.short_answer_evaluator = ShortAnswerEvaluator(llm_service)
        self.essay_evaluator = EssayEvaluator(llm_service)
        self.code_challenge_evaluator = CodeChallengeEvaluator(llm_service)
        self.self_assessment_evaluator = SelfAssessmentEvaluator()
    
    async def evaluate_exam(
        self,
        exam: Exam,
        submission: Dict[str, Any],
        user_id: str,
        attempt_number: int = 1
    ) -> ExamResult:
        """SÄ±navÄ± tÃ¼rÃ¼ne gÃ¶re deÄŸerlendir"""
        
        if exam.type == ExamType.FEYNMAN:
            return await self.feynman_evaluator.evaluate(
                exam=exam,
                user_explanation=submission.get("explanation", ""),
                user_id=user_id,
                attempt_number=attempt_number,
                audio_transcript=submission.get("audio_transcript")
            )
        
        elif exam.type == ExamType.MULTIPLE_CHOICE:
            return self.mc_evaluator.evaluate(
                exam=exam,
                answers=submission.get("answers", {}),
                user_id=user_id,
                attempt_number=attempt_number,
                time_taken_seconds=submission.get("time_taken_seconds", 0)
            )
        
        elif exam.type == ExamType.PROBLEM_SOLVING:
            return await self.problem_evaluator.evaluate(
                exam=exam,
                solutions=submission.get("solutions", {}),
                user_id=user_id,
                attempt_number=attempt_number,
                time_taken_seconds=submission.get("time_taken_seconds", 0)
            )
        
        elif exam.type == ExamType.TEACH_BACK:
            return await self.teach_back_evaluator.evaluate(
                exam=exam,
                teaching_content=submission.get("teaching_content", ""),
                user_id=user_id,
                attempt_number=attempt_number,
                qa_history=submission.get("qa_history")
            )
        
        elif exam.type == ExamType.CONCEPT_MAP:
            return await self.concept_map_evaluator.evaluate(
                exam=exam,
                concept_map=submission.get("concept_map", {}),
                user_id=user_id,
                attempt_number=attempt_number
            )
        
        elif exam.type == ExamType.TRUE_FALSE:
            return self.true_false_evaluator.evaluate(
                exam=exam,
                answers=submission.get("answers", {}),
                user_id=user_id,
                attempt_number=attempt_number,
                time_taken_seconds=submission.get("time_taken_seconds", 0)
            )
        
        elif exam.type == ExamType.FILL_BLANK:
            return self.fill_blank_evaluator.evaluate(
                exam=exam,
                answers=submission.get("answers", {}),
                user_id=user_id,
                attempt_number=attempt_number,
                time_taken_seconds=submission.get("time_taken_seconds", 0)
            )
        
        elif exam.type == ExamType.SHORT_ANSWER:
            return await self.short_answer_evaluator.evaluate(
                exam=exam,
                answers=submission.get("answers", {}),
                user_id=user_id,
                attempt_number=attempt_number,
                time_taken_seconds=submission.get("time_taken_seconds", 0)
            )
        
        elif exam.type == ExamType.ESSAY:
            return await self.essay_evaluator.evaluate(
                exam=exam,
                essay_text=submission.get("essay_text", ""),
                user_id=user_id,
                attempt_number=attempt_number,
                time_taken_seconds=submission.get("time_taken_seconds", 0)
            )
        
        elif exam.type == ExamType.CODE_CHALLENGE:
            return await self.code_challenge_evaluator.evaluate(
                exam=exam,
                code_submissions=submission.get("code_submissions", {}),
                user_id=user_id,
                attempt_number=attempt_number,
                time_taken_seconds=submission.get("time_taken_seconds", 0)
            )
        
        elif exam.type == ExamType.SELF_ASSESSMENT:
            return self.self_assessment_evaluator.evaluate(
                exam=exam,
                self_ratings=submission.get("self_ratings", {}),
                user_id=user_id,
                attempt_number=attempt_number
            )
        
        else:
            # Kalan tÃ¼rler iÃ§in basit deÄŸerlendirme (ORAL_PRESENTATION, PEER_REVIEW, PRACTICAL, SIMULATION_BASED)
            return ExamResult(
                exam_id=exam.id,
                user_id=user_id,
                exam_type=exam.type,
                total_score=0,
                max_possible_score=100,
                percentage=0,
                passed=False,
                attempt_number=attempt_number,
                detailed_feedback="Bu sÄ±nav tÃ¼rÃ¼ henÃ¼z desteklenmiyor. (ORAL_PRESENTATION, PEER_REVIEW, PRACTICAL, SIMULATION_BASED)"
            )
    
    async def generate_questions(
        self,
        topic: str,
        subtopics: List[str],
        exam_type: ExamType,
        count: int = 10,
        difficulty: DifficultyLevel = DifficultyLevel.INTERMEDIATE
    ) -> List[ExamQuestion]:
        """AI ile soru Ã¼ret"""
        
        if not self.llm_service:
            return self._generate_mock_questions(topic, exam_type, count)
        
        # LLM ile soru Ã¼retimi
        prompt = f"""AÅŸaÄŸÄ±daki konu iÃ§in {count} adet {exam_type.value} tÃ¼rÃ¼nde soru oluÅŸtur.

**Konu:** {topic}
**Alt Konular:** {', '.join(subtopics)}
**Zorluk:** {difficulty.value}

Her soru iÃ§in ÅŸu formatÄ± kullan:
{{
    "questions": [
        {{
            "question": "Soru metni",
            "options": ["A", "B", "C", "D"] (Ã§oktan seÃ§meli iÃ§in),
            "correct_answer": "A",
            "explanation": "AÃ§Ä±klama",
            "points": 10,
            "topic": "Alt konu"
        }}
    ]
}}"""

        try:
            response = await self.llm_service.generate(prompt, json_mode=True)
            data = json.loads(response)
            
            questions = []
            for q_data in data.get("questions", []):
                questions.append(ExamQuestion(
                    type=exam_type,
                    question=q_data.get("question", ""),
                    options=q_data.get("options"),
                    correct_answer=q_data.get("correct_answer"),
                    explanation=q_data.get("explanation"),
                    points=q_data.get("points", 10),
                    topic=q_data.get("topic", topic)
                ))
            return questions
        except:
            return self._generate_mock_questions(topic, exam_type, count)
    
    def _generate_mock_questions(
        self, 
        topic: str, 
        exam_type: ExamType, 
        count: int
    ) -> List[ExamQuestion]:
        """Mock soru Ã¼retimi"""
        
        questions = []
        for i in range(count):
            if exam_type == ExamType.MULTIPLE_CHOICE:
                q = ExamQuestion(
                    type=exam_type,
                    question=f"{topic} ile ilgili Ã¶rnek soru {i+1}?",
                    options=["A) SeÃ§enek 1", "B) SeÃ§enek 2", "C) SeÃ§enek 3", "D) SeÃ§enek 4"],
                    correct_answer="A",
                    explanation="Bu sorunun aÃ§Ä±klamasÄ±...",
                    points=10,
                    topic=topic
                )
            else:
                q = ExamQuestion(
                    type=exam_type,
                    question=f"{topic} konusunda problem {i+1}",
                    points=20,
                    topic=topic
                )
            questions.append(q)
        return questions


# ==================== SINGLETON ====================

_exam_system: Optional[ExamSystem] = None

def get_exam_system() -> ExamSystem:
    global _exam_system
    if _exam_system is None:
        _exam_system = ExamSystem()
    return _exam_system
