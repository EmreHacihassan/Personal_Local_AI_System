"""
Upload Cleanup & Reindex Script
- Duplicate dosyalarÄ± temizle (en yenisini tut)
- Eksik dosyalarÄ± yeniden index'le
"""

import sys
sys.path.insert(0, '.')

from pathlib import Path
from core.vector_store import vector_store
from core.config import settings
from rag.document_loader import document_loader
from rag.chunker import document_chunker
import os


def print_section(title: str):
    print(f"\n{'='*60}")
    print(f"  {title}")
    print(f"{'='*60}")


def cleanup_duplicate_uploads():
    """Duplicate dosyalarÄ± temizle - en yenisini tut"""
    print_section("DUPLICATE DOSYALARI TEMÄ°ZLE")
    
    upload_dir = settings.DATA_DIR / "uploads"
    if not upload_dir.exists():
        print("Upload klasÃ¶rÃ¼ yok")
        return
    
    # DosyalarÄ± orijinal isme gÃ¶re grupla
    files_by_name = {}
    for f in upload_dir.iterdir():
        if f.is_file():
            parts = f.name.split("_", 1)
            if len(parts) > 1:
                original_name = parts[1]
                if original_name not in files_by_name:
                    files_by_name[original_name] = []
                files_by_name[original_name].append(f)
    
    # Her dosya iÃ§in sadece en yenisini tut
    deleted_count = 0
    for original_name, files in files_by_name.items():
        if len(files) > 1:
            # En yeni dosyayÄ± bul
            files.sort(key=lambda x: x.stat().st_mtime, reverse=True)
            newest = files[0]
            
            print(f"\nğŸ“„ {original_name}: {len(files)} kopya var")
            print(f"   En yeni tutuldu: {newest.name}")
            
            # Eskileri sil
            for old_file in files[1:]:
                print(f"   âŒ Silindi: {old_file.name}")
                old_file.unlink()
                deleted_count += 1
    
    print(f"\nâœ… Toplam {deleted_count} duplicate dosya silindi")
    return files_by_name


def get_indexed_files():
    """Vector store'da hangi dosyalar var?"""
    all_data = vector_store.collection.get(include=['metadatas'])
    
    indexed = set()
    for meta in all_data['metadatas']:
        if meta:
            filename = meta.get('original_filename') or meta.get('filename', '')
            # UUID prefix'i kaldÄ±r
            if '_' in filename and len(filename.split('_')[0]) == 36:
                filename = filename.split('_', 1)[1]
            if filename:
                indexed.add(filename)
    
    return indexed


def reindex_missing_files():
    """Eksik dosyalarÄ± yeniden index'le"""
    print_section("EKSÄ°K DOSYALARI YENDEN INDEX'LE")
    
    upload_dir = settings.DATA_DIR / "uploads"
    if not upload_dir.exists():
        print("Upload klasÃ¶rÃ¼ yok")
        return
    
    # Mevcut indexed dosyalarÄ± al
    indexed_files = get_indexed_files()
    print(f"\nğŸ“Š Vector store'da {len(indexed_files)} dosya var")
    
    # Upload klasÃ¶rÃ¼ndeki dosyalarÄ± kontrol et
    missing_files = []
    for f in upload_dir.iterdir():
        if f.is_file():
            parts = f.name.split("_", 1)
            if len(parts) > 1:
                original_name = parts[1]
                if original_name not in indexed_files:
                    missing_files.append(f)
    
    print(f"ğŸ” Index'lenmemiÅŸ dosya sayÄ±sÄ±: {len(missing_files)}")
    
    if not missing_files:
        print("âœ… TÃ¼m dosyalar zaten index'lenmiÅŸ")
        return
    
    # Eksik dosyalarÄ± index'le
    for file_path in missing_files:
        parts = file_path.name.split("_", 1)
        document_id = parts[0]
        original_name = parts[1] if len(parts) > 1 else file_path.name
        
        print(f"\nğŸ“„ Index'leniyor: {original_name}")
        
        try:
            # DÃ¶kÃ¼manÄ± yÃ¼kle
            documents = document_loader.load_file(str(file_path))
            
            if not documents:
                print(f"   âš ï¸ BoÅŸ dÃ¶kÃ¼man, atlanÄ±yor")
                continue
            
            # Chunk'la
            chunks = document_chunker.chunk_documents(documents)
            
            if not chunks:
                from rag.chunker import Chunk
                chunks = [Chunk(content=doc.content, metadata=doc.metadata) for doc in documents]
            
            # Vector store'a ekle
            chunk_texts = [c.content for c in chunks]
            chunk_metadatas = [
                {**c.metadata, "document_id": document_id, "original_filename": original_name}
                for c in chunks
            ]
            
            vector_store.add_documents(
                documents=chunk_texts,
                metadatas=chunk_metadatas,
            )
            
            print(f"   âœ… {len(chunks)} chunk eklendi")
            
        except Exception as e:
            print(f"   âŒ Hata: {e}")
    
    # SonuÃ§
    new_count = vector_store.count()
    print(f"\nâœ… Yeniden index'leme tamamlandÄ±")
    print(f"   Yeni toplam chunk: {new_count}")


def show_final_status():
    """Son durumu gÃ¶ster"""
    print_section("SON DURUM")
    
    upload_dir = settings.DATA_DIR / "uploads"
    
    # Upload klasÃ¶rÃ¼
    upload_files = list(upload_dir.iterdir()) if upload_dir.exists() else []
    print(f"\nğŸ“ Upload klasÃ¶rÃ¼nde {len(upload_files)} dosya")
    
    # Vector store
    chunk_count = vector_store.count()
    indexed_files = get_indexed_files()
    print(f"ğŸ“Š Vector store'da {chunk_count} chunk, {len(indexed_files)} benzersiz kaynak")
    
    print("\nğŸ“„ Index'lenmiÅŸ dosyalar:")
    for filename in sorted(indexed_files):
        print(f"   â€¢ {filename}")


if __name__ == "__main__":
    print("\n" + "="*60)
    print("    UPLOAD CLEANUP & REINDEX TOOL")
    print("="*60)
    
    cleanup_duplicate_uploads()
    reindex_missing_files()
    show_final_status()
    
    print("\n" + "="*60)
    print("    Ä°ÅLEM TAMAMLANDI")
    print("="*60 + "\n")
