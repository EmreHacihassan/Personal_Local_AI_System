"""
DeepScholar v2.0 - Fraktal AraÅŸtÄ±rma ve Sentez Mimarisi
=======================================================

Kurumsal seviye akademik dÃ¶kÃ¼man oluÅŸturucu.

Ã–zellikler:
- Fraktal GeniÅŸleme (Fractal Expansion) - Dinamik derinlik yÃ¶netimi
- Global/Local State yÃ¶netimi - TutarlÄ±lÄ±k iÃ§in hafÄ±za mimarisi
- Information Gain algoritmasÄ± - AkÄ±llÄ± araÅŸtÄ±rma derinliÄŸi
- Cross-Pollination sentez motoru - Kaynak sentezleme
- Self-Correction dÃ¶ngÃ¼sÃ¼ - HalÃ¼sinasyon kontrolÃ¼
- User Proxy simÃ¼latÃ¶rÃ¼ - Ä°Ã§ kalite kontrolÃ¼
- Paralel araÅŸtÄ±rma + SÄ±ralÄ± yazÄ±m
- Akademik kaynakÃ§a (APA/IEEE/Chicago)
- PDF export
- Ã‡oklu dil desteÄŸi
- CanlÄ± ilerleme takibi (WebSocket)

Maksimum: 60 sayfa
"""

import asyncio
import json
import time
import hashlib
import re
from typing import Optional, List, Dict, Any, AsyncGenerator, Callable, Tuple
from dataclasses import dataclass, field
from datetime import datetime
from enum import Enum
from concurrent.futures import ThreadPoolExecutor
import traceback

from core.config import settings
from core.llm_manager import llm_manager


# ============================================================================
# ENUMS & CONSTANTS
# ============================================================================

class DocumentLanguage(str, Enum):
    """Desteklenen dÃ¶kÃ¼man dilleri."""
    TURKISH = "tr"
    ENGLISH = "en"
    GERMAN = "de"


class CitationStyle(str, Enum):
    """KaynakÃ§a stilleri."""
    APA = "apa"
    IEEE = "ieee"
    CHICAGO = "chicago"
    HARVARD = "harvard"


class ResearchDepth(str, Enum):
    """AraÅŸtÄ±rma derinliÄŸi."""
    SHALLOW = "shallow"      # 1-5 sayfa: YÃ¼zeysel Ã¶zet
    MODERATE = "moderate"    # 6-15 sayfa: Orta detay
    DEEP = "deep"           # 16-30 sayfa: Derin araÅŸtÄ±rma
    EXHAUSTIVE = "exhaustive"  # 31-60 sayfa: KapsamlÄ± analiz


class AgentRole(str, Enum):
    """Ajan rolleri."""
    ORCHESTRATOR = "orchestrator"
    PLANNER = "planner"
    RESEARCHER = "researcher"
    WRITER = "writer"
    FACT_CHECKER = "fact_checker"
    USER_PROXY = "user_proxy"
    SYNTHESIZER = "synthesizer"


class EventType(str, Enum):
    """WebSocket event tipleri."""
    PHASE_START = "phase_start"
    PHASE_END = "phase_end"
    AGENT_MESSAGE = "agent_message"
    RESEARCH_FOUND = "research_found"
    SECTION_START = "section_start"
    SECTION_COMPLETE = "section_complete"
    CONFLICT_DETECTED = "conflict_detected"
    FACT_CHECK = "fact_check"
    USER_PROXY_FEEDBACK = "user_proxy_feedback"
    PROGRESS = "progress"
    ERROR = "error"
    COMPLETE = "complete"


# ============================================================================
# DATA CLASSES
# ============================================================================

@dataclass
class Citation:
    """Akademik atÄ±f."""
    id: str
    source_type: str  # web, pdf, article, book
    title: str
    authors: List[str] = field(default_factory=list)
    year: Optional[int] = None
    url: Optional[str] = None
    doi: Optional[str] = None
    journal: Optional[str] = None
    publisher: Optional[str] = None
    pages: Optional[str] = None
    access_date: Optional[str] = None
    
    # DÃ¶kÃ¼man iÃ§i kullanÄ±m
    used_in_pages: List[int] = field(default_factory=list)
    used_in_sections: List[str] = field(default_factory=list)
    inline_citations: List[Dict] = field(default_factory=list)  # {page, line, text}
    
    def to_apa(self) -> str:
        """APA formatÄ±nda kaynakÃ§a."""
        authors_str = ", ".join(self.authors) if self.authors else "Unknown"
        year_str = f"({self.year})" if self.year else "(n.d.)"
        
        if self.url:
            return f"{authors_str} {year_str}. {self.title}. Retrieved from {self.url}"
        elif self.journal:
            return f"{authors_str} {year_str}. {self.title}. {self.journal}, {self.pages or ''}."
        else:
            return f"{authors_str} {year_str}. {self.title}. {self.publisher or ''}."
    
    def to_ieee(self) -> str:
        """IEEE formatÄ±nda kaynakÃ§a."""
        authors_str = ", ".join(self.authors) if self.authors else "Unknown"
        
        if self.journal:
            return f'{authors_str}, "{self.title}," {self.journal}, {self.year or "n.d."}.'
        elif self.url:
            return f'{authors_str}, "{self.title}," [Online]. Available: {self.url}'
        else:
            return f'{authors_str}, "{self.title}," {self.publisher or ""}, {self.year or "n.d."}.'
    
    def to_chicago(self) -> str:
        """Chicago formatÄ±nda kaynakÃ§a."""
        authors_str = ", ".join(self.authors) if self.authors else "Unknown"
        
        if self.journal:
            return f'{authors_str}. "{self.title}." {self.journal} ({self.year or "n.d."}): {self.pages or ""}.'
        else:
            return f'{authors_str}. {self.title}. {self.publisher or ""}, {self.year or "n.d."}.'


@dataclass
class ResearchItem:
    """AraÅŸtÄ±rma sonucu."""
    id: str
    content: str
    source_title: str
    source_url: Optional[str] = None
    source_type: str = "web"  # web, pdf, academic, local
    relevance_score: float = 0.0
    reliability_score: float = 0.5
    authors: List[str] = field(default_factory=list)
    year: Optional[int] = None
    abstract: Optional[str] = None
    keywords: List[str] = field(default_factory=list)
    claims: List[str] = field(default_factory=list)  # Ã‡Ä±karÄ±lan iddialar
    evidence: List[str] = field(default_factory=list)  # KanÄ±tlar


@dataclass
class SectionOutline:
    """BÃ¶lÃ¼m taslaÄŸÄ±."""
    id: str
    title: str
    level: int  # 1 = ana bÃ¶lÃ¼m, 2 = alt bÃ¶lÃ¼m, 3 = alt-alt
    page_start: int
    page_end: int
    word_target: int
    subtopics: List[str] = field(default_factory=list)
    key_points: List[str] = field(default_factory=list)
    research_queries: List[str] = field(default_factory=list)
    dependencies: List[str] = field(default_factory=list)  # BaÄŸÄ±mlÄ± olduÄŸu bÃ¶lÃ¼mler
    parent_id: Optional[str] = None


@dataclass
class GlobalState:
    """
    Global State (Uzun SÃ¼reli HafÄ±za)
    TÃ¼m ajanlar tarafÄ±ndan okunabilir, sadece Orchestrator tarafÄ±ndan deÄŸiÅŸtirilebilir.
    """
    # Master plan
    master_outline: List[SectionOutline] = field(default_factory=list)
    thesis_statement: str = ""
    
    # TutarlÄ±lÄ±k iÃ§in
    global_glossary: Dict[str, str] = field(default_factory=dict)  # Terim -> AÃ§Ä±klama
    style_guide: Dict[str, str] = field(default_factory=dict)
    
    # Kaynaklar
    all_citations: Dict[str, Citation] = field(default_factory=dict)  # id -> Citation
    citation_counter: int = 0
    
    # Ãœst bilgi
    document_title: str = ""
    document_topic: str = ""
    target_pages: int = 10
    language: DocumentLanguage = DocumentLanguage.TURKISH
    citation_style: CitationStyle = CitationStyle.APA
    
    # YazÄ±m tamamlanan bÃ¶lÃ¼mler
    completed_sections: Dict[str, str] = field(default_factory=dict)  # section_id -> content
    section_summaries: Dict[str, str] = field(default_factory=dict)  # section_id -> summary


@dataclass
class LocalState:
    """
    Local State (KÄ±sa SÃ¼reli HafÄ±za)
    Sadece aktif bÃ¶lÃ¼mÃ¼ ilgilendirir, bÃ¶lÃ¼m bitince temizlenir.
    """
    current_section: Optional[SectionOutline] = None
    current_sources: List[ResearchItem] = field(default_factory=list)
    previous_section_summary: str = ""
    current_content: str = ""
    current_word_count: int = 0
    
    # Sentez iÃ§in
    extracted_claims: List[Dict] = field(default_factory=list)
    detected_conflicts: List[Dict] = field(default_factory=list)
    
    def clear(self):
        """BÃ¶lÃ¼m tamamlandÄ±ÄŸÄ±nda temizle."""
        self.current_section = None
        self.current_sources = []
        self.current_content = ""
        self.current_word_count = 0
        self.extracted_claims = []
        self.detected_conflicts = []


@dataclass
class AgentMessage:
    """Ajanlar arasÄ± mesaj."""
    from_agent: AgentRole
    to_agent: AgentRole
    message_type: str
    content: Dict[str, Any]
    timestamp: str = field(default_factory=lambda: datetime.now().isoformat())


@dataclass
class DeepScholarConfig:
    """DeepScholar yapÄ±landÄ±rmasÄ±."""
    title: str
    topic: str
    page_count: int = 10
    language: DocumentLanguage = DocumentLanguage.TURKISH
    citation_style: CitationStyle = CitationStyle.APA
    style: str = "academic"  # academic, casual, detailed, summary, exam_prep
    
    # AraÅŸtÄ±rma ayarlarÄ±
    web_search: str = "auto"  # off, auto, on
    academic_search: bool = True
    max_sources_per_section: int = 10
    
    # Kalite ayarlarÄ±
    enable_fact_checking: bool = True
    enable_user_proxy: bool = True
    enable_conflict_detection: bool = True
    
    # KullanÄ±cÄ± talimatlarÄ±
    custom_instructions: str = ""
    user_persona: str = ""  # User Proxy iÃ§in persona
    
    # GeliÅŸmiÅŸ
    parallel_research: bool = True
    max_research_depth: int = 3  # Fraktal derinlik


# ============================================================================
# ACADEMIC SEARCH ENGINE
# ============================================================================

class AcademicSearchEngine:
    """
    Ãœcretsiz akademik arama motoru.
    
    Desteklenen kaynaklar:
    - Semantic Scholar API (Ã¼cretsiz)
    - arXiv API (Ã¼cretsiz)
    - CrossRef API (Ã¼cretsiz)
    - CORE API (Ã¼cretsiz)
    - OpenAlex API (Ã¼cretsiz)
    """
    
    def __init__(self):
        self.semantic_scholar_base = "https://api.semanticscholar.org/graph/v1"
        self.arxiv_base = "http://export.arxiv.org/api/query"
        self.crossref_base = "https://api.crossref.org/works"
        self.openalex_base = "https://api.openalex.org"
        
    async def search_semantic_scholar(
        self, 
        query: str, 
        limit: int = 10
    ) -> List[ResearchItem]:
        """Semantic Scholar'da arama."""
        import aiohttp
        
        results = []
        try:
            async with aiohttp.ClientSession() as session:
                url = f"{self.semantic_scholar_base}/paper/search"
                params = {
                    "query": query,
                    "limit": limit,
                    "fields": "title,abstract,authors,year,url,citationCount,journal"
                }
                
                async with session.get(url, params=params, timeout=30) as response:
                    if response.status == 200:
                        data = await response.json()
                        for paper in data.get("data", []):
                            authors = [a.get("name", "") for a in paper.get("authors", [])]
                            results.append(ResearchItem(
                                id=f"ss_{paper.get('paperId', '')}",
                                content=paper.get("abstract", "") or "",
                                source_title=paper.get("title", ""),
                                source_url=paper.get("url"),
                                source_type="academic",
                                authors=authors,
                                year=paper.get("year"),
                                abstract=paper.get("abstract"),
                                relevance_score=min(paper.get("citationCount", 0) / 100, 1.0),
                                reliability_score=0.9  # Akademik kaynak
                            ))
        except Exception as e:
            print(f"[Semantic Scholar Error] {e}")
        
        return results
    
    async def search_arxiv(
        self, 
        query: str, 
        limit: int = 10
    ) -> List[ResearchItem]:
        """arXiv'de arama."""
        import aiohttp
        import xml.etree.ElementTree as ET
        
        results = []
        try:
            async with aiohttp.ClientSession() as session:
                params = {
                    "search_query": f"all:{query}",
                    "start": 0,
                    "max_results": limit,
                    "sortBy": "relevance"
                }
                
                async with session.get(self.arxiv_base, params=params, timeout=30) as response:
                    if response.status == 200:
                        text = await response.text()
                        root = ET.fromstring(text)
                        
                        ns = {"atom": "http://www.w3.org/2005/Atom"}
                        
                        for entry in root.findall("atom:entry", ns):
                            title = entry.find("atom:title", ns)
                            summary = entry.find("atom:summary", ns)
                            published = entry.find("atom:published", ns)
                            link = entry.find("atom:id", ns)
                            
                            authors = []
                            for author in entry.findall("atom:author", ns):
                                name = author.find("atom:name", ns)
                                if name is not None:
                                    authors.append(name.text)
                            
                            year = None
                            if published is not None and published.text:
                                year = int(published.text[:4])
                            
                            results.append(ResearchItem(
                                id=f"arxiv_{hashlib.md5((link.text or '').encode()).hexdigest()[:8]}",
                                content=summary.text if summary is not None else "",
                                source_title=title.text if title is not None else "",
                                source_url=link.text if link is not None else None,
                                source_type="academic",
                                authors=authors,
                                year=year,
                                abstract=summary.text if summary is not None else None,
                                relevance_score=0.85,
                                reliability_score=0.9
                            ))
        except Exception as e:
            print(f"[arXiv Error] {e}")
        
        return results
    
    async def search_crossref(
        self, 
        query: str, 
        limit: int = 10
    ) -> List[ResearchItem]:
        """CrossRef'te arama."""
        import aiohttp
        
        results = []
        try:
            async with aiohttp.ClientSession() as session:
                params = {
                    "query": query,
                    "rows": limit,
                    "select": "title,abstract,author,published-print,URL,DOI,container-title"
                }
                
                async with session.get(self.crossref_base, params=params, timeout=30) as response:
                    if response.status == 200:
                        data = await response.json()
                        for item in data.get("message", {}).get("items", []):
                            authors = []
                            for author in item.get("author", []):
                                name = f"{author.get('given', '')} {author.get('family', '')}".strip()
                                if name:
                                    authors.append(name)
                            
                            year = None
                            published = item.get("published-print", {}).get("date-parts", [[]])
                            if published and published[0]:
                                year = published[0][0]
                            
                            title = item.get("title", [""])[0] if item.get("title") else ""
                            
                            results.append(ResearchItem(
                                id=f"cr_{item.get('DOI', hashlib.md5(title.encode()).hexdigest()[:8])}",
                                content=item.get("abstract", "") or "",
                                source_title=title,
                                source_url=item.get("URL"),
                                source_type="academic",
                                authors=authors,
                                year=year,
                                abstract=item.get("abstract"),
                                relevance_score=0.8,
                                reliability_score=0.85
                            ))
        except Exception as e:
            print(f"[CrossRef Error] {e}")
        
        return results
    
    async def search_all(
        self, 
        query: str, 
        limit_per_source: int = 5
    ) -> List[ResearchItem]:
        """TÃ¼m kaynaklarda paralel arama."""
        tasks = [
            self.search_semantic_scholar(query, limit_per_source),
            self.search_arxiv(query, limit_per_source),
            self.search_crossref(query, limit_per_source),
        ]
        
        results_lists = await asyncio.gather(*tasks, return_exceptions=True)
        
        all_results = []
        for results in results_lists:
            if isinstance(results, list):
                all_results.extend(results)
        
        # SÄ±rala ve deduplicate
        seen_titles = set()
        unique_results = []
        for r in sorted(all_results, key=lambda x: x.relevance_score, reverse=True):
            title_lower = r.source_title.lower()
            if title_lower not in seen_titles:
                seen_titles.add(title_lower)
                unique_results.append(r)
        
        return unique_results


# ============================================================================
# WEB SEARCH ENGINE
# ============================================================================

class WebSearchEngine:
    """Web arama motoru wrapper."""
    
    async def search(
        self, 
        query: str, 
        num_results: int = 5
    ) -> List[ResearchItem]:
        """Web'de arama."""
        results = []
        
        try:
            from tools.web_search_engine import PremiumWebSearchEngine
            
            engine = PremiumWebSearchEngine()
            web_result = engine.search(query, num_results=num_results)
            
            if hasattr(web_result, 'results'):
                for wr in web_result.results:
                    content = getattr(wr, 'full_content', None) or getattr(wr, 'snippet', '') or ''
                    
                    results.append(ResearchItem(
                        id=f"web_{hashlib.md5(getattr(wr, 'url', '').encode()).hexdigest()[:8]}",
                        content=content[:2000],
                        source_title=getattr(wr, 'title', '') or 'Web Page',
                        source_url=getattr(wr, 'url', None),
                        source_type="web",
                        relevance_score=getattr(wr, 'relevance_score', 0.7),
                        reliability_score=getattr(wr, 'reliability_score', 0.5)
                    ))
        except Exception as e:
            print(f"[Web Search Error] {e}")
        
        return results


# ============================================================================
# DEEP SCHOLAR AGENTS
# ============================================================================

class BaseAgent:
    """Temel ajan sÄ±nÄ±fÄ±."""
    
    def __init__(self, role: AgentRole, global_state: GlobalState):
        self.role = role
        self.global_state = global_state
        self._executor = ThreadPoolExecutor(max_workers=2)
    
    async def _llm_generate(self, prompt: str, temperature: float = 0.7) -> str:
        """LLM Ã§aÄŸrÄ±sÄ± (async wrapper)."""
        loop = asyncio.get_event_loop()
        return await loop.run_in_executor(
            self._executor,
            lambda: llm_manager.generate(prompt, temperature=temperature)
        )
    
    def _parse_json(self, text: str) -> Any:
        """JSON parse with fallback."""
        try:
            # JSON bloÄŸunu bul
            json_match = re.search(r'```json\s*([\s\S]*?)\s*```', text)
            if json_match:
                return json.loads(json_match.group(1))
            
            # Direkt JSON dene
            json_match = re.search(r'[\[\{][\s\S]*[\]\}]', text)
            if json_match:
                return json.loads(json_match.group(0))
            
            return json.loads(text)
        except:
            return None


class PlannerAgent(BaseAgent):
    """
    Planlama AjanÄ±
    
    GÃ¶revler:
    - DÃ¶kÃ¼man iskeletini oluÅŸtur
    - Sayfa daÄŸÄ±lÄ±mÄ±nÄ± hesapla
    - AraÅŸtÄ±rma sorgularÄ±nÄ± belirle
    - Fraktal geniÅŸleme kararlarÄ±
    """
    
    def __init__(self, global_state: GlobalState):
        super().__init__(AgentRole.PLANNER, global_state)
    
    async def create_master_outline(
        self, 
        config: DeepScholarConfig
    ) -> List[SectionOutline]:
        """Ana iÃ§erik planÄ±nÄ± oluÅŸtur."""
        
        # Derinlik hesapla
        if config.page_count <= 5:
            depth = ResearchDepth.SHALLOW
            section_count = 4  # GiriÅŸ, 2 bÃ¶lÃ¼m, SonuÃ§
        elif config.page_count <= 15:
            depth = ResearchDepth.MODERATE
            section_count = 6
        elif config.page_count <= 30:
            depth = ResearchDepth.DEEP
            section_count = 10
        else:
            depth = ResearchDepth.EXHAUSTIVE
            section_count = 15
        
        words_per_page = 400
        total_words = config.page_count * words_per_page
        
        lang_prompts = {
            DocumentLanguage.TURKISH: "TÃ¼rkÃ§e",
            DocumentLanguage.ENGLISH: "English",
            DocumentLanguage.GERMAN: "Deutsch"
        }
        
        prompt = f"""Sen akademik bir dÃ¶kÃ¼man planlayÄ±cÄ±sÄ±sÄ±n. AÅŸaÄŸÄ±daki konu iÃ§in {config.page_count} sayfalÄ±k kapsamlÄ± bir dÃ¶kÃ¼man planÄ± oluÅŸtur.

KONU: {config.topic}
BAÅLIK: {config.title}
DÄ°L: {lang_prompts.get(config.language, 'TÃ¼rkÃ§e')}
ARAÅTIRMA DERÄ°NLÄ°ÄÄ°: {depth.value}
TOPLAM KELÄ°ME HEDEFÄ°: ~{total_words} kelime
TAHMÄ°NÄ° BÃ–LÃœM SAYISI: {section_count}
YAZI STÄ°LÄ°: {config.style}

{f'KULLANICI TALÄ°MATLARI: {config.custom_instructions}' if config.custom_instructions else ''}

Her bÃ¶lÃ¼m iÃ§in ÅŸunlarÄ± planla:
1. BÃ¶lÃ¼m baÅŸlÄ±ÄŸÄ± ve seviyesi (1=ana, 2=alt, 3=alt-alt)
2. Sayfa aralÄ±ÄŸÄ±
3. Kelime hedefi
4. Alt konular
5. AraÅŸtÄ±rÄ±lmasÄ± gereken anahtar noktalar
6. Web/Akademik arama sorgularÄ±
7. BaÄŸÄ±mlÄ± olduÄŸu Ã¶nceki bÃ¶lÃ¼mler

JSON formatÄ±nda dÃ¶ndÃ¼r:
```json
[
  {{
    "id": "sec_1",
    "title": "GiriÅŸ",
    "level": 1,
    "page_start": 1,
    "page_end": 2,
    "word_target": 800,
    "subtopics": ["Konunun Ã¶nemi", "AraÅŸtÄ±rma sorusu"],
    "key_points": ["Ana argÃ¼man", "Kapsam"],
    "research_queries": ["konu genel bakÄ±ÅŸ", "konu tarihÃ§esi"],
    "dependencies": []
  }},
  ...
]
```

Ã–nemli kurallar:
- GiriÅŸ ve SonuÃ§ bÃ¶lÃ¼mleri mutlaka olmalÄ±
- Sayfa daÄŸÄ±lÄ±mÄ± dengeli olmalÄ±
- Toplam {config.page_count} sayfayÄ± geÃ§memeli
- Her bÃ¶lÃ¼m mantÄ±ksal bir akÄ±ÅŸ izlemeli
- AraÅŸtÄ±rma sorgularÄ± spesifik ve aranabilir olmalÄ±"""

        response = await self._llm_generate(prompt)
        
        outline_data = self._parse_json(response)
        
        if not outline_data:
            return self._default_outline(config)
        
        sections = []
        for i, sec in enumerate(outline_data):
            sections.append(SectionOutline(
                id=sec.get("id", f"sec_{i+1}"),
                title=sec.get("title", f"BÃ¶lÃ¼m {i+1}"),
                level=sec.get("level", 1),
                page_start=sec.get("page_start", i+1),
                page_end=sec.get("page_end", i+2),
                word_target=sec.get("word_target", 400),
                subtopics=sec.get("subtopics", []),
                key_points=sec.get("key_points", []),
                research_queries=sec.get("research_queries", [config.topic]),
                dependencies=sec.get("dependencies", []),
                parent_id=sec.get("parent_id")
            ))
        
        return sections
    
    def _default_outline(self, config: DeepScholarConfig) -> List[SectionOutline]:
        """VarsayÄ±lan plan."""
        words_per_page = 400
        total_words = config.page_count * words_per_page
        
        return [
            SectionOutline(
                id="sec_1",
                title="GiriÅŸ" if config.language == DocumentLanguage.TURKISH else "Introduction",
                level=1,
                page_start=1,
                page_end=max(1, config.page_count // 6),
                word_target=total_words // 6,
                subtopics=[config.topic],
                key_points=["Konunun Ã¶nemi", "AraÅŸtÄ±rma sorusu"],
                research_queries=[f"{config.topic} genel bakÄ±ÅŸ", f"{config.topic} tarihÃ§e"]
            ),
            SectionOutline(
                id="sec_2",
                title="Ana Ä°Ã§erik" if config.language == DocumentLanguage.TURKISH else "Main Content",
                level=1,
                page_start=max(1, config.page_count // 6) + 1,
                page_end=config.page_count - max(1, config.page_count // 6),
                word_target=int(total_words * 0.7),
                subtopics=[config.topic],
                key_points=["Temel kavramlar", "DetaylÄ± analiz"],
                research_queries=[config.topic, f"{config.topic} detaylar"]
            ),
            SectionOutline(
                id="sec_3",
                title="SonuÃ§" if config.language == DocumentLanguage.TURKISH else "Conclusion",
                level=1,
                page_start=config.page_count - max(1, config.page_count // 6) + 1,
                page_end=config.page_count,
                word_target=total_words // 6,
                subtopics=["Ã–zet"],
                key_points=["Ana bulgular", "Gelecek Ã§alÄ±ÅŸmalar"],
                research_queries=[]
            )
        ]
    
    async def expand_section(
        self, 
        section: SectionOutline, 
        current_research: List[ResearchItem],
        target_depth: int
    ) -> List[SectionOutline]:
        """
        Fraktal GeniÅŸleme - BÃ¶lÃ¼mÃ¼ alt bÃ¶lÃ¼mlere ayÄ±r.
        
        Information Gain algoritmasÄ±: EÄŸer toplanan veri yeterince zenginse,
        bÃ¶lÃ¼mÃ¼ daha kÃ¼Ã§Ã¼k parÃ§alara bÃ¶l.
        """
        if target_depth <= 0:
            return [section]
        
        # Bilgi kazancÄ±nÄ± hesapla
        info_density = len(current_research) * sum(r.relevance_score for r in current_research)
        
        # Yeterli bilgi yoksa geniÅŸleme yapma
        if info_density < 2.0:
            return [section]
        
        prompt = f"""Bu bÃ¶lÃ¼mÃ¼ alt bÃ¶lÃ¼mlere ayÄ±r:

BÃ–LÃœM: {section.title}
ALT KONULAR: {', '.join(section.subtopics)}
KELÄ°ME HEDEFÄ°: {section.word_target}

MEVCUT ARAÅTIRMA KONULARI:
{chr(10).join([f"- {r.source_title}: {r.content[:200]}..." for r in current_research[:5]])}

Bu bÃ¶lÃ¼mÃ¼ 2-4 alt bÃ¶lÃ¼me ayÄ±r. JSON formatÄ±nda dÃ¶ndÃ¼r:
```json
[
  {{"title": "Alt BÃ¶lÃ¼m 1", "key_points": ["..."], "word_target": 200}},
  ...
]
```"""

        response = await self._llm_generate(prompt)
        subsections_data = self._parse_json(response)
        
        if not subsections_data or len(subsections_data) < 2:
            return [section]
        
        # Alt bÃ¶lÃ¼mleri oluÅŸtur
        subsections = []
        word_per_sub = section.word_target // len(subsections_data)
        page_per_sub = max(1, (section.page_end - section.page_start + 1) // len(subsections_data))
        
        for i, sub in enumerate(subsections_data):
            subsections.append(SectionOutline(
                id=f"{section.id}_{i+1}",
                title=sub.get("title", f"Alt BÃ¶lÃ¼m {i+1}"),
                level=section.level + 1,
                page_start=section.page_start + i * page_per_sub,
                page_end=section.page_start + (i + 1) * page_per_sub - 1,
                word_target=word_per_sub,
                subtopics=sub.get("subtopics", []),
                key_points=sub.get("key_points", []),
                research_queries=sub.get("research_queries", [sub.get("title", "")]),
                parent_id=section.id
            ))
        
        return subsections


class ResearcherAgent(BaseAgent):
    """
    AraÅŸtÄ±rmacÄ± Ajan
    
    GÃ¶revler:
    - Web aramasÄ±
    - Akademik kaynak aramasÄ±
    - RAG ile yerel kaynak aramasÄ±
    - Claim extraction (Ä°ddia Ã§Ä±karma)
    - Conflict detection (Ã‡eliÅŸki tespiti)
    """
    
    def __init__(self, global_state: GlobalState):
        super().__init__(AgentRole.RESEARCHER, global_state)
        self.academic_engine = AcademicSearchEngine()
        self.web_engine = WebSearchEngine()
    
    async def research_section(
        self,
        section: SectionOutline,
        config: DeepScholarConfig,
        local_state: LocalState
    ) -> List[ResearchItem]:
        """BÃ¶lÃ¼m iÃ§in araÅŸtÄ±rma yap."""
        all_results = []
        
        # AraÅŸtÄ±rma sorgularÄ±
        queries = section.research_queries or [section.title]
        queries.extend(section.key_points[:3])
        queries = list(set(queries))[:8]  # Max 8 sorgu
        
        tasks = []
        
        # Web aramasÄ±
        if config.web_search in ["on", "auto"]:
            for query in queries[:3]:
                tasks.append(self.web_engine.search(query, 3))
        
        # Akademik arama
        if config.academic_search:
            for query in queries[:3]:
                tasks.append(self.academic_engine.search_all(query, 3))
        
        # Paralel arama
        if config.parallel_research:
            results_lists = await asyncio.gather(*tasks, return_exceptions=True)
            for results in results_lists:
                if isinstance(results, list):
                    all_results.extend(results)
        else:
            for task in tasks:
                try:
                    results = await task
                    all_results.extend(results)
                except:
                    pass
        
        # RAG aramasÄ±
        try:
            from core.vector_store import vector_store
            for query in queries[:5]:
                rag_results = vector_store.search_with_scores(
                    query=query,
                    n_results=5,
                    score_threshold=0.3
                )
                for r in rag_results:
                    all_results.append(ResearchItem(
                        id=f"rag_{hashlib.md5(r.get('document', '')[:100].encode()).hexdigest()[:8]}",
                        content=r.get("document", ""),
                        source_title=r.get("metadata", {}).get("original_filename", "Local Document"),
                        source_type="local",
                        relevance_score=r.get("score", 0.5),
                        reliability_score=0.8
                    ))
        except Exception as e:
            print(f"[RAG Search Error] {e}")
        
        # Deduplicate ve sÄ±rala
        seen = set()
        unique_results = []
        for r in sorted(all_results, key=lambda x: x.relevance_score * x.reliability_score, reverse=True):
            content_hash = hashlib.md5(r.content[:200].encode()).hexdigest()
            if content_hash not in seen:
                seen.add(content_hash)
                unique_results.append(r)
        
        return unique_results[:config.max_sources_per_section]
    
    async def extract_claims(
        self, 
        research_items: List[ResearchItem]
    ) -> List[Dict]:
        """
        Kaynaklardan iddialar ve kanÄ±tlar Ã§Ä±kar.
        Cross-Pollination iÃ§in temel.
        """
        if not research_items:
            return []
        
        sources_text = "\n\n".join([
            f"[KAYNAK {i+1}] ({r.source_title}):\n{r.content[:500]}"
            for i, r in enumerate(research_items[:8])
        ])
        
        prompt = f"""Bu kaynaklardan ana iddialarÄ± (claims) ve kanÄ±tlarÄ± (evidence) Ã§Ä±kar:

{sources_text}

Her iddia iÃ§in:
1. Ä°ddia metni
2. Hangi kaynaktan geldiÄŸi
3. Destekleyen kanÄ±t
4. GÃ¼ven skoru (0-1)

JSON formatÄ±nda dÃ¶ndÃ¼r:
```json
[
  {{
    "claim": "Ä°ddia metni",
    "source_index": 1,
    "evidence": "Destekleyen kanÄ±t",
    "confidence": 0.85
  }}
]
```"""

        response = await self._llm_generate(prompt)
        claims = self._parse_json(response)
        
        return claims if claims else []
    
    async def detect_conflicts(
        self, 
        claims: List[Dict]
    ) -> List[Dict]:
        """
        Ã‡eliÅŸen iddialarÄ± tespit et.
        Cross-Pollination sentez iÃ§in kritik.
        """
        if len(claims) < 2:
            return []
        
        claims_text = "\n".join([
            f"{i+1}. {c.get('claim', '')} (Kaynak {c.get('source_index', '?')})"
            for i, c in enumerate(claims)
        ])
        
        prompt = f"""Bu iddialarÄ± analiz et ve Ã§eliÅŸenleri bul:

{claims_text}

Ã‡eliÅŸen iddia Ã§iftlerini bul ve aÃ§Ä±kla. JSON formatÄ±nda dÃ¶ndÃ¼r:
```json
[
  {{
    "claim_1_index": 1,
    "claim_2_index": 3,
    "conflict_type": "factual|methodological|interpretive",
    "description": "Ã‡eliÅŸkinin aÃ§Ä±klamasÄ±",
    "possible_resolution": "Muhtemel aÃ§Ä±klama"
  }}
]
```

EÄŸer Ã§eliÅŸki yoksa boÅŸ liste dÃ¶ndÃ¼r: []"""

        response = await self._llm_generate(prompt)
        conflicts = self._parse_json(response)
        
        return conflicts if conflicts else []


class WriterAgent(BaseAgent):
    """
    Yazar Ajan
    
    GÃ¶revler:
    - BÃ¶lÃ¼m iÃ§eriÄŸi yazma
    - Kaynak entegrasyonu
    - Stil tutarlÄ±lÄ±ÄŸÄ±
    - GeÃ§iÅŸ paragraflarÄ±
    """
    
    def __init__(self, global_state: GlobalState):
        super().__init__(AgentRole.WRITER, global_state)
    
    async def write_section(
        self,
        section: SectionOutline,
        research: List[ResearchItem],
        local_state: LocalState,
        config: DeepScholarConfig
    ) -> Tuple[str, List[Citation]]:
        """BÃ¶lÃ¼m iÃ§eriÄŸi yaz."""
        
        lang_prompts = {
            DocumentLanguage.TURKISH: "TÃ¼rkÃ§e",
            DocumentLanguage.ENGLISH: "English",
            DocumentLanguage.GERMAN: "Deutsch"
        }
        
        style_prompts = {
            "academic": "akademik, formal ve detaylÄ±",
            "casual": "samimi, anlaÅŸÄ±lÄ±r ve akÄ±cÄ±",
            "detailed": "kapsamlÄ±, her detayÄ± aÃ§Ä±klayan",
            "summary": "Ã¶zet, ana noktalarÄ± vurgulayan",
            "exam_prep": "sÄ±nava yÃ¶nelik, Ã¶nemli noktalarÄ± vurgulayan"
        }
        
        # Kaynak metni oluÅŸtur
        sources_text = ""
        source_map = {}
        for i, r in enumerate(research[:10], 1):
            sources_text += f"\n[KAYNAK {i}] ({r.source_title}"
            if r.year:
                sources_text += f", {r.year}"
            sources_text += f"):\n{r.content[:600]}\n"
            source_map[i] = r
        
        # Ã–nceki bÃ¶lÃ¼m Ã¶zeti
        prev_summary = ""
        if local_state.previous_section_summary:
            prev_summary = f"\nÃ–NCEKÄ° BÃ–LÃœM Ã–ZETÄ°:\n{local_state.previous_section_summary}\n"
        
        # Tez statement
        thesis = ""
        if self.global_state.thesis_statement:
            thesis = f"\nANA TEZ: {self.global_state.thesis_statement}\n"
        
        # Terimler sÃ¶zlÃ¼ÄŸÃ¼
        glossary = ""
        if self.global_state.global_glossary:
            glossary = "\nKULLANILACAK TERÄ°MLER:\n" + "\n".join([
                f"- {term}: {defn}" 
                for term, defn in list(self.global_state.global_glossary.items())[:10]
            ]) + "\n"
        
        prompt = f"""Bu bÃ¶lÃ¼m iÃ§in akademik iÃ§erik yaz.

DÃ–KÃœMAN: {self.global_state.document_title}
BÃ–LÃœM: {section.title}
BÃ–LÃœM SEVÄ°YESÄ°: {'#' * section.level} (Markdown baÅŸlÄ±k)
ALT KONULAR: {', '.join(section.subtopics)}
ANAHTAR NOKTALAR: {', '.join(section.key_points)}

DÄ°L: {lang_prompts.get(config.language, 'TÃ¼rkÃ§e')}
YAZI STÄ°LÄ°: {style_prompts.get(config.style, 'akademik')}
KELÄ°ME HEDEFÄ°: ~{section.word_target} kelime
{thesis}
{prev_summary}
{glossary}

KAYNAKLAR:
{sources_text}

YAZIM KURALLARI:
1. {'#' * section.level} ile baÅŸlÄ±k kullan
2. Kaynaklara metin iÃ§inde [KAYNAK X] ÅŸeklinde atÄ±f yap
3. Akademik ve tutarlÄ± bir dil kullan
4. Paragraflar arasÄ± geÃ§iÅŸler yumuÅŸak olsun
5. Ã–nemli kavramlarÄ± **kalÄ±n** yap
6. Gerekirse madde iÅŸaretleri kullan
7. Kelime hedefine yaklaÅŸ

{f'Ã–ZEL TALÄ°MATLAR: {config.custom_instructions}' if config.custom_instructions else ''}

Åimdi bu bÃ¶lÃ¼mÃ¼n iÃ§eriÄŸini yaz:"""

        content = await self._llm_generate(prompt)
        
        # Citation'larÄ± Ã§Ä±kar
        citations = []
        citation_pattern = r'\[KAYNAK (\d+)\]'
        
        for match in re.finditer(citation_pattern, content):
            src_idx = int(match.group(1))
            if src_idx in source_map:
                r = source_map[src_idx]
                
                # Citation oluÅŸtur
                citation = Citation(
                    id=f"cite_{self.global_state.citation_counter + 1}",
                    source_type=r.source_type,
                    title=r.source_title,
                    authors=r.authors,
                    year=r.year,
                    url=r.source_url,
                    used_in_sections=[section.id]
                )
                
                citations.append(citation)
                self.global_state.citation_counter += 1
                
                # Global state'e ekle
                if citation.id not in self.global_state.all_citations:
                    self.global_state.all_citations[citation.id] = citation
        
        # [KAYNAK X] -> [X] formatÄ±na Ã§evir
        clean_content = content
        for i in range(1, 20):
            clean_content = clean_content.replace(f"[KAYNAK {i}]", f"[{i}]")
        
        return clean_content, citations


class FactCheckerAgent(BaseAgent):
    """
    GerÃ§ek KontrolÃ¼ AjanÄ±
    
    GÃ¶revler:
    - HalÃ¼sinasyon tespiti
    - Referans doÄŸrulama
    - SayÄ±sal doÄŸruluk kontrolÃ¼
    """
    
    def __init__(self, global_state: GlobalState):
        super().__init__(AgentRole.FACT_CHECKER, global_state)
    
    async def verify_content(
        self,
        content: str,
        research: List[ResearchItem]
    ) -> Dict:
        """Ä°Ã§eriÄŸi doÄŸrula."""
        
        sources_text = "\n".join([
            f"[KAYNAK {i+1}]: {r.content[:300]}"
            for i, r in enumerate(research[:8])
        ])
        
        prompt = f"""Bu iÃ§eriÄŸi kaynaklarla karÅŸÄ±laÅŸtÄ±r ve doÄŸrula:

Ä°Ã‡ERÄ°K:
{content[:2000]}

KAYNAKLAR:
{sources_text}

Her faktÃ¼el iddia iÃ§in:
1. Ä°ddiayÄ± bul
2. Kaynakta doÄŸrulama var mÄ±?
3. GÃ¼ven skoru (0-1)

JSON formatÄ±nda dÃ¶ndÃ¼r:
```json
{{
  "verified_claims": [
    {{"claim": "...", "verified": true, "source_index": 1, "confidence": 0.9}}
  ],
  "unverified_claims": [
    {{"claim": "...", "issue": "Kaynaklarda bulunamadÄ±"}}
  ],
  "overall_score": 0.85,
  "recommendations": ["..."]
}}
```"""

        response = await self._llm_generate(prompt)
        result = self._parse_json(response)
        
        return result if result else {
            "verified_claims": [],
            "unverified_claims": [],
            "overall_score": 0.7,
            "recommendations": []
        }


class UserProxyAgent(BaseAgent):
    """
    KullanÄ±cÄ± SimÃ¼lasyonu AjanÄ±
    
    GÃ¶revler:
    - KullanÄ±cÄ± perspektifinden iÃ§erik deÄŸerlendirme
    - AnlaÅŸÄ±lÄ±rlÄ±k kontrolÃ¼
    - Stil uygunluÄŸu
    """
    
    def __init__(self, global_state: GlobalState, persona: str = ""):
        super().__init__(AgentRole.USER_PROXY, global_state)
        self.persona = persona
    
    async def review_content(
        self,
        content: str,
        config: DeepScholarConfig
    ) -> Dict:
        """Ä°Ã§eriÄŸi kullanÄ±cÄ± perspektifinden deÄŸerlendir."""
        
        persona_text = self.persona or config.user_persona or "genel okuyucu"
        
        prompt = f"""Sen bir okuyucu olarak bu iÃ§eriÄŸi deÄŸerlendir.

OKUYUCU PROFÄ°LÄ°: {persona_text}
DÄ°L: {config.language.value}

Ä°Ã‡ERÄ°K:
{content[:2000]}

DeÄŸerlendirme kriterleri:
1. AnlaÅŸÄ±lÄ±rlÄ±k (1-10)
2. Bilgi yoÄŸunluÄŸu (1-10)
3. AkÄ±ÅŸ ve tutarlÄ±lÄ±k (1-10)
4. Teknik terim kullanÄ±mÄ± uygun mu?
5. Eksik veya karmaÅŸÄ±k kÄ±sÄ±mlar var mÄ±?

JSON formatÄ±nda dÃ¶ndÃ¼r:
```json
{{
  "clarity_score": 8,
  "density_score": 7,
  "flow_score": 8,
  "technical_terms_ok": true,
  "issues": [
    {{"location": "2. paragraf", "issue": "Terim aÃ§Ä±klanmamÄ±ÅŸ", "suggestion": "..."}}
  ],
  "overall_feedback": "Genel deÄŸerlendirme..."
}}
```"""

        response = await self._llm_generate(prompt)
        result = self._parse_json(response)
        
        return result if result else {
            "clarity_score": 7,
            "density_score": 7,
            "flow_score": 7,
            "technical_terms_ok": True,
            "issues": [],
            "overall_feedback": "Ä°Ã§erik genel olarak iyi."
        }


class SynthesizerAgent(BaseAgent):
    """
    Sentez AjanÄ±
    
    GÃ¶revler:
    - KaynaklarÄ± sentezleme
    - Ã‡eliÅŸkileri Ã§Ã¶zme
    - BÃ¼tÃ¼nleÅŸtirici paragraflar
    """
    
    def __init__(self, global_state: GlobalState):
        super().__init__(AgentRole.SYNTHESIZER, global_state)
    
    async def synthesize_claims(
        self,
        claims: List[Dict],
        conflicts: List[Dict]
    ) -> str:
        """Ã‡eliÅŸen iddialarÄ± sentezle."""
        
        if not conflicts:
            return ""
        
        claims_text = "\n".join([
            f"{i+1}. {c.get('claim', '')} (GÃ¼ven: {c.get('confidence', 0.5)})"
            for i, c in enumerate(claims)
        ])
        
        conflicts_text = "\n".join([
            f"- Ã‡eliÅŸki: Ä°ddia {c['claim_1_index']} vs Ä°ddia {c['claim_2_index']}: {c.get('description', '')}"
            for c in conflicts
        ])
        
        prompt = f"""Bu Ã§eliÅŸkileri analiz et ve sentezle:

Ä°DDÄ°ALAR:
{claims_text}

Ã‡ELÄ°ÅKÄ°LER:
{conflicts_text}

Her Ã§eliÅŸki iÃ§in:
1. Neden farklÄ±lÄ±k var?
2. Hangi kaynak daha gÃ¼venilir?
3. Sentez cÃ¼mlesi oluÅŸtur

Akademik bir dille, Ã§eliÅŸkileri aÃ§Ä±klayan ve sentezleyen bir paragraf yaz."""

        response = await self._llm_generate(prompt)
        return response


# ============================================================================
# DEEP SCHOLAR ORCHESTRATOR
# ============================================================================

class DeepScholarOrchestrator:
    """
    DeepScholar Ana OrkestratÃ¶r
    
    TÃ¼m ajanlarÄ± koordine eder ve dÃ¶kÃ¼man Ã¼retim sÃ¼recini yÃ¶netir.
    """
    
    def __init__(self):
        self.global_state = GlobalState()
        self.local_state = LocalState()
        
        # Ajanlar
        self.planner: Optional[PlannerAgent] = None
        self.researcher: Optional[ResearcherAgent] = None
        self.writer: Optional[WriterAgent] = None
        self.fact_checker: Optional[FactCheckerAgent] = None
        self.user_proxy: Optional[UserProxyAgent] = None
        self.synthesizer: Optional[SynthesizerAgent] = None
        
        # Event callback
        self._event_callback: Optional[Callable] = None
    
    def set_event_callback(self, callback: Callable):
        """Event callback ayarla (WebSocket iÃ§in)."""
        self._event_callback = callback
    
    async def _emit_event(
        self, 
        event_type: EventType, 
        data: Dict[str, Any]
    ):
        """Event yayÄ±nla."""
        event = {
            "type": event_type.value,
            "timestamp": datetime.now().isoformat(),
            **data
        }
        
        if self._event_callback:
            await self._event_callback(event)
    
    async def generate_document(
        self,
        config: DeepScholarConfig
    ) -> AsyncGenerator[Dict, None]:
        """
        DÃ¶kÃ¼man Ã¼retimi (ana akÄ±ÅŸ).
        
        Yields:
            Progress events
        """
        try:
            # Global state'i baÅŸlat
            self.global_state = GlobalState(
                document_title=config.title,
                document_topic=config.topic,
                target_pages=config.page_count,
                language=config.language,
                citation_style=config.citation_style
            )
            
            # AjanlarÄ± oluÅŸtur
            self.planner = PlannerAgent(self.global_state)
            self.researcher = ResearcherAgent(self.global_state)
            self.writer = WriterAgent(self.global_state)
            self.fact_checker = FactCheckerAgent(self.global_state)
            self.user_proxy = UserProxyAgent(self.global_state, config.user_persona)
            self.synthesizer = SynthesizerAgent(self.global_state)
            
            # ============ PHASE 1: PLANNING ============
            yield {
                "type": EventType.PHASE_START.value,
                "phase": "planning",
                "message": "ğŸ“‹ DÃ¶kÃ¼man planlanÄ±yor...",
                "progress": 5
            }
            
            yield {
                "type": EventType.AGENT_MESSAGE.value,
                "agent": AgentRole.PLANNER.value,
                "message": f"ğŸ§  {config.page_count} sayfalÄ±k dÃ¶kÃ¼man iÃ§in master outline oluÅŸturuluyor..."
            }
            
            outline = await self.planner.create_master_outline(config)
            self.global_state.master_outline = outline
            
            yield {
                "type": EventType.PHASE_END.value,
                "phase": "planning",
                "message": f"âœ… {len(outline)} bÃ¶lÃ¼mlÃ¼k plan hazÄ±rlandÄ±",
                "progress": 10,
                "data": {
                    "outline": [
                        {"id": s.id, "title": s.title, "level": s.level, "pages": f"{s.page_start}-{s.page_end}"}
                        for s in outline
                    ]
                }
            }
            
            # ============ PHASE 2: RESEARCH ============
            yield {
                "type": EventType.PHASE_START.value,
                "phase": "research",
                "message": "ğŸ” AraÅŸtÄ±rma yapÄ±lÄ±yor...",
                "progress": 15
            }
            
            all_research: Dict[str, List[ResearchItem]] = {}
            total_sources = 0
            
            for i, section in enumerate(outline):
                yield {
                    "type": EventType.AGENT_MESSAGE.value,
                    "agent": AgentRole.RESEARCHER.value,
                    "message": f"ğŸ” BÃ¶lÃ¼m {i+1}/{len(outline)} araÅŸtÄ±rÄ±lÄ±yor: {section.title}"
                }
                
                self.local_state.current_section = section
                research = await self.researcher.research_section(section, config, self.local_state)
                all_research[section.id] = research
                total_sources += len(research)
                
                yield {
                    "type": EventType.RESEARCH_FOUND.value,
                    "section": section.title,
                    "sources_count": len(research),
                    "sources": [
                        {"title": r.source_title, "type": r.source_type, "score": r.relevance_score}
                        for r in research[:5]
                    ]
                }
                
                # Fraktal geniÅŸleme kontrolÃ¼
                if config.max_research_depth > 1 and len(research) > 5:
                    claims = await self.researcher.extract_claims(research)
                    self.local_state.extracted_claims = claims
                    
                    if config.enable_conflict_detection:
                        conflicts = await self.researcher.detect_conflicts(claims)
                        self.local_state.detected_conflicts = conflicts
                        
                        if conflicts:
                            yield {
                                "type": EventType.CONFLICT_DETECTED.value,
                                "section": section.title,
                                "conflicts": conflicts
                            }
            
            yield {
                "type": EventType.PHASE_END.value,
                "phase": "research",
                "message": f"ğŸ“š Toplam {total_sources} kaynak bulundu",
                "progress": 30
            }
            
            # ============ PHASE 3: WRITING ============
            yield {
                "type": EventType.PHASE_START.value,
                "phase": "writing",
                "message": "âœï¸ Ä°Ã§erik yazÄ±lÄ±yor...",
                "progress": 35
            }
            
            all_content = []
            all_citations = []
            
            for i, section in enumerate(outline):
                progress = 35 + int((i / len(outline)) * 45)
                
                yield {
                    "type": EventType.SECTION_START.value,
                    "section_index": i,
                    "section_title": section.title,
                    "progress": progress
                }
                
                yield {
                    "type": EventType.AGENT_MESSAGE.value,
                    "agent": AgentRole.WRITER.value,
                    "message": f"âœï¸ YazÄ±lÄ±yor: {section.title} ({section.word_target} kelime hedef)"
                }
                
                # Local state gÃ¼ncelle
                self.local_state.current_section = section
                self.local_state.current_sources = all_research.get(section.id, [])
                
                if i > 0:
                    prev_section = outline[i-1]
                    self.local_state.previous_section_summary = self.global_state.section_summaries.get(
                        prev_section.id, ""
                    )
                
                # YazÄ±m
                content, citations = await self.writer.write_section(
                    section, 
                    self.local_state.current_sources,
                    self.local_state,
                    config
                )
                
                all_content.append({
                    "section_id": section.id,
                    "title": section.title,
                    "level": section.level,
                    "content": content,
                    "word_count": len(content.split())
                })
                all_citations.extend(citations)
                
                # Global state gÃ¼ncelle
                self.global_state.completed_sections[section.id] = content
                
                # Ã–zet oluÅŸtur (sonraki bÃ¶lÃ¼m iÃ§in)
                summary_prompt = f"Bu iÃ§eriÄŸi 2-3 cÃ¼mleyle Ã¶zetle:\n{content[:1000]}"
                summary = await self.writer._llm_generate(summary_prompt, temperature=0.3)
                self.global_state.section_summaries[section.id] = summary[:300]
                
                # Fact check (opsiyonel)
                if config.enable_fact_checking:
                    yield {
                        "type": EventType.AGENT_MESSAGE.value,
                        "agent": AgentRole.FACT_CHECKER.value,
                        "message": f"ğŸ” DoÄŸrulama: {section.title}"
                    }
                    
                    verification = await self.fact_checker.verify_content(
                        content, 
                        self.local_state.current_sources
                    )
                    
                    yield {
                        "type": EventType.FACT_CHECK.value,
                        "section": section.title,
                        "score": verification.get("overall_score", 0.7),
                        "verified_count": len(verification.get("verified_claims", [])),
                        "unverified_count": len(verification.get("unverified_claims", []))
                    }
                
                # User proxy review (opsiyonel)
                if config.enable_user_proxy:
                    yield {
                        "type": EventType.AGENT_MESSAGE.value,
                        "agent": AgentRole.USER_PROXY.value,
                        "message": f"ğŸ‘¤ Okuyucu deÄŸerlendirmesi: {section.title}"
                    }
                    
                    review = await self.user_proxy.review_content(content, config)
                    
                    yield {
                        "type": EventType.USER_PROXY_FEEDBACK.value,
                        "section": section.title,
                        "clarity": review.get("clarity_score", 7),
                        "issues": review.get("issues", [])
                    }
                
                yield {
                    "type": EventType.SECTION_COMPLETE.value,
                    "section_index": i,
                    "section_title": section.title,
                    "word_count": len(content.split()),
                    "content_preview": content[:500] + "..." if len(content) > 500 else content,
                    "progress": progress + 5
                }
                
                # Local state temizle
                self.local_state.clear()
            
            yield {
                "type": EventType.PHASE_END.value,
                "phase": "writing",
                "message": "âœ… Ä°Ã§erik yazÄ±mÄ± tamamlandÄ±",
                "progress": 85
            }
            
            # ============ PHASE 4: BIBLIOGRAPHY ============
            yield {
                "type": EventType.PHASE_START.value,
                "phase": "bibliography",
                "message": "ğŸ“– KaynakÃ§a oluÅŸturuluyor...",
                "progress": 88
            }
            
            bibliography = self._create_bibliography(config)
            
            yield {
                "type": EventType.PHASE_END.value,
                "phase": "bibliography",
                "message": f"ğŸ“š {len(self.global_state.all_citations)} kaynak listelendi",
                "progress": 92
            }
            
            # ============ PHASE 5: FINALIZE ============
            yield {
                "type": EventType.PHASE_START.value,
                "phase": "finalize",
                "message": "ğŸ”§ DÃ¶kÃ¼man birleÅŸtiriliyor...",
                "progress": 94
            }
            
            # Final dÃ¶kÃ¼man
            final_content = self._combine_content(all_content, bibliography, config)
            
            yield {
                "type": EventType.COMPLETE.value,
                "message": "ğŸ‰ DÃ¶kÃ¼man baÅŸarÄ±yla oluÅŸturuldu!",
                "progress": 100,
                "document": {
                    "title": config.title,
                    "content": final_content,
                    "word_count": len(final_content.split()),
                    "page_count": config.page_count,
                    "citations_count": len(self.global_state.all_citations),
                    "sections": [
                        {"id": s.id, "title": s.title}
                        for s in outline
                    ]
                }
            }
            
        except Exception as e:
            yield {
                "type": EventType.ERROR.value,
                "message": f"âŒ Hata: {str(e)}",
                "error": str(e),
                "trace": traceback.format_exc()
            }
    
    def _create_bibliography(self, config: DeepScholarConfig) -> str:
        """Akademik kaynakÃ§a oluÅŸtur."""
        
        if not self.global_state.all_citations:
            return ""
        
        lang_headers = {
            DocumentLanguage.TURKISH: "KAYNAKÃ‡A",
            DocumentLanguage.ENGLISH: "REFERENCES",
            DocumentLanguage.GERMAN: "LITERATURVERZEICHNIS"
        }
        
        bibliography = f"\n\n---\n\n# ğŸ“š {lang_headers.get(config.language, 'KAYNAKÃ‡A')}\n\n"
        
        # Kaynak tipine gÃ¶re grupla
        web_sources = []
        academic_sources = []
        local_sources = []
        
        for citation in self.global_state.all_citations.values():
            if citation.source_type == "academic":
                academic_sources.append(citation)
            elif citation.source_type == "web":
                web_sources.append(citation)
            else:
                local_sources.append(citation)
        
        # Akademik kaynaklar
        if academic_sources:
            bibliography += "## Akademik Kaynaklar\n\n"
            for i, c in enumerate(academic_sources, 1):
                if config.citation_style == CitationStyle.APA:
                    bibliography += f"[{i}] {c.to_apa()}\n\n"
                elif config.citation_style == CitationStyle.IEEE:
                    bibliography += f"[{i}] {c.to_ieee()}\n\n"
                else:
                    bibliography += f"[{i}] {c.to_chicago()}\n\n"
        
        # Web kaynaklarÄ±
        if web_sources:
            bibliography += "## Web KaynaklarÄ±\n\n"
            for i, c in enumerate(web_sources, len(academic_sources) + 1):
                date = c.access_date or datetime.now().strftime("%Y-%m-%d")
                bibliography += f"[{i}] {c.title}. EriÅŸim: {c.url} ({date})\n\n"
        
        # Yerel kaynaklar
        if local_sources:
            bibliography += "## Yerel DÃ¶kÃ¼manlar\n\n"
            for i, c in enumerate(local_sources, len(academic_sources) + len(web_sources) + 1):
                bibliography += f"[{i}] {c.title}\n\n"
        
        return bibliography
    
    def _combine_content(
        self, 
        sections: List[Dict], 
        bibliography: str,
        config: DeepScholarConfig
    ) -> str:
        """TÃ¼m iÃ§eriÄŸi birleÅŸtir."""
        
        # BaÅŸlÄ±k sayfasÄ±
        lang_headers = {
            DocumentLanguage.TURKISH: {"title": "ARAÅTIRMA DÃ–KÃœMANI", "topic": "Konu", "date": "Tarih", "pages": "Sayfa"},
            DocumentLanguage.ENGLISH: {"title": "RESEARCH DOCUMENT", "topic": "Topic", "date": "Date", "pages": "Pages"},
            DocumentLanguage.GERMAN: {"title": "FORSCHUNGSDOKUMENT", "topic": "Thema", "date": "Datum", "pages": "Seiten"}
        }
        
        headers = lang_headers.get(config.language, lang_headers[DocumentLanguage.TURKISH])
        
        content = f"""# {config.title}

**{headers['topic']}:** {config.topic}
**{headers['date']}:** {datetime.now().strftime('%Y-%m-%d')}
**{headers['pages']}:** {config.page_count}

---

"""
        
        # Ä°Ã§indekiler
        content += "## ğŸ“‘ Ä°Ã‡Ä°NDEKÄ°LER\n\n"
        for section in sections:
            indent = "  " * (section.get("level", 1) - 1)
            content += f"{indent}- [{section['title']}](#{section['section_id']})\n"
        content += "\n---\n\n"
        
        # BÃ¶lÃ¼mler
        for section in sections:
            content += section['content'] + "\n\n---\n\n"
        
        # KaynakÃ§a
        content += bibliography
        
        return content


# ============================================================================
# PDF EXPORT
# ============================================================================

class PDFExporter:
    """PDF export iÅŸlemleri."""
    
    @staticmethod
    async def export_to_pdf(
        content: str,
        title: str,
        output_path: str
    ) -> bool:
        """Markdown iÃ§eriÄŸi PDF'e Ã§evir."""
        try:
            # WeasyPrint veya pdfkit kullan
            import markdown
            
            # Markdown -> HTML
            html_content = markdown.markdown(
                content,
                extensions=['tables', 'fenced_code', 'toc']
            )
            
            # HTML ÅŸablonu
            html_template = f"""
<!DOCTYPE html>
<html>
<head>
    <meta charset="UTF-8">
    <title>{title}</title>
    <style>
        body {{
            font-family: 'Georgia', serif;
            line-height: 1.6;
            max-width: 800px;
            margin: 0 auto;
            padding: 40px;
        }}
        h1 {{ color: #2c3e50; border-bottom: 2px solid #3498db; padding-bottom: 10px; }}
        h2 {{ color: #34495e; margin-top: 30px; }}
        h3 {{ color: #7f8c8d; }}
        code {{ background: #f4f4f4; padding: 2px 6px; border-radius: 3px; }}
        pre {{ background: #f4f4f4; padding: 15px; border-radius: 5px; overflow-x: auto; }}
        blockquote {{ border-left: 3px solid #3498db; margin-left: 0; padding-left: 20px; color: #555; }}
        table {{ border-collapse: collapse; width: 100%; }}
        th, td {{ border: 1px solid #ddd; padding: 8px; text-align: left; }}
        th {{ background: #f4f4f4; }}
    </style>
</head>
<body>
{html_content}
</body>
</html>
"""
            
            # PDF oluÅŸtur (pdfkit kullan)
            try:
                import pdfkit
                pdfkit.from_string(html_template, output_path)
                return True
            except ImportError:
                # Alternatif: HTML olarak kaydet
                html_path = output_path.replace('.pdf', '.html')
                with open(html_path, 'w', encoding='utf-8') as f:
                    f.write(html_template)
                print(f"PDF export iÃ§in pdfkit gerekli. HTML olarak kaydedildi: {html_path}")
                return False
                
        except Exception as e:
            print(f"[PDF Export Error] {e}")
            return False


# ============================================================================
# SINGLETON
# ============================================================================

deep_scholar = DeepScholarOrchestrator()
