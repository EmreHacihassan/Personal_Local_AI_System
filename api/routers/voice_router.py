"""
üé§üîäüñºÔ∏è Voice & Multimodal API Router
=====================================

Fully LOCAL voice and vision endpoints.
All processing happens on your machine - no data leaves your computer.

Features:
- Speech-to-Text (STT) via local Whisper
- Text-to-Speech (TTS) via Pyttsx3
- Vision/Image analysis via LLaVA (Ollama)
"""

import asyncio
import base64
import io
import logging
import tempfile
from pathlib import Path
from typing import Optional, List
from datetime import datetime

from fastapi import APIRouter, HTTPException, UploadFile, File, Form
from fastapi.responses import StreamingResponse, Response
from pydantic import BaseModel, Field

logger = logging.getLogger(__name__)

router = APIRouter(prefix="/api/voice", tags=["Voice & Multimodal"])


# ============ PYDANTIC MODELS ============

class TTSRequest(BaseModel):
    """Text-to-Speech isteƒüi."""
    text: str = Field(..., min_length=1, max_length=5000, description="Seslendirilecek metin")
    voice: Optional[str] = Field(default=None, description="Ses se√ßimi (opsiyonel)")
    rate: int = Field(default=150, ge=50, le=300, description="Konu≈üma hƒ±zƒ±")
    volume: float = Field(default=1.0, ge=0.0, le=1.0, description="Ses seviyesi")
    language: str = Field(default="tr", description="Dil kodu")


class TTSResponse(BaseModel):
    """Text-to-Speech yanƒ±tƒ±."""
    success: bool
    audio_base64: Optional[str] = None
    format: str = "wav"
    duration_ms: Optional[float] = None
    message: Optional[str] = None


class STTResponse(BaseModel):
    """Speech-to-Text yanƒ±tƒ±."""
    success: bool
    text: Optional[str] = None
    language: Optional[str] = None
    confidence: Optional[float] = None
    duration_ms: Optional[float] = None
    message: Optional[str] = None


class VisionRequest(BaseModel):
    """Vision analiz isteƒüi."""
    prompt: str = Field(default="Bu g√∂rseli detaylƒ± a√ßƒ±kla.", description="G√∂rsel i√ßin soru/prompt")
    image_base64: str = Field(..., description="Base64 encoded g√∂rsel")
    model: str = Field(default="llava", description="Vision model (llava, bakllava, etc.)")


class VisionResponse(BaseModel):
    """Vision analiz yanƒ±tƒ±."""
    success: bool
    description: Optional[str] = None
    objects_detected: List[str] = []
    confidence: Optional[float] = None
    model_used: Optional[str] = None
    message: Optional[str] = None


class MultimodalChatRequest(BaseModel):
    """Multimodal chat isteƒüi."""
    text: Optional[str] = Field(default=None, description="Metin mesajƒ±")
    audio_base64: Optional[str] = Field(default=None, description="Base64 encoded ses")
    image_base64: Optional[str] = Field(default=None, description="Base64 encoded g√∂rsel")
    generate_audio_response: bool = Field(default=False, description="Sesli yanƒ±t √ºret")


class MultimodalChatResponse(BaseModel):
    """Multimodal chat yanƒ±tƒ±."""
    success: bool
    text_response: Optional[str] = None
    audio_response_base64: Optional[str] = None
    transcription: Optional[str] = None
    image_description: Optional[str] = None
    message: Optional[str] = None


# ============ TTS ENDPOINT ============

@router.post("/tts", response_model=TTSResponse)
async def text_to_speech(request: TTSRequest):
    """
    üîä Text-to-Speech (Metin ‚Üí Ses)
    
    Metni sese d√∂n√º≈üt√ºr√ºr. Tamamen yerel i≈ülenir, veri dƒ±≈üarƒ± gitmez.
    
    - **text**: Seslendirilecek metin
    - **rate**: Konu≈üma hƒ±zƒ± (50-300)
    - **volume**: Ses seviyesi (0.0-1.0)
    - **language**: Dil kodu (tr, en, etc.)
    """
    try:
        from core.voice_multimodal import Pyttsx3TTS, AudioFormat
        
        # TTS engine olu≈ütur
        tts = Pyttsx3TTS(rate=request.rate, volume=request.volume)
        
        # Sesi sentezle
        result = await tts.synthesize(request.text, request.voice)
        
        # Base64'e √ßevir
        audio_base64 = base64.b64encode(result.audio_data).decode('utf-8')
        
        return TTSResponse(
            success=True,
            audio_base64=audio_base64,
            format=result.format.value,
            duration_ms=result.duration_ms
        )
        
    except ImportError as e:
        logger.error(f"TTS import error: {e}")
        return TTSResponse(
            success=False,
            message="pyttsx3 paketi y√ºkl√º deƒüil. pip install pyttsx3"
        )
    except Exception as e:
        logger.error(f"TTS error: {e}")
        return TTSResponse(
            success=False,
            message=str(e)
        )


@router.post("/tts/stream")
async def text_to_speech_stream(request: TTSRequest):
    """
    üîä Text-to-Speech Streaming
    
    Metni sese d√∂n√º≈üt√ºr√ºr ve streaming olarak d√∂nd√ºr√ºr.
    """
    try:
        from core.voice_multimodal import Pyttsx3TTS
        
        tts = Pyttsx3TTS(rate=request.rate, volume=request.volume)
        result = await tts.synthesize(request.text, request.voice)
        
        return Response(
            content=result.audio_data,
            media_type="audio/wav",
            headers={
                "Content-Disposition": f"attachment; filename=speech_{datetime.now().strftime('%Y%m%d_%H%M%S')}.wav"
            }
        )
        
    except Exception as e:
        logger.error(f"TTS stream error: {e}")
        raise HTTPException(status_code=500, detail=str(e))


# ============ STT ENDPOINT ============

@router.post("/stt", response_model=STTResponse)
async def speech_to_text(
    audio: UploadFile = File(..., description="Ses dosyasƒ± (wav, mp3, etc.)"),
    model_size: str = Form(default="base", description="Whisper model boyutu: tiny, base, small, medium, large")
):
    """
    üé§ Speech-to-Text (Ses ‚Üí Metin)
    
    Ses dosyasƒ±nƒ± metne d√∂n√º≈üt√ºr√ºr. Tamamen yerel i≈ülenir.
    
    - **audio**: Ses dosyasƒ± (wav, mp3, ogg, flac)
    - **model_size**: Whisper model boyutu (tiny, base, small, medium, large)
    """
    try:
        from core.voice_multimodal import WhisperLocalSTT, AudioSegment, AudioFormat
        
        # Dosya i√ßeriƒüini oku
        audio_data = await audio.read()
        
        # Format tespit
        filename = audio.filename or "audio.wav"
        ext = Path(filename).suffix.lower().strip(".")
        
        format_map = {
            "wav": AudioFormat.WAV,
            "mp3": AudioFormat.MP3,
            "ogg": AudioFormat.OGG,
            "flac": AudioFormat.FLAC,
            "webm": AudioFormat.WEBM,
        }
        audio_format = format_map.get(ext, AudioFormat.WAV)
        
        # Audio segment olu≈ütur
        segment = AudioSegment(
            data=audio_data,
            format=audio_format
        )
        
        # STT engine olu≈ütur ve transcribe et
        stt = WhisperLocalSTT(model_size=model_size)
        result = await stt.transcribe(segment)
        
        return STTResponse(
            success=True,
            text=result.text,
            language=result.language,
            confidence=result.confidence,
            duration_ms=result.duration_ms
        )
        
    except ImportError as e:
        logger.error(f"STT import error: {e}")
        return STTResponse(
            success=False,
            message="Whisper paketi y√ºkl√º deƒüil. pip install faster-whisper veya pip install openai-whisper"
        )
    except Exception as e:
        logger.error(f"STT error: {e}")
        return STTResponse(
            success=False,
            message=str(e)
        )


@router.post("/stt/base64", response_model=STTResponse)
async def speech_to_text_base64(
    audio_base64: str = Form(..., description="Base64 encoded ses"),
    format: str = Form(default="wav", description="Ses formatƒ±"),
    model_size: str = Form(default="base", description="Whisper model boyutu")
):
    """
    üé§ Speech-to-Text (Base64)
    
    Base64 encoded ses verisini metne d√∂n√º≈üt√ºr√ºr.
    """
    try:
        from core.voice_multimodal import WhisperLocalSTT, AudioSegment, AudioFormat
        
        # Base64'√º decode et
        audio_data = base64.b64decode(audio_base64)
        
        format_map = {
            "wav": AudioFormat.WAV,
            "mp3": AudioFormat.MP3,
            "ogg": AudioFormat.OGG,
            "flac": AudioFormat.FLAC,
            "webm": AudioFormat.WEBM,
        }
        audio_format = format_map.get(format.lower(), AudioFormat.WAV)
        
        segment = AudioSegment(data=audio_data, format=audio_format)
        
        stt = WhisperLocalSTT(model_size=model_size)
        result = await stt.transcribe(segment)
        
        return STTResponse(
            success=True,
            text=result.text,
            language=result.language,
            confidence=result.confidence,
            duration_ms=result.duration_ms
        )
        
    except Exception as e:
        logger.error(f"STT base64 error: {e}")
        return STTResponse(
            success=False,
            message=str(e)
        )


# ============ VISION ENDPOINT ============

@router.post("/vision", response_model=VisionResponse)
async def analyze_image(request: VisionRequest):
    """
    üñºÔ∏è Vision Analysis (G√∂rsel Analizi)
    
    G√∂rseli analiz eder ve a√ßƒ±klama √ºretir. LLaVA via Ollama kullanƒ±r.
    Tamamen yerel i≈ülenir, veri dƒ±≈üarƒ± gitmez.
    
    - **image_base64**: Base64 encoded g√∂rsel
    - **prompt**: G√∂rsel i√ßin soru (√∂rn: "Bu g√∂rselde ne var?")
    - **model**: Vision model (llava, bakllava)
    """
    try:
        from core.voice_multimodal import LLaVAVision, ImageInput, ImageFormat
        
        # Vision analyzer olu≈ütur
        vision = LLaVAVision(model=request.model)
        
        # Image input olu≈ütur
        image = ImageInput(base64_data=request.image_base64)
        
        # Analiz et
        result = await vision.analyze(image, request.prompt)
        
        return VisionResponse(
            success=True,
            description=result.description,
            objects_detected=result.objects_detected,
            confidence=result.confidence,
            model_used=request.model
        )
        
    except Exception as e:
        logger.error(f"Vision error: {e}")
        return VisionResponse(
            success=False,
            message=str(e)
        )


@router.post("/vision/upload", response_model=VisionResponse)
async def analyze_uploaded_image(
    image: UploadFile = File(..., description="G√∂rsel dosyasƒ±"),
    prompt: str = Form(default="Bu g√∂rseli detaylƒ± a√ßƒ±kla.", description="Soru/prompt"),
    model: str = Form(default="llava", description="Vision model")
):
    """
    üñºÔ∏è Vision Analysis (Dosya Upload)
    
    Y√ºklenen g√∂rseli analiz eder.
    """
    try:
        from core.voice_multimodal import LLaVAVision, ImageInput, ImageFormat
        
        # Dosyayƒ± oku ve base64'e √ßevir
        image_data = await image.read()
        image_base64 = base64.b64encode(image_data).decode('utf-8')
        
        # Vision analyzer
        vision = LLaVAVision(model=model)
        image_input = ImageInput(base64_data=image_base64)
        
        result = await vision.analyze(image_input, prompt)
        
        return VisionResponse(
            success=True,
            description=result.description,
            objects_detected=result.objects_detected,
            confidence=result.confidence,
            model_used=model
        )
        
    except Exception as e:
        logger.error(f"Vision upload error: {e}")
        return VisionResponse(
            success=False,
            message=str(e)
        )


# ============ MULTIMODAL CHAT ============

@router.post("/multimodal-chat", response_model=MultimodalChatResponse)
async def multimodal_chat(request: MultimodalChatRequest):
    """
    üé§üñºÔ∏èüí¨ Multimodal Chat
    
    Ses, g√∂rsel ve metin kombinasyonu ile sohbet.
    T√ºm i≈ülemler yerel yapƒ±lƒ±r.
    
    - **text**: Metin mesajƒ±
    - **audio_base64**: Base64 encoded ses (opsiyonel)
    - **image_base64**: Base64 encoded g√∂rsel (opsiyonel)
    - **generate_audio_response**: Sesli yanƒ±t √ºret
    """
    try:
        from core.voice_multimodal import (
            create_multimodal_pipeline,
            MultimodalInput,
            AudioSegment,
            AudioFormat,
            ImageInput
        )
        from core.llm_manager import llm_manager
        
        # Pipeline olu≈ütur
        pipeline = create_multimodal_pipeline(llm_client=llm_manager)
        
        # Input hazƒ±rla
        audio_segment = None
        if request.audio_base64:
            audio_data = base64.b64decode(request.audio_base64)
            audio_segment = AudioSegment(data=audio_data, format=AudioFormat.WAV)
        
        images = []
        if request.image_base64:
            images.append(ImageInput(base64_data=request.image_base64))
        
        multimodal_input = MultimodalInput(
            text=request.text,
            audio=audio_segment,
            images=images
        )
        
        # ƒ∞≈üle
        result = await pipeline.process(
            multimodal_input,
            generate_audio_response=request.generate_audio_response
        )
        
        # Yanƒ±t hazƒ±rla
        audio_response_base64 = None
        if result.audio_response:
            audio_response_base64 = base64.b64encode(result.audio_response.audio_data).decode('utf-8')
        
        transcription = None
        if result.transcription:
            transcription = result.transcription.text
        
        image_description = None
        if result.image_descriptions:
            image_description = result.image_descriptions[0].description
        
        return MultimodalChatResponse(
            success=True,
            text_response=result.text_response,
            audio_response_base64=audio_response_base64,
            transcription=transcription,
            image_description=image_description
        )
        
    except Exception as e:
        logger.error(f"Multimodal chat error: {e}")
        return MultimodalChatResponse(
            success=False,
            message=str(e)
        )


# ============ STATUS & CAPABILITIES ============

@router.get("/status")
async def get_voice_status():
    """
    üìä Voice & Multimodal durumu
    
    Mevcut ses ve g√∂rsel yeteneklerinin durumunu d√∂nd√ºr√ºr.
    """
    status = {
        "tts": {
            "provider": "Pyttsx3 (LOCAL)",
            "available": False,
            "data_privacy": "100% LOCAL - No data leaves your computer"
        },
        "stt": {
            "provider": "Whisper Local (faster-whisper)",
            "available": False,
            "models": ["tiny", "base", "small", "medium", "large"],
            "data_privacy": "100% LOCAL - No data leaves your computer"
        },
        "vision": {
            "provider": "LLaVA (Ollama)",
            "available": False,
            "models": ["llava", "bakllava", "llava-llama3"],
            "data_privacy": "100% LOCAL - No data leaves your computer"
        }
    }
    
    # TTS kontrol√º
    try:
        import pyttsx3
        status["tts"]["available"] = True
        status["tts"]["message"] = "Pyttsx3 TTS hazƒ±r"
    except ImportError:
        status["tts"]["message"] = "pyttsx3 y√ºkl√º deƒüil: pip install pyttsx3"
    
    # STT kontrol√º
    try:
        from faster_whisper import WhisperModel
        status["stt"]["available"] = True
        status["stt"]["message"] = "Faster-Whisper STT hazƒ±r"
    except ImportError:
        try:
            import whisper
            status["stt"]["available"] = True
            status["stt"]["message"] = "OpenAI-Whisper STT hazƒ±r"
        except ImportError:
            status["stt"]["message"] = "Whisper y√ºkl√º deƒüil: pip install faster-whisper"
    
    # Vision kontrol√º
    try:
        import aiohttp
        # Ollama kontrol√º
        import httpx
        try:
            response = httpx.get("http://localhost:11434/api/tags", timeout=2)
            if response.status_code == 200:
                models = response.json().get("models", [])
                vision_models = [m["name"] for m in models if "llava" in m["name"].lower()]
                if vision_models:
                    status["vision"]["available"] = True
                    status["vision"]["installed_models"] = vision_models
                    status["vision"]["message"] = f"LLaVA modelleri mevcut: {vision_models}"
                else:
                    status["vision"]["message"] = "LLaVA modeli y√ºkl√º deƒüil: ollama pull llava"
            else:
                status["vision"]["message"] = "Ollama baƒülantƒ± hatasƒ±"
        except Exception:
            status["vision"]["message"] = "Ollama √ßalƒ±≈ümƒ±yor"
    except ImportError:
        status["vision"]["message"] = "aiohttp y√ºkl√º deƒüil"
    
    return {
        "success": True,
        "capabilities": status,
        "privacy_note": "üîí T√ºm i≈ülemler bilgisayarƒ±nƒ±zda yapƒ±lƒ±r. Hi√ßbir veri dƒ±≈üarƒ± g√∂nderilmez."
    }


@router.get("/voices")
async def list_available_voices():
    """
    üîä Mevcut TTS seslerini listele
    """
    try:
        import pyttsx3
        
        engine = pyttsx3.init()
        voices = engine.getProperty('voices')
        
        voice_list = []
        for voice in voices:
            voice_list.append({
                "id": voice.id,
                "name": voice.name,
                "languages": voice.languages,
                "gender": getattr(voice, 'gender', 'unknown')
            })
        
        engine.stop()
        
        return {
            "success": True,
            "voices": voice_list,
            "count": len(voice_list)
        }
        
    except ImportError:
        return {
            "success": False,
            "message": "pyttsx3 y√ºkl√º deƒüil",
            "voices": []
        }
    except Exception as e:
        return {
            "success": False,
            "message": str(e),
            "voices": []
        }


# Export router
voice_router = router
