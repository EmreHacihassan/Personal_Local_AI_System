"""
ðŸ§  Base Curriculum Agent

TÃ¼m Curriculum Studio agent'larÄ±nÄ±n temel sÄ±nÄ±fÄ±.
Visible reasoning, multi-model fallback ve streaming desteÄŸi.
"""

import asyncio
import json
import random
from abc import ABC, abstractmethod
from dataclasses import dataclass, field
from datetime import datetime
from typing import List, Dict, Any, Optional, AsyncGenerator, Tuple
from enum import Enum


class ThinkingPhase(str, Enum):
    """DÃ¼ÅŸÃ¼nme fazlarÄ±"""
    ANALYZING = "analyzing"
    REASONING = "reasoning"
    DECIDING = "deciding"
    CONCLUDING = "concluding"


@dataclass
class AgentThought:
    """
    Visible Reasoning - GÃ¶rÃ¼nÃ¼r DÃ¼ÅŸÃ¼nce
    
    Her agent'Ä±n dÃ¼ÅŸÃ¼nce sÃ¼reci kullanÄ±cÄ±ya gÃ¶sterilir.
    Deep Scholar 2.0 tarzÄ± "AI dÃ¼ÅŸÃ¼nÃ¼yor" deneyimi.
    """
    agent_name: str
    agent_icon: str = "ðŸ¤–"
    step: str = ""
    phase: ThinkingPhase = ThinkingPhase.ANALYZING
    
    # DÃ¼ÅŸÃ¼nce iÃ§eriÄŸi
    thinking: str = ""          # KÄ±sa dÃ¼ÅŸÃ¼nce ("KonularÄ± analiz ediyorum...")
    reasoning: str = ""         # DetaylÄ± mantÄ±k zinciri
    evidence: List[str] = field(default_factory=list)  # KanÄ±tlar/kaynaklar
    conclusion: str = ""        # SonuÃ§
    
    # Meta
    confidence: float = 0.0     # 0.0 - 1.0
    duration_ms: int = 0
    timestamp: str = field(default_factory=lambda: datetime.now().isoformat())
    
    # Streaming
    is_streaming: bool = False
    is_complete: bool = False
    
    def to_dict(self) -> Dict[str, Any]:
        return {
            "agent_name": self.agent_name,
            "agent_icon": self.agent_icon,
            "step": self.step,
            "phase": self.phase.value,
            "thinking": self.thinking,
            "reasoning": self.reasoning,
            "evidence": self.evidence,
            "conclusion": self.conclusion,
            "confidence": self.confidence,
            "duration_ms": self.duration_ms,
            "timestamp": self.timestamp,
            "is_streaming": self.is_streaming,
            "is_complete": self.is_complete
        }


@dataclass
class AgentOutput:
    """Agent Ã§Ä±ktÄ±sÄ±"""
    agent_name: str
    result: Dict[str, Any] = field(default_factory=dict)
    thoughts: List[AgentThought] = field(default_factory=list)
    success: bool = True
    error: Optional[str] = None
    execution_time_ms: int = 0
    
    def to_dict(self) -> Dict[str, Any]:
        return {
            "agent_name": self.agent_name,
            "result": self.result,
            "thoughts": [t.to_dict() for t in self.thoughts],
            "success": self.success,
            "error": self.error,
            "execution_time_ms": self.execution_time_ms
        }


class BaseCurriculumAgent(ABC):
    """
    Curriculum Studio Base Agent
    
    Her agent:
    - BaÄŸÄ±msÄ±z dÃ¼ÅŸÃ¼nebilir
    - DÃ¼ÅŸÃ¼nce sÃ¼recini stream edebilir
    - Multi-model fallback kullanabilir
    - DiÄŸer agent'larla iletiÅŸim kurabilir
    
    Features:
    - Visible reasoning (dÃ¼ÅŸÃ¼nce sÃ¼reci gÃ¶rÃ¼nÃ¼r)
    - Confidence scoring
    - Evidence-based conclusions
    - Streaming support
    """
    
    # Agent avatarlarÄ±
    AGENT_ICONS = {
        "pedagogy": "ðŸ‘¨â€ðŸ«",
        "research": "ðŸ”",
        "content": "ðŸ“",
        "exam": "ðŸ“‹",
        "review": "ðŸ”¬",
        "orchestrator": "ðŸŽ­"
    }
    
    def __init__(
        self,
        name: str,
        role: str,
        specialty: str,
        model_preference: str = "ollama/qwen3:8b",
        fallback_models: List[str] = None,
        thinking_style: str = "analytical"
    ):
        self.name = name
        self.role = role
        self.specialty = specialty
        self.model_preference = model_preference
        self.fallback_models = fallback_models or [
            "ollama/llama3.2",
            "openai/gpt-4o-mini"
        ]
        self.thinking_style = thinking_style
        self.thoughts: List[AgentThought] = []
        self.agent_type = name.lower().split()[0] if name else "agent"
        self.icon = self.AGENT_ICONS.get(self.agent_type, "ðŸ¤–")
        
        # LLM service (lazy loading)
        self._llm_service = None
        
    @property
    def llm_service(self):
        """Lazy load LLM service"""
        if self._llm_service is None:
            try:
                from core.llm_router import get_best_available_llm
                self._llm_service = get_best_available_llm()
            except:
                pass
        return self._llm_service
    
    @llm_service.setter
    def llm_service(self, value):
        """Set LLM service"""
        self._llm_service = value
    
    async def think(
        self, 
        prompt: str, 
        step: str,
        context: Dict[str, Any] = None
    ) -> AsyncGenerator[AgentThought, None]:
        """
        DÃ¼ÅŸÃ¼nme sÃ¼reci - stream olarak dÃ¼ÅŸÃ¼nceleri yayÄ±nla
        
        Args:
            prompt: LLM'e gÃ¶nderilecek prompt
            step: AdÄ±m adÄ± (Ã¶rn: "hedef_analizi")
            context: Ek baÄŸlam bilgisi
            
        Yields:
            AgentThought - Her dÃ¼ÅŸÃ¼nce adÄ±mÄ±
        """
        start_time = datetime.now()
        
        # Faz 1: Analyzing
        yield AgentThought(
            agent_name=self.name,
            agent_icon=self.icon,
            step=step,
            phase=ThinkingPhase.ANALYZING,
            thinking=f"ðŸ” {step.replace('_', ' ').title()} Ã¼zerinde Ã§alÄ±ÅŸÄ±yorum...",
            is_streaming=True,
            is_complete=False
        )
        
        # GerÃ§ekÃ§i dÃ¼ÅŸÃ¼nme sÃ¼resi simÃ¼lasyonu
        await asyncio.sleep(random.uniform(1.5, 3.0))
        
        # Faz 2: Reasoning
        yield AgentThought(
            agent_name=self.name,
            agent_icon=self.icon,
            step=step,
            phase=ThinkingPhase.REASONING,
            thinking=f"ðŸ’­ Bilgileri deÄŸerlendiriyorum...",
            reasoning=self._generate_reasoning_preview(step, context),
            is_streaming=True,
            is_complete=False
        )
        
        await asyncio.sleep(random.uniform(1.0, 2.5))
        
        # LLM Ã§aÄŸrÄ±sÄ±
        try:
            response = await self._call_llm(prompt, context)
            
            # Faz 3: Concluding
            duration_ms = int((datetime.now() - start_time).total_seconds() * 1000)
            
            thought = AgentThought(
                agent_name=self.name,
                agent_icon=self.icon,
                step=step,
                phase=ThinkingPhase.CONCLUDING,
                thinking=f"âœ… {step.replace('_', ' ').title()} tamamlandÄ±",
                reasoning=response.get("reasoning", ""),
                evidence=response.get("evidence", []),
                conclusion=response.get("conclusion", ""),
                confidence=response.get("confidence", 0.85),
                duration_ms=duration_ms,
                is_streaming=False,
                is_complete=True
            )
            self.thoughts.append(thought)
            yield thought
            
        except Exception as e:
            yield AgentThought(
                agent_name=self.name,
                agent_icon=self.icon,
                step=step,
                phase=ThinkingPhase.CONCLUDING,
                thinking=f"âš ï¸ Hata oluÅŸtu: {str(e)[:50]}",
                reasoning="Fallback strateji uygulanÄ±yor...",
                confidence=0.5,
                is_streaming=False,
                is_complete=True
            )
    
    def _generate_reasoning_preview(self, step: str, context: Dict[str, Any] = None) -> str:
        """Reasoning Ã¶nizlemesi oluÅŸtur"""
        previews = {
            "hedef_analizi": "Ã–ÄŸrencinin hedefini, mevcut seviyesini ve Ã¶ÄŸrenme stilini analiz ediyorum...",
            "pedagojik_siralama": "Bloom taksonomisine gÃ¶re konularÄ± sÄ±ralÄ±yorum, Ã¶n koÅŸullarÄ± belirliyorum...",
            "ogrenme_stili": "GÃ¶rsel, iÅŸitsel ve kinestetik Ã¶ÄŸrenme tercihleri deÄŸerlendiriliyor...",
            "rag_arastirmasi": "Bilgi tabanÄ±ndan ilgili dokÃ¼manlarÄ± Ã§ekiyorum...",
            "web_arastirmasi": "GÃ¼ncel kaynaklarÄ± ve akademik makaleleri tarÄ±yorum...",
            "icerik_tasarimi": "Multimedya iÃ§erik yapÄ±sÄ±nÄ± planlÄ±yorum...",
            "soru_uretimi": "Bloom taksonomisi seviyelerine gÃ¶re sorular Ã¼retiyorum...",
            "kalite_kontrol": "Pedagojik tutarlÄ±lÄ±k ve iÃ§erik kalitesini deÄŸerlendiriyorum..."
        }
        return previews.get(step, "DetaylÄ± analiz yapÄ±lÄ±yor...")
    
    async def _call_llm(
        self, 
        prompt: str, 
        context: Dict[str, Any] = None
    ) -> Dict[str, Any]:
        """
        LLM Ã§aÄŸrÄ±sÄ± - multi-model fallback ile
        
        Returns:
            Dict with: reasoning, conclusion, confidence, evidence
        """
        system_prompt = f"""Sen bir {self.role}sÄ±n. UzmanlÄ±k alanÄ±n: {self.specialty}.
        
DÃ¼ÅŸÃ¼nme stilin: {self.thinking_style}

Her zaman yapÄ±landÄ±rÄ±lmÄ±ÅŸ, kanÄ±ta dayalÄ± ve gÃ¼ven seviyesi belirten yanÄ±tlar ver.
YanÄ±tÄ±nÄ± JSON formatÄ±nda dÃ¶ndÃ¼r:
{{
    "reasoning": "DetaylÄ± mantÄ±k zincirin",
    "conclusion": "Ana sonuÃ§",
    "confidence": 0.0-1.0 arasÄ± gÃ¼ven seviyesi,
    "evidence": ["KanÄ±t 1", "KanÄ±t 2"],
    "recommendations": ["Ã–neri 1", "Ã–neri 2"]
}}"""

        # Context'i prompt'a ekle
        if context:
            context_str = json.dumps(context, ensure_ascii=False, indent=2, default=str)
            full_prompt = f"BaÄŸlam:\n{context_str}\n\nGÃ¶rev:\n{prompt}"
        else:
            full_prompt = prompt
        
        # LLM service varsa kullan
        if self.llm_service:
            try:
                response = await self._call_llm_service(system_prompt, full_prompt)
                return response
            except Exception as e:
                print(f"[{self.name}] LLM error: {e}")
        
        # Fallback: Mock response
        return self._generate_mock_response(prompt, context)
    
    async def _call_llm_service(
        self, 
        system_prompt: str, 
        user_prompt: str
    ) -> Dict[str, Any]:
        """GerÃ§ek LLM servisi Ã§aÄŸrÄ±sÄ±"""
        try:
            # Ollama veya diÄŸer LLM servislerini dene
            response_text = await self.llm_service.generate(
                prompt=user_prompt,
                system_prompt=system_prompt,
                temperature=0.7,
                max_tokens=2000
            )
            
            # JSON parse et
            try:
                # JSON bloÄŸunu bul
                if "```json" in response_text:
                    json_str = response_text.split("```json")[1].split("```")[0]
                elif "```" in response_text:
                    json_str = response_text.split("```")[1].split("```")[0]
                else:
                    json_str = response_text
                    
                return json.loads(json_str.strip())
            except:
                return {
                    "reasoning": response_text[:500],
                    "conclusion": response_text[:200],
                    "confidence": 0.75,
                    "evidence": [],
                    "recommendations": []
                }
        except Exception as e:
            raise e
    
    def _generate_mock_response(
        self, 
        prompt: str, 
        context: Dict[str, Any] = None
    ) -> Dict[str, Any]:
        """Mock response - LLM olmadan Ã§alÄ±ÅŸma"""
        return {
            "reasoning": f"[{self.name}] {self.specialty} perspektifinden analiz yapÄ±ldÄ±.",
            "conclusion": f"BaÅŸarÄ±lÄ± analiz tamamlandÄ±.",
            "confidence": random.uniform(0.75, 0.95),
            "evidence": [
                "Pedagojik ilkelere uygunluk kontrol edildi",
                "Ã–ÄŸrenme hedefleri ile uyum saÄŸlandÄ±"
            ],
            "recommendations": [
                "AdÄ±m adÄ±m ilerleme Ã¶nerilir",
                "Pratik uygulamalar eklenmeli"
            ]
        }
    
    @abstractmethod
    async def execute(
        self, 
        context: Dict[str, Any]
    ) -> AsyncGenerator[AgentThought, None]:
        """
        Agent'Ä±n ana gÃ¶revi
        
        Args:
            context: Ã‡alÄ±ÅŸma baÄŸlamÄ± (goal, previous results, etc.)
            
        Yields:
            AgentThought - Her dÃ¼ÅŸÃ¼nce adÄ±mÄ±
        """
        pass
    
    async def get_final_output(self) -> AgentOutput:
        """TÃ¼m dÃ¼ÅŸÃ¼ncelerden final output oluÅŸtur"""
        # Son dÃ¼ÅŸÃ¼ncedeki conclusion'Ä± al
        conclusions = [t.conclusion for t in self.thoughts if t.conclusion]
        reasonings = [t.reasoning for t in self.thoughts if t.reasoning]
        
        total_time = sum(t.duration_ms for t in self.thoughts)
        avg_confidence = sum(t.confidence for t in self.thoughts) / max(len(self.thoughts), 1)
        
        return AgentOutput(
            agent_name=self.name,
            result={
                "conclusions": conclusions,
                "reasoning_chain": reasonings,
                "confidence": avg_confidence
            },
            thoughts=self.thoughts,
            success=True,
            execution_time_ms=total_time
        )
    
    def reset(self):
        """Agent durumunu sÄ±fÄ±rla"""
        self.thoughts = []
