"""
ResearchAnalytics - AraÅŸtÄ±rma AnalitiÄŸi Sistemi
================================================

Kaynak daÄŸÄ±lÄ±mÄ±, atÄ±f aÄŸlarÄ± ve konu haritalarÄ± oluÅŸturur.

Ã–zellikler:
- Source distribution analysis
- Citation network graph
- Topic coverage heatmap
- Research depth metrics
- Methodology analysis
"""

import asyncio
import json
import math
from dataclasses import dataclass, field
from typing import Dict, List, Any, Optional, Tuple
from datetime import datetime
from enum import Enum
from collections import Counter, defaultdict

from core.llm_manager import llm_manager
from core.logger import get_logger

logger = get_logger("research_analytics")


class SourceType(str, Enum):
    """Kaynak tÃ¼rleri."""
    ACADEMIC_PAPER = "academic_paper"
    BOOK = "book"
    WEB_ARTICLE = "web_article"
    NEWS = "news"
    REPORT = "report"
    THESIS = "thesis"
    CONFERENCE = "conference"
    PREPRINT = "preprint"


class QualityTier(str, Enum):
    """Kalite seviyeleri."""
    TIER_1 = "tier_1"  # Q1 dergiler, prestijli yayÄ±nlar
    TIER_2 = "tier_2"  # Q2-Q3 dergiler
    TIER_3 = "tier_3"  # Web kaynaklarÄ±, preprint
    UNKNOWN = "unknown"


@dataclass
class SourceMetrics:
    """Kaynak metrikleri."""
    total_sources: int
    by_type: Dict[str, int]
    by_quality: Dict[str, int]
    by_year: Dict[int, int]
    avg_citation_count: float
    diversity_score: float  # 0-1, ne kadar Ã§eÅŸitli kaynaklar
    recency_score: float  # 0-1, ne kadar gÃ¼ncel


@dataclass
class CitationNode:
    """AtÄ±f aÄŸÄ± dÃ¼ÄŸÃ¼mÃ¼."""
    id: str
    title: str
    authors: List[str]
    year: int
    type: SourceType
    citations: List[str]  # AtÄ±f yaptÄ±ÄŸÄ± diÄŸer kaynaklarÄ±n ID'leri
    cited_by: List[str]  # Bu kaynaÄŸÄ± atÄ±f yapanlar
    centrality: float  # AÄŸdaki Ã¶nem


@dataclass
class TopicCluster:
    """Konu kÃ¼mesi."""
    name: str
    keywords: List[str]
    source_count: int
    relevance_score: float
    sub_topics: List[str]


@dataclass
class ResearchAnalyticsReport:
    """AraÅŸtÄ±rma analitik raporu."""
    research_topic: str
    generated_at: datetime
    source_metrics: SourceMetrics
    citation_network: Dict[str, CitationNode]
    topic_clusters: List[TopicCluster]
    coverage_heatmap: Dict[str, Dict[str, float]]
    methodology_breakdown: Dict[str, int]
    gaps_identified: List[str]
    recommendations: List[str]
    quality_score: float  # 0-100 genel kalite skoru


class ResearchAnalyticsEngine:
    """
    AraÅŸtÄ±rma AnalitiÄŸi Motoru
    
    AraÅŸtÄ±rma kalitesini ve kapsamÄ±nÄ± analiz eder.
    """
    
    def __init__(self):
        self.sources: List[Dict[str, Any]] = []
        self.citation_graph: Dict[str, CitationNode] = {}
        self.topic_model: Dict[str, List[str]] = {}
    
    async def analyze_sources(
        self,
        sources: List[Dict[str, Any]],
        topic: str
    ) -> ResearchAnalyticsReport:
        """
        KaynaklarÄ± analiz et ve rapor oluÅŸtur.
        
        Args:
            sources: Kaynak listesi
            topic: AraÅŸtÄ±rma konusu
        
        Returns:
            ResearchAnalyticsReport
        """
        self.sources = sources
        
        # Source metrics hesapla
        source_metrics = self._calculate_source_metrics(sources)
        
        # AtÄ±f aÄŸÄ± oluÅŸtur
        citation_network = await self._build_citation_network(sources)
        
        # Konu kÃ¼meleri Ã§Ä±kar
        topic_clusters = await self._extract_topic_clusters(sources, topic)
        
        # Coverage heatmap
        coverage_heatmap = self._generate_coverage_heatmap(
            topic_clusters,
            sources
        )
        
        # Metodoloji breakdown
        methodology = await self._analyze_methodologies(sources)
        
        # Gap analizi
        gaps = await self._identify_gaps(topic, topic_clusters, sources)
        
        # Ã–neriler
        recommendations = await self._generate_recommendations(
            source_metrics,
            gaps,
            topic
        )
        
        # Genel kalite skoru
        quality_score = self._calculate_quality_score(
            source_metrics,
            topic_clusters,
            coverage_heatmap
        )
        
        return ResearchAnalyticsReport(
            research_topic=topic,
            generated_at=datetime.now(),
            source_metrics=source_metrics,
            citation_network=citation_network,
            topic_clusters=topic_clusters,
            coverage_heatmap=coverage_heatmap,
            methodology_breakdown=methodology,
            gaps_identified=gaps,
            recommendations=recommendations,
            quality_score=quality_score
        )
    
    def _calculate_source_metrics(
        self,
        sources: List[Dict[str, Any]]
    ) -> SourceMetrics:
        """Kaynak metriklerini hesapla."""
        if not sources:
            return SourceMetrics(
                total_sources=0,
                by_type={},
                by_quality={},
                by_year={},
                avg_citation_count=0,
                diversity_score=0,
                recency_score=0
            )
        
        # TÃ¼r daÄŸÄ±lÄ±mÄ±
        by_type = Counter()
        for s in sources:
            source_type = s.get("type", "unknown")
            by_type[source_type] += 1
        
        # Kalite daÄŸÄ±lÄ±mÄ±
        by_quality = Counter()
        for s in sources:
            quality = self._assess_source_quality(s)
            by_quality[quality] += 1
        
        # YÄ±l daÄŸÄ±lÄ±mÄ±
        by_year = Counter()
        current_year = datetime.now().year
        for s in sources:
            year = s.get("year", s.get("publication_year", current_year))
            if isinstance(year, int) and 1900 < year <= current_year + 1:
                by_year[year] += 1
        
        # Ortalama atÄ±f sayÄ±sÄ±
        citation_counts = [
            s.get("citations", s.get("citation_count", 0))
            for s in sources
        ]
        avg_citations = sum(citation_counts) / len(citation_counts) if citation_counts else 0
        
        # Ã‡eÅŸitlilik skoru (Shannon entropy based)
        type_probs = [c / len(sources) for c in by_type.values()]
        diversity = -sum(p * math.log(p) if p > 0 else 0 for p in type_probs)
        max_entropy = math.log(max(len(SourceType), 1))
        diversity_score = diversity / max_entropy if max_entropy > 0 else 0
        
        # GÃ¼ncellik skoru
        if by_year:
            weighted_sum = sum(
                year * count for year, count in by_year.items()
            )
            total_count = sum(by_year.values())
            avg_year = weighted_sum / total_count if total_count > 0 else current_year
            # Son 5 yÄ±l = 1.0, 20+ yÄ±l = 0
            years_old = current_year - avg_year
            recency_score = max(0, 1 - (years_old / 20))
        else:
            recency_score = 0.5
        
        return SourceMetrics(
            total_sources=len(sources),
            by_type=dict(by_type),
            by_quality=dict(by_quality),
            by_year=dict(by_year),
            avg_citation_count=avg_citations,
            diversity_score=min(1.0, diversity_score),
            recency_score=recency_score
        )
    
    def _assess_source_quality(self, source: Dict[str, Any]) -> str:
        """Kaynak kalitesini deÄŸerlendir."""
        source_type = source.get("type", "").lower()
        journal = source.get("journal", "").lower()
        citations = source.get("citations", source.get("citation_count", 0))
        
        # Tier 1 indicators
        tier1_journals = ["nature", "science", "lancet", "cell", "nejm", "pnas"]
        if any(j in journal for j in tier1_journals):
            return QualityTier.TIER_1.value
        
        if source_type in ["academic_paper", "thesis"] and citations > 50:
            return QualityTier.TIER_1.value
        
        # Tier 2
        if source_type in ["academic_paper", "book", "conference"] and citations > 10:
            return QualityTier.TIER_2.value
        
        # Tier 3
        if source_type in ["web_article", "news", "preprint"]:
            return QualityTier.TIER_3.value
        
        return QualityTier.UNKNOWN.value
    
    async def _build_citation_network(
        self,
        sources: List[Dict[str, Any]]
    ) -> Dict[str, CitationNode]:
        """AtÄ±f aÄŸÄ± oluÅŸtur."""
        network: Dict[str, CitationNode] = {}
        
        for i, source in enumerate(sources):
            source_id = source.get("id", f"source_{i}")
            citations = source.get("references", source.get("citations_list", []))
            
            node = CitationNode(
                id=source_id,
                title=source.get("title", "Unknown"),
                authors=source.get("authors", []),
                year=source.get("year", 2024),
                type=SourceType(source.get("type", "web_article")),
                citations=[str(c) for c in citations if c],
                cited_by=[],
                centrality=0.0
            )
            network[source_id] = node
        
        # Cited_by iliÅŸkilerini kur
        for node_id, node in network.items():
            for cited_id in node.citations:
                if cited_id in network:
                    network[cited_id].cited_by.append(node_id)
        
        # Centrality hesapla (basit degree centrality)
        max_connections = max(
            len(n.citations) + len(n.cited_by)
            for n in network.values()
        ) if network else 1
        
        for node in network.values():
            connections = len(node.citations) + len(node.cited_by)
            node.centrality = connections / max_connections if max_connections > 0 else 0
        
        return network
    
    async def _extract_topic_clusters(
        self,
        sources: List[Dict[str, Any]],
        main_topic: str
    ) -> List[TopicCluster]:
        """Konu kÃ¼melerini Ã§Ä±kar."""
        if not sources:
            return []
        
        # Kaynaklardan anahtar kelimeleri topla
        all_keywords = []
        for source in sources:
            keywords = source.get("keywords", [])
            if isinstance(keywords, list):
                all_keywords.extend(keywords)
            
            # Title'dan keyword Ã§Ä±kar
            title_words = source.get("title", "").split()
            all_keywords.extend([
                w.lower() for w in title_words
                if len(w) > 4 and w.isalpha()
            ])
        
        # Keyword frekansÄ±
        keyword_freq = Counter(all_keywords)
        top_keywords = [kw for kw, _ in keyword_freq.most_common(20)]
        
        # LLM ile kÃ¼meleme
        prompt = f"""AÅŸaÄŸÄ±daki anahtar kelimeleri {main_topic} konusu iÃ§in tematik kÃ¼melere ayÄ±r.
Her kÃ¼me iÃ§in bir isim ve alt konular belirle.

Anahtar Kelimeler:
{', '.join(top_keywords[:30])}

Ana Konu: {main_topic}

KÃ¼meler (JSON formatÄ±nda, 3-5 kÃ¼me):
[{{"name": "kÃ¼me adÄ±", "keywords": ["kw1", "kw2"], "sub_topics": ["alt konu 1", "alt konu 2"]}}]"""
        
        try:
            response = await llm_manager.generate_async(
                prompt=prompt,
                temperature=0.3,
                max_tokens=800
            )
            
            # JSON parse
            json_match = response.find("[")
            json_end = response.rfind("]") + 1
            if json_match >= 0 and json_end > json_match:
                clusters_data = json.loads(response[json_match:json_end])
            else:
                clusters_data = []
            
            clusters = []
            for cd in clusters_data[:5]:
                # Her kÃ¼me iÃ§in kaynak sayÄ±sÄ±nÄ± hesapla
                cluster_keywords = set(cd.get("keywords", []))
                source_count = sum(
                    1 for s in sources
                    if cluster_keywords.intersection(set(s.get("keywords", [])))
                )
                
                clusters.append(TopicCluster(
                    name=cd.get("name", "Unknown"),
                    keywords=cd.get("keywords", []),
                    source_count=max(1, source_count),
                    relevance_score=0.8,  # Default
                    sub_topics=cd.get("sub_topics", [])
                ))
            
            return clusters
            
        except Exception as e:
            logger.error(f"Topic clustering error: {e}")
            # Fallback: basit kÃ¼meleme
            return [TopicCluster(
                name=main_topic,
                keywords=top_keywords[:10],
                source_count=len(sources),
                relevance_score=1.0,
                sub_topics=[]
            )]
    
    def _generate_coverage_heatmap(
        self,
        clusters: List[TopicCluster],
        sources: List[Dict[str, Any]]
    ) -> Dict[str, Dict[str, float]]:
        """Kapsam heatmap'i oluÅŸtur."""
        heatmap = {}
        
        # Dimensions: topic cluster vs source type
        source_types = list(set(
            s.get("type", "unknown") for s in sources
        ))
        
        for cluster in clusters:
            cluster_sources = [
                s for s in sources
                if set(cluster.keywords).intersection(set(s.get("keywords", [])))
            ]
            
            type_coverage = {}
            for st in source_types:
                count = sum(1 for s in cluster_sources if s.get("type") == st)
                # Normalize to 0-1
                type_coverage[st] = min(1.0, count / 5)  # 5+ = full coverage
            
            heatmap[cluster.name] = type_coverage
        
        return heatmap
    
    async def _analyze_methodologies(
        self,
        sources: List[Dict[str, Any]]
    ) -> Dict[str, int]:
        """Metodoloji daÄŸÄ±lÄ±mÄ±nÄ± analiz et."""
        methodology_counts = Counter()
        
        default_methodologies = {
            "literature_review": 0,
            "empirical_study": 0,
            "case_study": 0,
            "survey": 0,
            "experimental": 0,
            "theoretical": 0,
            "meta_analysis": 0,
            "mixed_methods": 0
        }
        
        for source in sources:
            methodology = source.get("methodology", "unknown")
            if methodology != "unknown":
                methodology_counts[methodology] += 1
            else:
                # Heuristic: abstract'tan tahmin et
                abstract = source.get("abstract", "").lower()
                if "systematic review" in abstract or "meta-analysis" in abstract:
                    methodology_counts["meta_analysis"] += 1
                elif "survey" in abstract or "questionnaire" in abstract:
                    methodology_counts["survey"] += 1
                elif "experiment" in abstract or "rct" in abstract:
                    methodology_counts["experimental"] += 1
                elif "case study" in abstract:
                    methodology_counts["case_study"] += 1
                else:
                    methodology_counts["literature_review"] += 1
        
        # Merge with defaults
        default_methodologies.update(methodology_counts)
        return dict(default_methodologies)
    
    async def _identify_gaps(
        self,
        topic: str,
        clusters: List[TopicCluster],
        sources: List[Dict[str, Any]]
    ) -> List[str]:
        """AraÅŸtÄ±rma boÅŸluklarÄ±nÄ± tespit et."""
        gaps = []
        
        # DÃ¼ÅŸÃ¼k kapsama alanlarÄ±
        for cluster in clusters:
            if cluster.source_count < 2:
                gaps.append(
                    f"'{cluster.name}' konusunda kaynak yetersizliÄŸi ({cluster.source_count} kaynak)"
                )
        
        # Metodoloji eksiklikleri
        source_types = Counter(s.get("type", "unknown") for s in sources)
        if source_types.get("academic_paper", 0) < 3:
            gaps.append("Akademik makale sayÄ±sÄ± yetersiz")
        
        # GÃ¼ncellik eksikliÄŸi
        current_year = datetime.now().year
        recent_sources = sum(
            1 for s in sources
            if s.get("year", 0) >= current_year - 2
        )
        if recent_sources < len(sources) * 0.3:
            gaps.append("GÃ¼ncel kaynak oranÄ± dÃ¼ÅŸÃ¼k (%30'un altÄ±nda)")
        
        # LLM ile ek gap tespiti
        if clusters:
            cluster_names = ", ".join(c.name for c in clusters)
            prompt = f"""AÅŸaÄŸÄ±daki konu ve alt baÅŸlÄ±klar iÃ§in potansiyel araÅŸtÄ±rma boÅŸluklarÄ±nÄ± belirle.

Ana Konu: {topic}
Mevcut Alt BaÅŸlÄ±klar: {cluster_names}
Kaynak SayÄ±sÄ±: {len(sources)}

Eksik olabilecek 2-3 alan (kÄ±sa maddeler):"""
            
            try:
                response = await llm_manager.generate_async(
                    prompt=prompt,
                    temperature=0.4,
                    max_tokens=200
                )
                
                for line in response.split("\n"):
                    line = line.strip()
                    if line and line[0] in "-â€¢123456789":
                        gap = line.lstrip("-â€¢0123456789.) ").strip()
                        if gap and len(gap) > 10:
                            gaps.append(gap)
                
            except Exception as e:
                logger.error(f"Gap analysis error: {e}")
        
        return gaps[:6]
    
    async def _generate_recommendations(
        self,
        metrics: SourceMetrics,
        gaps: List[str],
        topic: str
    ) -> List[str]:
        """Ã–neriler oluÅŸtur."""
        recommendations = []
        
        # Diversity Ã¶nerileri
        if metrics.diversity_score < 0.5:
            recommendations.append(
                "ğŸ”„ Kaynak Ã§eÅŸitliliÄŸini artÄ±rÄ±n - farklÄ± tÃ¼rde kaynaklar ekleyin"
            )
        
        # GÃ¼ncellik Ã¶nerileri
        if metrics.recency_score < 0.5:
            recommendations.append(
                "ğŸ“… Daha gÃ¼ncel kaynaklar ekleyin (son 3 yÄ±l)"
            )
        
        # Kalite Ã¶nerileri
        tier1_ratio = metrics.by_quality.get("tier_1", 0) / max(1, metrics.total_sources)
        if tier1_ratio < 0.2:
            recommendations.append(
                "â­ Tier-1 akademik kaynaklarÄ±n oranÄ±nÄ± artÄ±rÄ±n"
            )
        
        # Gap-based Ã¶neriler
        for gap in gaps[:2]:
            recommendations.append(f"ğŸ“Š {gap} - bu alanda araÅŸtÄ±rma geniÅŸletin")
        
        return recommendations[:5]
    
    def _calculate_quality_score(
        self,
        metrics: SourceMetrics,
        clusters: List[TopicCluster],
        heatmap: Dict[str, Dict[str, float]]
    ) -> float:
        """Genel kalite skoru hesapla (0-100)."""
        score = 0.0
        
        # Kaynak sayÄ±sÄ± (max 20 pts)
        source_score = min(20, metrics.total_sources * 2)
        score += source_score
        
        # Ã‡eÅŸitlilik (max 20 pts)
        score += metrics.diversity_score * 20
        
        # GÃ¼ncellik (max 20 pts)
        score += metrics.recency_score * 20
        
        # Kalite daÄŸÄ±lÄ±mÄ± (max 20 pts)
        tier1_ratio = metrics.by_quality.get("tier_1", 0) / max(1, metrics.total_sources)
        tier2_ratio = metrics.by_quality.get("tier_2", 0) / max(1, metrics.total_sources)
        quality_ratio = tier1_ratio + tier2_ratio * 0.5
        score += quality_ratio * 20
        
        # Konu kapsamÄ± (max 20 pts)
        if clusters and heatmap:
            avg_coverage = sum(
                sum(v.values()) / max(1, len(v))
                for v in heatmap.values()
            ) / len(heatmap)
            score += avg_coverage * 20
        
        return min(100, round(score, 1))
    
    def to_event(self, report: ResearchAnalyticsReport) -> Dict[str, Any]:
        """WebSocket event formatÄ±na dÃ¶nÃ¼ÅŸtÃ¼r."""
        return {
            "type": "research_analytics",
            "timestamp": report.generated_at.isoformat(),
            "topic": report.research_topic,
            "quality_score": report.quality_score,
            "source_count": report.source_metrics.total_sources,
            "diversity_score": round(report.source_metrics.diversity_score, 2),
            "recency_score": round(report.source_metrics.recency_score, 2),
            "topic_clusters": [
                {"name": c.name, "count": c.source_count}
                for c in report.topic_clusters
            ],
            "gaps_identified": report.gaps_identified[:3],
            "recommendations": report.recommendations[:3],
            "message": f"ğŸ“Š AraÅŸtÄ±rma kalitesi: {report.quality_score}/100"
        }
    
    def generate_markdown_report(self, report: ResearchAnalyticsReport) -> str:
        """Markdown formatÄ±nda rapor oluÅŸtur."""
        md = []
        md.append(f"# ğŸ“Š AraÅŸtÄ±rma AnalitiÄŸi Raporu")
        md.append(f"**Konu:** {report.research_topic}")
        md.append(f"**OluÅŸturulma:** {report.generated_at.strftime('%Y-%m-%d %H:%M')}")
        md.append(f"**Kalite Skoru:** {report.quality_score}/100")
        md.append("")
        
        # Kaynak metrikleri
        md.append("## ğŸ“š Kaynak Metrikleri")
        md.append(f"- **Toplam Kaynak:** {report.source_metrics.total_sources}")
        md.append(f"- **Ã‡eÅŸitlilik Skoru:** {report.source_metrics.diversity_score:.2f}")
        md.append(f"- **GÃ¼ncellik Skoru:** {report.source_metrics.recency_score:.2f}")
        md.append(f"- **Ortalama AtÄ±f:** {report.source_metrics.avg_citation_count:.1f}")
        md.append("")
        
        # Kaynak tÃ¼rÃ¼ daÄŸÄ±lÄ±mÄ±
        md.append("### Kaynak TÃ¼rÃ¼ DaÄŸÄ±lÄ±mÄ±")
        for stype, count in report.source_metrics.by_type.items():
            md.append(f"- {stype}: {count}")
        md.append("")
        
        # Konu kÃ¼meleri
        md.append("## ğŸ¯ Konu KÃ¼meleri")
        for cluster in report.topic_clusters:
            md.append(f"### {cluster.name}")
            md.append(f"- Kaynak sayÄ±sÄ±: {cluster.source_count}")
            md.append(f"- Anahtar kelimeler: {', '.join(cluster.keywords[:5])}")
            if cluster.sub_topics:
                md.append(f"- Alt konular: {', '.join(cluster.sub_topics)}")
            md.append("")
        
        # BoÅŸluklar
        md.append("## âš ï¸ Tespit Edilen BoÅŸluklar")
        for gap in report.gaps_identified:
            md.append(f"- {gap}")
        md.append("")
        
        # Ã–neriler
        md.append("## ğŸ’¡ Ã–neriler")
        for rec in report.recommendations:
            md.append(f"- {rec}")
        
        return "\n".join(md)
